index,project,file,func_name,func_doc
1,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\auth.py,login_required,View decorator that redirects anonymous users to the login page.
2,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\auth.py,load_logged_in_user,"If a user id is stored in the session, load the user object from
    the database into ``g.user``."
3,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\auth.py,register,"Register a new user.

    Validates that the username is not already taken. Hashes the
    password for security.
    "
4,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\auth.py,login,Log in a registered user by adding the user id to the session.
5,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\auth.py,logout,"Clear the current session, including the stored user id."
6,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\blog.py,index,"Show all the posts, most recent first."
7,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\blog.py,get_post,"Get a post and its author by id.

    Checks that the id exists and optionally that the current user is
    the author.

    :param id: id of post to get
    :param check_author: require the current user to be the author
    :return: the post with author information
    :raise 404: if a post with the given id doesn't exist
    :raise 403: if the current user isn't the author
    "
8,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\blog.py,create,Create a new post for the current user.
9,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\blog.py,update,Update a post if the current user is the author.
10,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\blog.py,delete,"Delete a post.

    Ensures that the post exists and that the logged in user is the
    author of the post.
    "
11,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\db.py,get_db,"Connect to the application's configured database. The connection
    is unique for each request and will be reused if this is called
    again.
    "
12,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\db.py,close_db,"If this request connected to the database, close the
    connection.
    "
13,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\db.py,init_db,Clear existing data and create new tables.
14,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\db.py,init_db_command,Clear existing data and create new tables.
15,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\db.py,init_app,"Register database functions with the Flask app. This is called by
    the application factory.
    "
16,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\flaskr\__init__.py,create_app,Create and configure an instance of the Flask application.
17,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\tests\conftest.py,app,Create and configure a new app instance for each test.
18,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\tests\conftest.py,client,A test client for the app.
19,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\tests\conftest.py,runner,A test runner for the app's Click commands.
20,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\examples\tutorial\tests\test_factory.py,test_config,Test create_app without passing test config.
21,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,setupmethod,"Wraps a method so that it performs a check in debug mode if the
    first request was already handled.
    "
22,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,name,"The name of the application.  This is usually the import name
        with the difference that it's guessed from the run file if the
        import name is main.  This name is used as a display name when
        Flask needs the name of the application.  It can be set and overridden
        to change the value.

        .. versionadded:: 0.8
        "
23,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,propagate_exceptions,"Returns the value of the ``PROPAGATE_EXCEPTIONS`` configuration
        value in case it's set, otherwise a sensible default is returned.

        .. versionadded:: 0.7
        "
24,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,preserve_context_on_exception,"Returns the value of the ``PRESERVE_CONTEXT_ON_EXCEPTION``
        configuration value in case it's set, otherwise a sensible default
        is returned.

        .. versionadded:: 0.7
        "
25,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,logger,"The ``'flask.app'`` logger, a standard Python
        :class:`~logging.Logger`.

        In debug mode, the logger's :attr:`~logging.Logger.level` will be set
        to :data:`~logging.DEBUG`.

        If there are no handlers configured, a default handler will be added.
        See :ref:`logging` for more information.

        .. versionchanged:: 1.0
            Behavior was simplified. The logger is always named
            ``flask.app``. The level is only set during configuration, it
            doesn't check ``app.debug`` each time. Only one format is used,
            not different ones depending on ``app.debug``. No handlers are
            removed, and a handler is only added if no handlers are already
            configured.

        .. versionadded:: 0.3
        "
26,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,jinja_env,The Jinja2 environment used to load templates.
27,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,got_first_request,"This attribute is set to ``True`` if the application started
        handling the first request.

        .. versionadded:: 0.8
        "
28,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,make_config,"Used to create the config attribute by the Flask constructor.
        The `instance_relative` parameter is passed in from the constructor
        of Flask (there named `instance_relative_config`) and indicates if
        the config should be relative to the instance path or the root path
        of the application.

        .. versionadded:: 0.8
        "
29,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,auto_find_instance_path,"Tries to locate the instance path if it was not provided to the
        constructor of the application class.  It will basically calculate
        the path to a folder named ``instance`` next to your main file or
        the package.

        .. versionadded:: 0.8
        "
30,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,open_instance_resource,"Opens a resource from the application's instance folder
        (:attr:`instance_path`).  Otherwise works like
        :meth:`open_resource`.  Instance resources can also be opened for
        writing.

        :param resource: the name of the resource.  To access resources within
                         subfolders use forward slashes as separator.
        :param mode: resource file opening mode, default is 'rb'.
        "
31,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,_get_templates_auto_reload,"Reload templates when they are changed. Used by
        :meth:`create_jinja_environment`.

        This attribute can be configured with :data:`TEMPLATES_AUTO_RELOAD`. If
        not set, it will be enabled in debug mode.

        .. versionadded:: 1.0
            This property was added but the underlying config and behavior
            already existed.
        "
32,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,create_jinja_environment,"Creates the Jinja2 environment based on :attr:`jinja_options`
        and :meth:`select_jinja_autoescape`.  Since 0.7 this also adds
        the Jinja2 globals and filters after initialization.  Override
        this function to customize the behavior.

        .. versionadded:: 0.5
        .. versionchanged:: 0.11
           ``Environment.auto_reload`` set in accordance with
           ``TEMPLATES_AUTO_RELOAD`` configuration option.
        "
33,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,create_global_jinja_loader,"Creates the loader for the Jinja2 environment.  Can be used to
        override just the loader and keeping the rest unchanged.  It's
        discouraged to override this function.  Instead one should override
        the :meth:`jinja_loader` function instead.

        The global loader dispatches between the loaders of the application
        and the individual blueprints.

        .. versionadded:: 0.7
        "
34,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,select_jinja_autoescape,"Returns ``True`` if autoescaping should be active for the given
        template name. If no template name is given, returns `True`.

        .. versionadded:: 0.5
        "
35,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,update_template_context,"Update the template context with some commonly used variables.
        This injects request, session, config and g into the template
        context as well as everything template context processors want
        to inject.  Note that the as of Flask 0.6, the original values
        in the context will not be overridden if a context processor
        decides to return a value with the same key.

        :param context: the context as a dictionary that is updated in place
                        to add extra variables.
        "
36,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,make_shell_context,"Returns the shell context for an interactive shell for this
        application.  This runs all the registered shell context
        processors.

        .. versionadded:: 0.11
        "
37,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,run,"Runs the application on a local development server.

        Do not use ``run()`` in a production setting. It is not intended to
        meet security and performance requirements for a production server.
        Instead, see :ref:`deployment` for WSGI server recommendations.

        If the :attr:`debug` flag is set the server will automatically reload
        for code changes and show a debugger in case an exception happened.

        If you want to run the application in debug mode, but disable the
        code execution on the interactive debugger, you can pass
        ``use_evalex=False`` as parameter.  This will keep the debugger's
        traceback screen active, but disable code execution.

        It is not recommended to use this function for development with
        automatic reloading as this is badly supported.  Instead you should
        be using the :command:`flask` command line script's ``run`` support.

        .. admonition:: Keep in Mind

           Flask will suppress any server error with a generic error page
           unless it is in debug mode.  As such to enable just the
           interactive debugger without the code reloading, you have to
           invoke :meth:`run` with ``debug=True`` and ``use_reloader=False``.
           Setting ``use_debugger`` to ``True`` without being in debug mode
           won't catch any exceptions because there won't be any to
           catch.

        :param host: the hostname to listen on. Set this to ``'0.0.0.0'`` to
            have the server available externally as well. Defaults to
            ``'127.0.0.1'`` or the host in the ``SERVER_NAME`` config variable
            if present.
        :param port: the port of the webserver. Defaults to ``5000`` or the
            port defined in the ``SERVER_NAME`` config variable if present.
        :param debug: if given, enable or disable debug mode. See
            :attr:`debug`.
        :param load_dotenv: Load the nearest :file:`.env` and :file:`.flaskenv`
            files to set environment variables. Will also change the working
            directory to the directory containing the first file found.
        :param options: the options to be forwarded to the underlying Werkzeug
            server. See :func:`werkzeug.serving.run_simple` for more
            information.

        .. versionchanged:: 1.0
            If installed, python-dotenv will be used to load environment
            variables from :file:`.env` and :file:`.flaskenv` files.

            If set, the :envvar:`FLASK_ENV` and :envvar:`FLASK_DEBUG`
            environment variables will override :attr:`env` and
            :attr:`debug`.

            Threaded mode is enabled by default.

        .. versionchanged:: 0.10
            The default port is now picked from the ``SERVER_NAME``
            variable.
        "
38,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,test_client,"Creates a test client for this application.  For information
        about unit testing head over to :ref:`testing`.

        Note that if you are testing for assertions or exceptions in your
        application code, you must set ``app.testing = True`` in order for the
        exceptions to propagate to the test client.  Otherwise, the exception
        will be handled by the application (not visible to the test client) and
        the only indication of an AssertionError or other exception will be a
        500 status code response to the test client.  See the :attr:`testing`
        attribute.  For example::

            app.testing = True
            client = app.test_client()

        The test client can be used in a ``with`` block to defer the closing down
        of the context until the end of the ``with`` block.  This is useful if
        you want to access the context locals for testing::

            with app.test_client() as c:
                rv = c.get('/?vodka=42')
                assert request.args['vodka'] == '42'

        Additionally, you may pass optional keyword arguments that will then
        be passed to the application's :attr:`test_client_class` constructor.
        For example::

            from flask.testing import FlaskClient

            class CustomClient(FlaskClient):
                def __init__(self, *args, **kwargs):
                    self._authentication = kwargs.pop(""authentication"")
                    super(CustomClient,self).__init__( *args, **kwargs)

            app.test_client_class = CustomClient
            client = app.test_client(authentication='Basic ....')

        See :class:`~flask.testing.FlaskClient` for more information.

        .. versionchanged:: 0.4
           added support for ``with`` block usage for the client.

        .. versionadded:: 0.7
           The `use_cookies` parameter was added as well as the ability
           to override the client to be used by setting the
           :attr:`test_client_class` attribute.

        .. versionchanged:: 0.11
           Added `**kwargs` to support passing additional keyword arguments to
           the constructor of :attr:`test_client_class`.
        "
39,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,test_cli_runner,"Create a CLI runner for testing CLI commands.
        See :ref:`testing-cli`.

        Returns an instance of :attr:`test_cli_runner_class`, by default
        :class:`~flask.testing.FlaskCliRunner`. The Flask app object is
        passed as the first argument.

        .. versionadded:: 1.0
        "
40,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,open_session,"Creates or opens a new session.  Default implementation stores all
        session data in a signed cookie.  This requires that the
        :attr:`secret_key` is set.  Instead of overriding this method
        we recommend replacing the :class:`session_interface`.

        .. deprecated: 1.0
            Will be removed in 1.1. Use ``session_interface.open_session``
            instead.

        :param request: an instance of :attr:`request_class`.
        "
41,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,save_session,"Saves the session if it needs updates.  For the default
        implementation, check :meth:`open_session`.  Instead of overriding this
        method we recommend replacing the :class:`session_interface`.

        .. deprecated: 1.0
            Will be removed in 1.1. Use ``session_interface.save_session``
            instead.

        :param session: the session to be saved (a
                        :class:`~werkzeug.contrib.securecookie.SecureCookie`
                        object)
        :param response: an instance of :attr:`response_class`
        "
42,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,make_null_session,"Creates a new instance of a missing session.  Instead of overriding
        this method we recommend replacing the :class:`session_interface`.

        .. deprecated: 1.0
            Will be removed in 1.1. Use ``session_interface.make_null_session``
            instead.

        .. versionadded:: 0.7
        "
43,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,register_blueprint,"Register a :class:`~flask.Blueprint` on the application. Keyword
        arguments passed to this method will override the defaults set on the
        blueprint.

        Calls the blueprint's :meth:`~flask.Blueprint.register` method after
        recording the blueprint in the application's :attr:`blueprints`.

        :param blueprint: The blueprint to register.
        :param url_prefix: Blueprint routes will be prefixed with this.
        :param subdomain: Blueprint routes will match on this subdomain.
        :param url_defaults: Blueprint routes will use these default values for
            view arguments.
        :param options: Additional keyword arguments are passed to
            :class:`~flask.blueprints.BlueprintSetupState`. They can be
            accessed in :meth:`~flask.Blueprint.record` callbacks.

        .. versionadded:: 0.7
        "
44,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,iter_blueprints,"Iterates over all blueprints by the order they were registered.

        .. versionadded:: 0.11
        "
45,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,add_url_rule,"Connects a URL rule.  Works exactly like the :meth:`route`
        decorator.  If a view_func is provided it will be registered with the
        endpoint.

        Basically this example::

            @app.route('/')
            def index():
                pass

        Is equivalent to the following::

            def index():
                pass
            app.add_url_rule('/', 'index', index)

        If the view_func is not provided you will need to connect the endpoint
        to a view function like so::

            app.view_functions['index'] = index

        Internally :meth:`route` invokes :meth:`add_url_rule` so if you want
        to customize the behavior via subclassing you only need to change
        this method.

        For more information refer to :ref:`url-route-registrations`.

        .. versionchanged:: 0.2
           `view_func` parameter added.

        .. versionchanged:: 0.6
           ``OPTIONS`` is added automatically as method.

        :param rule: the URL rule as string
        :param endpoint: the endpoint for the registered URL rule.  Flask
                         itself assumes the name of the view function as
                         endpoint
        :param view_func: the function to call when serving a request to the
                          provided endpoint
        :param provide_automatic_options: controls whether the ``OPTIONS``
            method should be added automatically. This can also be controlled
            by setting the ``view_func.provide_automatic_options = False``
            before adding the rule.
        :param options: the options to be forwarded to the underlying
                        :class:`~werkzeug.routing.Rule` object.  A change
                        to Werkzeug is handling of method options.  methods
                        is a list of methods this rule should be limited
                        to (``GET``, ``POST`` etc.).  By default a rule
                        just listens for ``GET`` (and implicitly ``HEAD``).
                        Starting with Flask 0.6, ``OPTIONS`` is implicitly
                        added and handled by the standard request handling.
        "
46,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,route,"A decorator that is used to register a view function for a
        given URL rule.  This does the same thing as :meth:`add_url_rule`
        but is intended for decorator usage::

            @app.route('/')
            def index():
                return 'Hello World'

        For more information refer to :ref:`url-route-registrations`.

        :param rule: the URL rule as string
        :param endpoint: the endpoint for the registered URL rule.  Flask
                         itself assumes the name of the view function as
                         endpoint
        :param options: the options to be forwarded to the underlying
                        :class:`~werkzeug.routing.Rule` object.  A change
                        to Werkzeug is handling of method options.  methods
                        is a list of methods this rule should be limited
                        to (``GET``, ``POST`` etc.).  By default a rule
                        just listens for ``GET`` (and implicitly ``HEAD``).
                        Starting with Flask 0.6, ``OPTIONS`` is implicitly
                        added and handled by the standard request handling.
        "
47,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,endpoint,"A decorator to register a function as an endpoint.
        Example::

            @app.endpoint('example.endpoint')
            def example():
                return ""example""

        :param endpoint: the name of the endpoint
        "
48,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,_get_exc_class_and_code,Ensure that we register only exceptions as handler keys
49,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,errorhandler,"Register a function to handle errors by code or exception class.

        A decorator that is used to register a function given an
        error code.  Example::

            @app.errorhandler(404)
            def page_not_found(error):
                return 'This page does not exist', 404

        You can also register handlers for arbitrary exceptions::

            @app.errorhandler(DatabaseError)
            def special_exception_handler(error):
                return 'Database connection failed', 500

        .. versionadded:: 0.7
            Use :meth:`register_error_handler` instead of modifying
            :attr:`error_handler_spec` directly, for application wide error
            handlers.

        .. versionadded:: 0.7
           One can now additionally also register custom exception types
           that do not necessarily have to be a subclass of the
           :class:`~werkzeug.exceptions.HTTPException` class.

        :param code_or_exception: the code as integer for the handler, or
                                  an arbitrary exception
        "
50,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,register_error_handler,"Alternative error attach function to the :meth:`errorhandler`
        decorator that is more straightforward to use for non decorator
        usage.

        .. versionadded:: 0.7
        "
51,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,_register_error_handler,"
        :type key: None|str
        :type code_or_exception: int|T<=Exception
        :type f: callable
        "
52,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,template_filter,"A decorator that is used to register custom template filter.
        You can specify a name for the filter, otherwise the function
        name will be used. Example::

          @app.template_filter()
          def reverse(s):
              return s[::-1]

        :param name: the optional name of the filter, otherwise the
                     function name will be used.
        "
53,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,add_template_filter,"Register a custom template filter.  Works exactly like the
        :meth:`template_filter` decorator.

        :param name: the optional name of the filter, otherwise the
                     function name will be used.
        "
54,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,template_test,"A decorator that is used to register custom template test.
        You can specify a name for the test, otherwise the function
        name will be used. Example::

          @app.template_test()
          def is_prime(n):
              if n == 2:
                  return True
              for i in range(2, int(math.ceil(math.sqrt(n))) + 1):
                  if n % i == 0:
                      return False
              return True

        .. versionadded:: 0.10

        :param name: the optional name of the test, otherwise the
                     function name will be used.
        "
55,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,add_template_test,"Register a custom template test.  Works exactly like the
        :meth:`template_test` decorator.

        .. versionadded:: 0.10

        :param name: the optional name of the test, otherwise the
                     function name will be used.
        "
56,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,template_global,"A decorator that is used to register a custom template global function.
        You can specify a name for the global function, otherwise the function
        name will be used. Example::

            @app.template_global()
            def double(n):
                return 2 * n

        .. versionadded:: 0.10

        :param name: the optional name of the global function, otherwise the
                     function name will be used.
        "
57,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,add_template_global,"Register a custom template global function. Works exactly like the
        :meth:`template_global` decorator.

        .. versionadded:: 0.10

        :param name: the optional name of the global function, otherwise the
                     function name will be used.
        "
58,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,before_request,"Registers a function to run before each request.

        For example, this can be used to open a database connection, or to load
        the logged in user from the session.

        The function will be called without any arguments. If it returns a
        non-None value, the value is handled as if it was the return value from
        the view, and further request handling is stopped.
        "
59,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,before_first_request,"Registers a function to be run before the first request to this
        instance of the application.

        The function will be called without any arguments and its return
        value is ignored.

        .. versionadded:: 0.8
        "
60,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,after_request,"Register a function to be run after each request.

        Your function must take one parameter, an instance of
        :attr:`response_class` and return a new response object or the
        same (see :meth:`process_response`).

        As of Flask 0.7 this function might not be executed at the end of the
        request in case an unhandled exception occurred.
        "
61,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,teardown_request,"Register a function to be run at the end of each request,
        regardless of whether there was an exception or not.  These functions
        are executed when the request context is popped, even if not an
        actual request was performed.

        Example::

            ctx = app.test_request_context()
            ctx.push()
            ...
            ctx.pop()

        When ``ctx.pop()`` is executed in the above example, the teardown
        functions are called just before the request context moves from the
        stack of active contexts.  This becomes relevant if you are using
        such constructs in tests.

        Generally teardown functions must take every necessary step to avoid
        that they will fail.  If they do execute code that might fail they
        will have to surround the execution of these code by try/except
        statements and log occurring errors.

        When a teardown function was called because of an exception it will
        be passed an error object.

        The return values of teardown functions are ignored.

        .. admonition:: Debug Note

           In debug mode Flask will not tear down a request on an exception
           immediately.  Instead it will keep it alive so that the interactive
           debugger can still access it.  This behavior can be controlled
           by the ``PRESERVE_CONTEXT_ON_EXCEPTION`` configuration variable.
        "
62,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,teardown_appcontext,"Registers a function to be called when the application context
        ends.  These functions are typically also called when the request
        context is popped.

        Example::

            ctx = app.app_context()
            ctx.push()
            ...
            ctx.pop()

        When ``ctx.pop()`` is executed in the above example, the teardown
        functions are called just before the app context moves from the
        stack of active contexts.  This becomes relevant if you are using
        such constructs in tests.

        Since a request context typically also manages an application
        context it would also be called when you pop a request context.

        When a teardown function was called because of an unhandled exception
        it will be passed an error object. If an :meth:`errorhandler` is
        registered, it will handle the exception and the teardown will not
        receive it.

        The return values of teardown functions are ignored.

        .. versionadded:: 0.9
        "
63,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,context_processor,Registers a template context processor function.
64,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,shell_context_processor,"Registers a shell context processor function.

        .. versionadded:: 0.11
        "
65,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,url_value_preprocessor,"Register a URL value preprocessor function for all view
        functions in the application. These functions will be called before the
        :meth:`before_request` functions.

        The function can modify the values captured from the matched url before
        they are passed to the view. For example, this can be used to pop a
        common language code value and place it in ``g`` rather than pass it to
        every view.

        The function is passed the endpoint name and values dict. The return
        value is ignored.
        "
66,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,url_defaults,"Callback function for URL defaults for all view functions of the
        application.  It's called with the endpoint and values and should
        update the values passed in place.
        "
67,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,_find_error_handler,"Return a registered error handler for an exception in this order:
        blueprint handler for a specific code, app handler for a specific code,
        blueprint handler for an exception class, app handler for an exception
        class, or ``None`` if a suitable handler is not found.
        "
68,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,handle_http_exception,"Handles an HTTP exception.  By default this will invoke the
        registered error handlers and fall back to returning the
        exception as response.

        .. versionadded:: 0.3
        "
69,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,trap_http_exception,"Checks if an HTTP exception should be trapped or not.  By default
        this will return ``False`` for all exceptions except for a bad request
        key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It
        also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.

        This is called for all HTTP exceptions raised by a view function.
        If it returns ``True`` for any exception the error handler for this
        exception is not called and it shows up as regular exception in the
        traceback.  This is helpful for debugging implicitly raised HTTP
        exceptions.

        .. versionchanged:: 1.0
            Bad request errors are not trapped by default in debug mode.

        .. versionadded:: 0.8
        "
70,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,handle_user_exception,"This method is called whenever an exception occurs that should be
        handled.  A special case are
        :class:`~werkzeug.exception.HTTPException`\s which are forwarded by
        this function to the :meth:`handle_http_exception` method.  This
        function will either return a response value or reraise the
        exception with the same traceback.

        .. versionchanged:: 1.0
            Key errors raised from request data like ``form`` show the bad
            key in debug mode rather than a generic bad request message.

        .. versionadded:: 0.7
        "
71,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,handle_exception,"Default exception handling that kicks in when an exception
        occurs that is not caught.  In debug mode the exception will
        be re-raised immediately, otherwise it is logged and the handler
        for a 500 internal server error is used.  If no such handler
        exists, a default 500 internal server error message is displayed.

        .. versionadded:: 0.3
        "
72,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,log_exception,"Logs an exception.  This is called by :meth:`handle_exception`
        if debugging is disabled and right before the handler is called.
        The default implementation logs the exception as error on the
        :attr:`logger`.

        .. versionadded:: 0.8
        "
73,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,raise_routing_exception,"Exceptions that are recording during routing are reraised with
        this method.  During debug we are not reraising redirect requests
        for non ``GET``, ``HEAD``, or ``OPTIONS`` requests and we're raising
        a different error instead to help debug situations.

        :internal:
        "
74,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,dispatch_request,"Does the request dispatching.  Matches the URL and returns the
        return value of the view or error handler.  This does not have to
        be a response object.  In order to convert the return value to a
        proper response object, call :func:`make_response`.

        .. versionchanged:: 0.7
           This no longer does the exception handling, this code was
           moved to the new :meth:`full_dispatch_request`.
        "
75,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,full_dispatch_request,"Dispatches the request and on top of that performs request
        pre and postprocessing as well as HTTP exception catching and
        error handling.

        .. versionadded:: 0.7
        "
76,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,finalize_request,"Given the return value from a view function this finalizes
        the request by converting it into a response and invoking the
        postprocessing functions.  This is invoked for both normal
        request dispatching as well as error handlers.

        Because this means that it might be called as a result of a
        failure a special safe mode is available which can be enabled
        with the `from_error_handler` flag.  If enabled, failures in
        response processing will be logged and otherwise ignored.

        :internal:
        "
77,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,try_trigger_before_first_request_functions,"Called before each request and will ensure that it triggers
        the :attr:`before_first_request_funcs` and only exactly once per
        application instance (which means process usually).

        :internal:
        "
78,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,make_default_options_response,"This method is called to create the default ``OPTIONS`` response.
        This can be changed through subclassing to change the default
        behavior of ``OPTIONS`` responses.

        .. versionadded:: 0.7
        "
79,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,should_ignore_error,"This is called to figure out if an error should be ignored
        or not as far as the teardown system is concerned.  If this
        function returns ``True`` then the teardown handlers will not be
        passed the error.

        .. versionadded:: 0.10
        "
80,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,make_response,"Convert the return value from a view function to an instance of
        :attr:`response_class`.

        :param rv: the return value from the view function. The view function
            must return a response. Returning ``None``, or the view ending
            without returning, is not allowed. The following types are allowed
            for ``view_rv``:

            ``str`` (``unicode`` in Python 2)
                A response object is created with the string encoded to UTF-8
                as the body.

            ``bytes`` (``str`` in Python 2)
                A response object is created with the bytes as the body.

            ``tuple``
                Either ``(body, status, headers)``, ``(body, status)``, or
                ``(body, headers)``, where ``body`` is any of the other types
                allowed here, ``status`` is a string or an integer, and
                ``headers`` is a dictionary or a list of ``(key, value)``
                tuples. If ``body`` is a :attr:`response_class` instance,
                ``status`` overwrites the exiting value and ``headers`` are
                extended.

            :attr:`response_class`
                The object is returned unchanged.

            other :class:`~werkzeug.wrappers.Response` class
                The object is coerced to :attr:`response_class`.

            :func:`callable`
                The function is called as a WSGI application. The result is
                used to create a response object.

        .. versionchanged:: 0.9
           Previously a tuple was interpreted as the arguments for the
           response object.
        "
81,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,create_url_adapter,"Creates a URL adapter for the given request. The URL adapter
        is created at a point where the request context is not yet set
        up so the request is passed explicitly.

        .. versionadded:: 0.6

        .. versionchanged:: 0.9
           This can now also be called without a request object when the
           URL adapter is created for the application context.

        .. versionchanged:: 1.0
            :data:`SERVER_NAME` no longer implicitly enables subdomain
            matching. Use :attr:`subdomain_matching` instead.
        "
82,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,inject_url_defaults,"Injects the URL defaults for the given endpoint directly into
        the values dictionary passed.  This is used internally and
        automatically called on URL building.

        .. versionadded:: 0.7
        "
83,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,handle_url_build_error,"Handle :class:`~werkzeug.routing.BuildError` on :meth:`url_for`.
        "
84,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,preprocess_request,"Called before the request is dispatched. Calls
        :attr:`url_value_preprocessors` registered with the app and the
        current blueprint (if any). Then calls :attr:`before_request_funcs`
        registered with the app and the blueprint.

        If any :meth:`before_request` handler returns a non-None value, the
        value is handled as if it was the return value from the view, and
        further request handling is stopped.
        "
85,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,process_response,"Can be overridden in order to modify the response object
        before it's sent to the WSGI server.  By default this will
        call all the :meth:`after_request` decorated functions.

        .. versionchanged:: 0.5
           As of Flask 0.5 the functions registered for after request
           execution are called in reverse order of registration.

        :param response: a :attr:`response_class` object.
        :return: a new response object or the same, has to be an
                 instance of :attr:`response_class`.
        "
86,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,do_teardown_request,"Called after the request is dispatched and the response is
        returned, right before the request context is popped.

        This calls all functions decorated with
        :meth:`teardown_request`, and :meth:`Blueprint.teardown_request`
        if a blueprint handled the request. Finally, the
        :data:`request_tearing_down` signal is sent.

        This is called by
        :meth:`RequestContext.pop() <flask.ctx.RequestContext.pop>`,
        which may be delayed during testing to maintain access to
        resources.

        :param exc: An unhandled exception raised while dispatching the
            request. Detected from the current exception information if
            not passed. Passed to each teardown function.

        .. versionchanged:: 0.9
            Added the ``exc`` argument.
        "
87,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,do_teardown_appcontext,"Called right before the application context is popped.

        When handling a request, the application context is popped
        after the request context. See :meth:`do_teardown_request`.

        This calls all functions decorated with
        :meth:`teardown_appcontext`. Then the
        :data:`appcontext_tearing_down` signal is sent.

        This is called by
        :meth:`AppContext.pop() <flask.ctx.AppContext.pop>`.

        .. versionadded:: 0.9
        "
88,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,app_context,"Create an :class:`~flask.ctx.AppContext`. Use as a ``with``
        block to push the context, which will make :data:`current_app`
        point at this application.

        An application context is automatically pushed by
        :meth:`RequestContext.push() <flask.ctx.RequestContext.push>`
        when handling a request, and when running a CLI command. Use
        this to manually create a context outside of these situations.

        ::

            with app.app_context():
                init_db()

        See :doc:`/appcontext`.

        .. versionadded:: 0.9
        "
89,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,request_context,"Create a :class:`~flask.ctx.RequestContext` representing a
        WSGI environment. Use a ``with`` block to push the context,
        which will make :data:`request` point at this request.

        See :doc:`/reqcontext`.

        Typically you should not call this from your own code. A request
        context is automatically pushed by the :meth:`wsgi_app` when
        handling a request. Use :meth:`test_request_context` to create
        an environment and context instead of this method.

        :param environ: a WSGI environment
        "
90,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,test_request_context,"Create a :class:`~flask.ctx.RequestContext` for a WSGI
        environment created from the given values. This is mostly useful
        during testing, where you may want to run a function that uses
        request data without dispatching a full request.

        See :doc:`/reqcontext`.

        Use a ``with`` block to push the context, which will make
        :data:`request` point at the request for the created
        environment. ::

            with test_request_context(...):
                generate_report()

        When using the shell, it may be easier to push and pop the
        context manually to avoid indentation. ::

            ctx = app.test_request_context(...)
            ctx.push()
            ...
            ctx.pop()

        Takes the same arguments as Werkzeug's
        :class:`~werkzeug.test.EnvironBuilder`, with some defaults from
        the application. See the linked Werkzeug docs for most of the
        available arguments. Flask-specific behavior is listed here.

        :param path: URL path being requested.
        :param base_url: Base URL where the app is being served, which
            ``path`` is relative to. If not given, built from
            :data:`PREFERRED_URL_SCHEME`, ``subdomain``,
            :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.
        :param subdomain: Subdomain name to append to
            :data:`SERVER_NAME`.
        :param url_scheme: Scheme to use instead of
            :data:`PREFERRED_URL_SCHEME`.
        :param data: The request body, either as a string or a dict of
            form keys and values.
        :param json: If given, this is serialized as JSON and passed as
            ``data``. Also defaults ``content_type`` to
            ``application/json``.
        :param args: other positional arguments passed to
            :class:`~werkzeug.test.EnvironBuilder`.
        :param kwargs: other keyword arguments passed to
            :class:`~werkzeug.test.EnvironBuilder`.
        "
91,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,wsgi_app,"The actual WSGI application. This is not implemented in
        :meth:`__call__` so that middlewares can be applied without
        losing a reference to the app object. Instead of doing this::

            app = MyMiddleware(app)

        It's a better idea to do this instead::

            app.wsgi_app = MyMiddleware(app.wsgi_app)

        Then you still have the original application object around and
        can continue to call methods on it.

        .. versionchanged:: 0.7
            Teardown events for the request and app contexts are called
            even if an unhandled error occurs. Other events may not be
            called depending on when an error occurs during dispatch.
            See :ref:`callbacks-and-errors`.

        :param environ: A WSGI environment.
        :param start_response: A callable accepting a status code,
            a list of headers, and an optional exception context to
            start the response.
        "
92,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\app.py,__call__,"The WSGI server calls the Flask application object as the
        WSGI application. This calls :meth:`wsgi_app` which can be
        wrapped to applying middleware."
93,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,add_url_rule,"A helper method to register a rule (and optionally a view function)
        to the application.  The endpoint is automatically prefixed with the
        blueprint's name.
        "
94,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,record,"Registers a function that is called when the blueprint is
        registered on the application.  This function is called with the
        state as argument as returned by the :meth:`make_setup_state`
        method.
        "
95,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,record_once,"Works like :meth:`record` but wraps the function in another
        function that will ensure the function is only called once.  If the
        blueprint is registered a second time on the application, the
        function passed is not called.
        "
96,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,make_setup_state,"Creates an instance of :meth:`~flask.blueprints.BlueprintSetupState`
        object that is later passed to the register callback functions.
        Subclasses can override this to return a subclass of the setup state.
        "
97,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,register,"Called by :meth:`Flask.register_blueprint` to register all views
        and callbacks registered on the blueprint with the application. Creates
        a :class:`.BlueprintSetupState` and calls each :meth:`record` callback
        with it.

        :param app: The application this blueprint is being registered with.
        :param options: Keyword arguments forwarded from
            :meth:`~Flask.register_blueprint`.
        :param first_registration: Whether this is the first time this
            blueprint has been registered on the application.
        "
98,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,route,"Like :meth:`Flask.route` but for a blueprint.  The endpoint for the
        :func:`url_for` function is prefixed with the name of the blueprint.
        "
99,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,add_url_rule,"Like :meth:`Flask.add_url_rule` but for a blueprint.  The endpoint for
        the :func:`url_for` function is prefixed with the name of the blueprint.
        "
100,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,endpoint,"Like :meth:`Flask.endpoint` but for a blueprint.  This does not
        prefix the endpoint with the blueprint name, this has to be done
        explicitly by the user of this method.  If the endpoint is prefixed
        with a `.` it will be registered to the current blueprint, otherwise
        it's an application independent endpoint.
        "
101,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_template_filter,"Register a custom template filter, available application wide.  Like
        :meth:`Flask.template_filter` but for a blueprint.

        :param name: the optional name of the filter, otherwise the
                     function name will be used.
        "
102,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,add_app_template_filter,"Register a custom template filter, available application wide.  Like
        :meth:`Flask.add_template_filter` but for a blueprint.  Works exactly
        like the :meth:`app_template_filter` decorator.

        :param name: the optional name of the filter, otherwise the
                     function name will be used.
        "
103,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_template_test,"Register a custom template test, available application wide.  Like
        :meth:`Flask.template_test` but for a blueprint.

        .. versionadded:: 0.10

        :param name: the optional name of the test, otherwise the
                     function name will be used.
        "
104,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,add_app_template_test,"Register a custom template test, available application wide.  Like
        :meth:`Flask.add_template_test` but for a blueprint.  Works exactly
        like the :meth:`app_template_test` decorator.

        .. versionadded:: 0.10

        :param name: the optional name of the test, otherwise the
                     function name will be used.
        "
105,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_template_global,"Register a custom template global, available application wide.  Like
        :meth:`Flask.template_global` but for a blueprint.

        .. versionadded:: 0.10

        :param name: the optional name of the global, otherwise the
                     function name will be used.
        "
106,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,add_app_template_global,"Register a custom template global, available application wide.  Like
        :meth:`Flask.add_template_global` but for a blueprint.  Works exactly
        like the :meth:`app_template_global` decorator.

        .. versionadded:: 0.10

        :param name: the optional name of the global, otherwise the
                     function name will be used.
        "
107,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,before_request,"Like :meth:`Flask.before_request` but for a blueprint.  This function
        is only executed before each request that is handled by a function of
        that blueprint.
        "
108,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,before_app_request,"Like :meth:`Flask.before_request`.  Such a function is executed
        before each request, even if outside of a blueprint.
        "
109,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,before_app_first_request,"Like :meth:`Flask.before_first_request`.  Such a function is
        executed before the first request to the application.
        "
110,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,after_request,"Like :meth:`Flask.after_request` but for a blueprint.  This function
        is only executed after each request that is handled by a function of
        that blueprint.
        "
111,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,after_app_request,"Like :meth:`Flask.after_request` but for a blueprint.  Such a function
        is executed after each request, even if outside of the blueprint.
        "
112,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,teardown_request,"Like :meth:`Flask.teardown_request` but for a blueprint.  This
        function is only executed when tearing down requests handled by a
        function of that blueprint.  Teardown request functions are executed
        when the request context is popped, even when no actual request was
        performed.
        "
113,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,teardown_app_request,"Like :meth:`Flask.teardown_request` but for a blueprint.  Such a
        function is executed when tearing down each request, even if outside of
        the blueprint.
        "
114,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,context_processor,"Like :meth:`Flask.context_processor` but for a blueprint.  This
        function is only executed for requests handled by a blueprint.
        "
115,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_context_processor,"Like :meth:`Flask.context_processor` but for a blueprint.  Such a
        function is executed each request, even if outside of the blueprint.
        "
116,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_errorhandler,"Like :meth:`Flask.errorhandler` but for a blueprint.  This
        handler is used for all requests, even if outside of the blueprint.
        "
117,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,url_value_preprocessor,"Registers a function as URL value preprocessor for this
        blueprint.  It's called before the view functions are called and
        can modify the url values provided.
        "
118,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,url_defaults,"Callback function for URL defaults for this blueprint.  It's called
        with the endpoint and values and should update the values passed
        in place.
        "
119,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_url_value_preprocessor,"Same as :meth:`url_value_preprocessor` but application wide.
        "
120,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,app_url_defaults,"Same as :meth:`url_defaults` but application wide.
        "
121,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,errorhandler,"Registers an error handler that becomes active for this blueprint
        only.  Please be aware that routing does not happen local to a
        blueprint so an error handler for 404 usually is not handled by
        a blueprint unless it is caused inside a view function.  Another
        special case is the 500 internal server error which is always looked
        up from the application.

        Otherwise works as the :meth:`~flask.Flask.errorhandler` decorator
        of the :class:`~flask.Flask` object.
        "
122,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\blueprints.py,register_error_handler,"Non-decorator version of the :meth:`errorhandler` error attach
        function, akin to the :meth:`~flask.Flask.register_error_handler`
        application-wide function of the :class:`~flask.Flask` object but
        for error handlers limited to this blueprint.

        .. versionadded:: 0.11
        "
123,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,find_best_app,"Given a module instance this tries to find the best possible
    application in the module or raises an exception.
    "
124,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,call_factory,"Takes an app factory, a ``script_info` object and  optionally a tuple
    of arguments. Checks for the existence of a script_info argument and calls
    the app_factory depending on that and the arguments provided.
    "
125,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,_called_with_wrong_args,"Check whether calling a function raised a ``TypeError`` because
    the call failed or because something in the factory raised the
    error.

    :param factory: the factory function that was called
    :return: true if the call failed
    "
126,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,find_app_by_string,"Checks if the given string is a variable name or a function. If it is a
    function, it checks for specified arguments and whether it takes a
    ``script_info`` argument and calls the function with the appropriate
    arguments.
    "
127,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,prepare_import,"Given a filename this will try to calculate the python path, add it
    to the search path and return the actual module name that is expected.
    "
128,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,with_appcontext,"Wraps a callback so that it's guaranteed to be executed with the
    script's application context.  If callbacks are registered directly
    to the ``app.cli`` object then they are wrapped with this function
    by default unless it's disabled.
    "
129,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,_path_is_ancestor,"Take ``other`` and remove the length of ``path`` from it. Then join it
    to ``path``. If it is the original value, ``path`` is an ancestor of
    ``other``."
130,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,load_dotenv,"Load ""dotenv"" files in order of precedence to set environment variables.

    If an env var is already set it is not overwritten, so earlier files in the
    list are preferred over later files.

    Changes the current working directory to the location of the first file
    found, with the assumption that it is in the top level project directory
    and will be where the Python path should import local packages from.

    This is a no-op if `python-dotenv`_ is not installed.

    .. _python-dotenv: https://github.com/theskumar/python-dotenv#readme

    :param path: Load the file at this location instead of searching.
    :return: ``True`` if a file was loaded.

    .. versionadded:: 1.0
    "
131,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,show_server_banner,"Show extra startup messages the first time the server is run,
    ignoring the reloader.
    "
132,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,_validate_key,"The ``--key`` option must be specified when ``--cert`` is a file.
    Modifies the ``cert`` param to be a ``(cert, key)`` pair if needed.
    "
133,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,run_command,"Run a local development server.

    This server is for development purposes only. It does not provide
    the stability, security, or performance of production WSGI servers.

    The reloader and debugger are enabled by default if
    FLASK_ENV=development or FLASK_DEBUG=1.
    "
134,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,shell_command,"Runs an interactive Python shell in the context of a given
    Flask application.  The application will populate the default
    namespace of this shell according to it's configuration.

    This is useful for executing small snippets of management code
    without having to manually configure the application.
    "
135,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,routes_command,Show all registered routes with endpoints and methods.
136,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,load_app,"Loads the Flask app (if not yet loaded) and returns it.  Calling
        this multiple times will just result in the already loaded app to
        be returned.
        "
137,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,command,"This works exactly like the method of the same name on a regular
        :class:`click.Group` but it wraps callbacks in :func:`with_appcontext`
        unless it's disabled by passing ``with_appcontext=False``.
        "
138,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\cli.py,group,"This works exactly like the method of the same name on a regular
        :class:`click.Group` but it defaults the group class to
        :class:`AppGroup`.
        "
139,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\config.py,from_envvar,"Loads a configuration from an environment variable pointing to
        a configuration file.  This is basically just a shortcut with nicer
        error messages for this line of code::

            app.config.from_pyfile(os.environ['YOURAPPLICATION_SETTINGS'])

        :param variable_name: name of the environment variable
        :param silent: set to ``True`` if you want silent failure for missing
                       files.
        :return: bool. ``True`` if able to load config, ``False`` otherwise.
        "
140,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\config.py,from_pyfile,"Updates the values in the config from a Python file.  This function
        behaves as if the file was imported as module with the
        :meth:`from_object` function.

        :param filename: the filename of the config.  This can either be an
                         absolute filename or a filename relative to the
                         root path.
        :param silent: set to ``True`` if you want silent failure for missing
                       files.

        .. versionadded:: 0.7
           `silent` parameter.
        "
141,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\config.py,from_object,"Updates the values from the given object.  An object can be of one
        of the following two types:

        -   a string: in this case the object with that name will be imported
        -   an actual object reference: that object is used directly

        Objects are usually either modules or classes. :meth:`from_object`
        loads only the uppercase attributes of the module/class. A ``dict``
        object will not work with :meth:`from_object` because the keys of a
        ``dict`` are not attributes of the ``dict`` class.

        Example of module-based configuration::

            app.config.from_object('yourapplication.default_config')
            from yourapplication import default_config
            app.config.from_object(default_config)

        You should not use this function to load the actual configuration but
        rather configuration defaults.  The actual config should be loaded
        with :meth:`from_pyfile` and ideally from a location not within the
        package because the package might be installed system wide.

        See :ref:`config-dev-prod` for an example of class-based configuration
        using :meth:`from_object`.

        :param obj: an import name or object
        "
142,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\config.py,from_json,"Updates the values in the config from a JSON file. This function
        behaves as if the JSON object was a dictionary and passed to the
        :meth:`from_mapping` function.

        :param filename: the filename of the JSON file.  This can either be an
                         absolute filename or a filename relative to the
                         root path.
        :param silent: set to ``True`` if you want silent failure for missing
                       files.

        .. versionadded:: 0.11
        "
143,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\config.py,from_mapping,"Updates the config like :meth:`update` ignoring items with non-upper
        keys.

        .. versionadded:: 0.11
        "
144,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\config.py,get_namespace,"Returns a dictionary containing a subset of configuration options
        that match the specified namespace/prefix. Example usage::

            app.config['IMAGE_STORE_TYPE'] = 'fs'
            app.config['IMAGE_STORE_PATH'] = '/var/app/images'
            app.config['IMAGE_STORE_BASE_URL'] = 'http://img.website.com'
            image_store_config = app.config.get_namespace('IMAGE_STORE_')

        The resulting dictionary `image_store_config` would look like::

            {
                'type': 'fs',
                'path': '/var/app/images',
                'base_url': 'http://img.website.com'
            }

        This is often useful when configuration options map directly to
        keyword arguments in functions or class constructors.

        :param namespace: a configuration namespace
        :param lowercase: a flag indicating if the keys of the resulting
                          dictionary should be lowercase
        :param trim_namespace: a flag indicating if the keys of the resulting
                          dictionary should not include the namespace

        .. versionadded:: 0.11
        "
145,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,after_this_request,"Executes a function after this request.  This is useful to modify
    response objects.  The function is passed the response object and has
    to return the same or a new one.

    Example::

        @app.route('/')
        def index():
            @after_this_request
            def add_header(response):
                response.headers['X-Foo'] = 'Parachute'
                return response
            return 'Hello World!'

    This is more useful if a function other than the view function wants to
    modify a response.  For instance think of a decorator that wants to add
    some headers without converting the return value into a response object.

    .. versionadded:: 0.9
    "
146,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,copy_current_request_context,"A helper function that decorates a function to retain the current
    request context.  This is useful when working with greenlets.  The moment
    the function is decorated a copy of the request context is created and
    then pushed when the function is called.  The current session is also
    included in the copied request context.

    Example::

        import gevent
        from flask import copy_current_request_context

        @app.route('/')
        def index():
            @copy_current_request_context
            def do_some_work():
                # do some work here, it can access flask.request or
                # flask.session like you would otherwise in the view function.
                ...
            gevent.spawn(do_some_work)
            return 'Regular response'

    .. versionadded:: 0.10
    "
147,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,has_request_context,"If you have code that wants to test if a request context is there or
    not this function can be used.  For instance, you may want to take advantage
    of request information if the request object is available, but fail
    silently if it is unavailable.

    ::

        class User(db.Model):

            def __init__(self, username, remote_addr=None):
                self.username = username
                if remote_addr is None and has_request_context():
                    remote_addr = request.remote_addr
                self.remote_addr = remote_addr

    Alternatively you can also just test any of the context bound objects
    (such as :class:`request` or :class:`g` for truthness)::

        class User(db.Model):

            def __init__(self, username, remote_addr=None):
                self.username = username
                if remote_addr is None and request:
                    remote_addr = request.remote_addr
                self.remote_addr = remote_addr

    .. versionadded:: 0.7
    "
148,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,has_app_context,"Works like :func:`has_request_context` but for the application
    context.  You can also just do a boolean check on the
    :data:`current_app` object instead.

    .. versionadded:: 0.9
    "
149,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,get,"Get an attribute by name, or a default value. Like
        :meth:`dict.get`.

        :param name: Name of attribute to get.
        :param default: Value to return if the attribute is not present.

        .. versionadded:: 0.10
        "
150,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,pop,"Get and remove an attribute by name. Like :meth:`dict.pop`.

        :param name: Name of attribute to pop.
        :param default: Value to return if the attribute is not present,
            instead of raise a ``KeyError``.

        .. versionadded:: 0.11
        "
151,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,setdefault,"Get the value of an attribute if it is present, otherwise
        set and return a default value. Like :meth:`dict.setdefault`.

        :param name: Name of attribute to get.
        :param: default: Value to set and return if the attribute is not
            present.

        .. versionadded:: 0.11
        "
152,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,push,Binds the app context to the current context.
153,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,pop,Pops the app context.
154,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,copy,"Creates a copy of this request context with the same request object.
        This can be used to move a request context to a different greenlet.
        Because the actual request object is the same this cannot be used to
        move a request context to a different thread unless access to the
        request object is locked.

        .. versionadded:: 0.10

        .. versionchanged:: 1.1
           The current session object is used instead of reloading the original
           data. This prevents `flask.session` pointing to an out-of-date object.
        "
155,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,match_request,"Can be overridden by a subclass to hook into the matching
        of the request.
        "
156,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,push,Binds the request context to the current context.
157,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\ctx.py,pop,"Pops the request context and unbinds it by doing that.  This will
        also trigger the execution of functions registered by the
        :meth:`~flask.Flask.teardown_request` decorator.

        .. versionchanged:: 0.9
           Added the `exc` argument.
        "
158,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\debughelpers.py,attach_enctype_error_multidict,"Since Flask 0.8 we're monkeypatching the files object in case a
    request is detected that does not use multipart form data but the files
    object is accessed.
    "
159,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\debughelpers.py,explain_template_loading_attempts,This should help developers understand what failed
160,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_env,"Get the environment the app is running in, indicated by the
    :envvar:`FLASK_ENV` environment variable. The default is
    ``'production'``.
    "
161,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_debug_flag,"Get whether debug mode should be enabled for the app, indicated
    by the :envvar:`FLASK_DEBUG` environment variable. The default is
    ``True`` if :func:`.get_env` returns ``'development'``, or ``False``
    otherwise.
    "
162,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_load_dotenv,"Get whether the user has disabled loading dotenv files by setting
    :envvar:`FLASK_SKIP_DOTENV`. The default is ``True``, load the
    files.

    :param default: What to return if the env var isn't set.
    "
163,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,_endpoint_from_view_func,"Internal helper that returns the default endpoint for a given
    function.  This always is the function name.
    "
164,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,stream_with_context,"Request contexts disappear when the response is started on the server.
    This is done for efficiency reasons and to make it less likely to encounter
    memory leaks with badly written WSGI middlewares.  The downside is that if
    you are using streamed responses, the generator cannot access request bound
    information any more.

    This function however can help you keep the context around for longer::

        from flask import stream_with_context, request, Response

        @app.route('/stream')
        def streamed_response():
            @stream_with_context
            def generate():
                yield 'Hello '
                yield request.args['name']
                yield '!'
            return Response(generate())

    Alternatively it can also be used around a specific generator::

        from flask import stream_with_context, request, Response

        @app.route('/stream')
        def streamed_response():
            def generate():
                yield 'Hello '
                yield request.args['name']
                yield '!'
            return Response(stream_with_context(generate()))

    .. versionadded:: 0.9
    "
165,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,make_response,"Sometimes it is necessary to set additional headers in a view.  Because
    views do not have to return response objects but can return a value that
    is converted into a response object by Flask itself, it becomes tricky to
    add headers to it.  This function can be called instead of using a return
    and you will get a response object which you can use to attach headers.

    If view looked like this and you want to add a new header::

        def index():
            return render_template('index.html', foo=42)

    You can now do something like this::

        def index():
            response = make_response(render_template('index.html', foo=42))
            response.headers['X-Parachutes'] = 'parachutes are cool'
            return response

    This function accepts the very same arguments you can return from a
    view function.  This for example creates a response with a 404 error
    code::

        response = make_response(render_template('not_found.html'), 404)

    The other use case of this function is to force the return value of a
    view function into a response which is helpful with view
    decorators::

        response = make_response(view_function())
        response.headers['X-Parachutes'] = 'parachutes are cool'

    Internally this function does the following things:

    -   if no arguments are passed, it creates a new response argument
    -   if one argument is passed, :meth:`flask.Flask.make_response`
        is invoked with it.
    -   if more than one argument is passed, the arguments are passed
        to the :meth:`flask.Flask.make_response` function as tuple.

    .. versionadded:: 0.6
    "
166,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,url_for,"Generates a URL to the given endpoint with the method provided.

    Variable arguments that are unknown to the target endpoint are appended
    to the generated URL as query arguments.  If the value of a query argument
    is ``None``, the whole pair is skipped.  In case blueprints are active
    you can shortcut references to the same blueprint by prefixing the
    local endpoint with a dot (``.``).

    This will reference the index function local to the current blueprint::

        url_for('.index')

    For more information, head over to the :ref:`Quickstart <url-building>`.

    To integrate applications, :class:`Flask` has a hook to intercept URL build
    errors through :attr:`Flask.url_build_error_handlers`.  The `url_for`
    function results in a :exc:`~werkzeug.routing.BuildError` when the current
    app does not have a URL for the given endpoint and values.  When it does, the
    :data:`~flask.current_app` calls its :attr:`~Flask.url_build_error_handlers` if
    it is not ``None``, which can return a string to use as the result of
    `url_for` (instead of `url_for`'s default to raise the
    :exc:`~werkzeug.routing.BuildError` exception) or re-raise the exception.
    An example::

        def external_url_handler(error, endpoint, values):
            ""Looks up an external URL when `url_for` cannot build a URL.""
            # This is an example of hooking the build_error_handler.
            # Here, lookup_url is some utility function you've built
            # which looks up the endpoint in some external URL registry.
            url = lookup_url(endpoint, **values)
            if url is None:
                # External lookup did not have a URL.
                # Re-raise the BuildError, in context of original traceback.
                exc_type, exc_value, tb = sys.exc_info()
                if exc_value is error:
                    raise exc_type, exc_value, tb
                else:
                    raise error
            # url_for will use this result, instead of raising BuildError.
            return url

        app.url_build_error_handlers.append(external_url_handler)

    Here, `error` is the instance of :exc:`~werkzeug.routing.BuildError`, and
    `endpoint` and `values` are the arguments passed into `url_for`.  Note
    that this is for building URLs outside the current application, and not for
    handling 404 NotFound errors.

    .. versionadded:: 0.10
       The `_scheme` parameter was added.

    .. versionadded:: 0.9
       The `_anchor` and `_method` parameters were added.

    .. versionadded:: 0.9
       Calls :meth:`Flask.handle_build_error` on
       :exc:`~werkzeug.routing.BuildError`.

    :param endpoint: the endpoint of the URL (name of the function)
    :param values: the variable arguments of the URL rule
    :param _external: if set to ``True``, an absolute URL is generated. Server
      address can be changed via ``SERVER_NAME`` configuration variable which
      defaults to `localhost`.
    :param _scheme: a string specifying the desired URL scheme. The `_external`
      parameter must be set to ``True`` or a :exc:`ValueError` is raised. The default
      behavior uses the same scheme as the current request, or
      ``PREFERRED_URL_SCHEME`` from the :ref:`app configuration <config>` if no
      request context is available. As of Werkzeug 0.10, this also can be set
      to an empty string to build protocol-relative URLs.
    :param _anchor: if provided this is added as anchor to the URL.
    :param _method: if provided this explicitly specifies an HTTP method.
    "
167,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_template_attribute,"Loads a macro (or variable) a template exports.  This can be used to
    invoke a macro from within Python code.  If you for example have a
    template named :file:`_cider.html` with the following contents:

    .. sourcecode:: html+jinja

       {% macro hello(name) %}Hello {{ name }}!{% endmacro %}

    You can access this from Python code like this::

        hello = get_template_attribute('_cider.html', 'hello')
        return hello('World')

    .. versionadded:: 0.2

    :param template_name: the name of the template
    :param attribute: the name of the variable of macro to access
    "
168,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,flash,"Flashes a message to the next request.  In order to remove the
    flashed message from the session and to display it to the user,
    the template has to call :func:`get_flashed_messages`.

    .. versionchanged:: 0.3
       `category` parameter added.

    :param message: the message to be flashed.
    :param category: the category for the message.  The following values
                     are recommended: ``'message'`` for any kind of message,
                     ``'error'`` for errors, ``'info'`` for information
                     messages and ``'warning'`` for warnings.  However any
                     kind of string can be used as category.
    "
169,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_flashed_messages,"Pulls all flashed messages from the session and returns them.
    Further calls in the same request to the function will return
    the same messages.  By default just the messages are returned,
    but when `with_categories` is set to ``True``, the return value will
    be a list of tuples in the form ``(category, message)`` instead.

    Filter the flashed messages to one or more categories by providing those
    categories in `category_filter`.  This allows rendering categories in
    separate html blocks.  The `with_categories` and `category_filter`
    arguments are distinct:

    * `with_categories` controls whether categories are returned with message
      text (``True`` gives a tuple, where ``False`` gives just the message text).
    * `category_filter` filters the messages down to only those matching the
      provided categories.

    See :ref:`message-flashing-pattern` for examples.

    .. versionchanged:: 0.3
       `with_categories` parameter added.

    .. versionchanged:: 0.9
        `category_filter` parameter added.

    :param with_categories: set to ``True`` to also receive categories.
    :param category_filter: whitelist of categories to limit return values
    "
170,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,send_file,"Sends the contents of a file to the client.  This will use the
    most efficient method available and configured.  By default it will
    try to use the WSGI server's file_wrapper support.  Alternatively
    you can set the application's :attr:`~Flask.use_x_sendfile` attribute
    to ``True`` to directly emit an ``X-Sendfile`` header.  This however
    requires support of the underlying webserver for ``X-Sendfile``.

    By default it will try to guess the mimetype for you, but you can
    also explicitly provide one.  For extra security you probably want
    to send certain files as attachment (HTML for instance).  The mimetype
    guessing requires a `filename` or an `attachment_filename` to be
    provided.

    ETags will also be attached automatically if a `filename` is provided. You
    can turn this off by setting `add_etags=False`.

    If `conditional=True` and `filename` is provided, this method will try to
    upgrade the response stream to support range requests.  This will allow
    the request to be answered with partial content response.

    Please never pass filenames to this function from user sources;
    you should use :func:`send_from_directory` instead.

    .. versionadded:: 0.2

    .. versionadded:: 0.5
       The `add_etags`, `cache_timeout` and `conditional` parameters were
       added.  The default behavior is now to attach etags.

    .. versionchanged:: 0.7
       mimetype guessing and etag support for file objects was
       deprecated because it was unreliable.  Pass a filename if you are
       able to, otherwise attach an etag yourself.  This functionality
       will be removed in Flask 1.0

    .. versionchanged:: 0.9
       cache_timeout pulls its default from application config, when None.

    .. versionchanged:: 0.12
       The filename is no longer automatically inferred from file objects. If
       you want to use automatic mimetype and etag support, pass a filepath via
       `filename_or_fp` or `attachment_filename`.

    .. versionchanged:: 0.12
       The `attachment_filename` is preferred over `filename` for MIME-type
       detection.

    .. versionchanged:: 1.0
        UTF-8 filenames, as specified in `RFC 2231`_, are supported.

    .. _RFC 2231: https://tools.ietf.org/html/rfc2231#section-4

    .. versionchanged:: 1.0.3
        Filenames are encoded with ASCII instead of Latin-1 for broader
        compatibility with WSGI servers.

    :param filename_or_fp: the filename of the file to send.
                           This is relative to the :attr:`~Flask.root_path`
                           if a relative path is specified.
                           Alternatively a file object might be provided in
                           which case ``X-Sendfile`` might not work and fall
                           back to the traditional method.  Make sure that the
                           file pointer is positioned at the start of data to
                           send before calling :func:`send_file`.
    :param mimetype: the mimetype of the file if provided. If a file path is
                     given, auto detection happens as fallback, otherwise an
                     error will be raised.
    :param as_attachment: set to ``True`` if you want to send this file with
                          a ``Content-Disposition: attachment`` header.
    :param attachment_filename: the filename for the attachment if it
                                differs from the file's filename.
    :param add_etags: set to ``False`` to disable attaching of etags.
    :param conditional: set to ``True`` to enable conditional responses.

    :param cache_timeout: the timeout in seconds for the headers. When ``None``
                          (default), this value is set by
                          :meth:`~Flask.get_send_file_max_age` of
                          :data:`~flask.current_app`.
    :param last_modified: set the ``Last-Modified`` header to this value,
        a :class:`~datetime.datetime` or timestamp.
        If a file was passed, this overrides its mtime.
    "
171,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,safe_join,"Safely join `directory` and zero or more untrusted `pathnames`
    components.

    Example usage::

        @app.route('/wiki/<path:filename>')
        def wiki_page(filename):
            filename = safe_join(app.config['WIKI_FOLDER'], filename)
            with open(filename, 'rb') as fd:
                content = fd.read()  # Read and process the file content...

    :param directory: the trusted base directory.
    :param pathnames: the untrusted pathnames relative to that directory.
    :raises: :class:`~werkzeug.exceptions.NotFound` if one or more passed
            paths fall out of its boundaries.
    "
172,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,send_from_directory,"Send a file from a given directory with :func:`send_file`.  This
    is a secure way to quickly expose static files from an upload folder
    or something similar.

    Example usage::

        @app.route('/uploads/<path:filename>')
        def download_file(filename):
            return send_from_directory(app.config['UPLOAD_FOLDER'],
                                       filename, as_attachment=True)

    .. admonition:: Sending files and Performance

       It is strongly recommended to activate either ``X-Sendfile`` support in
       your webserver or (if no authentication happens) to tell the webserver
       to serve files for the given path on its own without calling into the
       web application for improved performance.

    .. versionadded:: 0.5

    :param directory: the directory where all the files are stored.
    :param filename: the filename relative to that directory to
                     download.
    :param options: optional keyword arguments that are directly
                    forwarded to :func:`send_file`.
    "
173,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_root_path,"Returns the path to a package or cwd if that cannot be found.  This
    returns the path of a package or the folder that contains a module.

    Not to be confused with the package path returned by :func:`find_package`.
    "
174,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,_matching_loader_thinks_module_is_package,"Given the loader that loaded a module and the module this function
    attempts to figure out if the given module is actually a package.
    "
175,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,find_package,"Finds a package and returns the prefix (or None if the package is
    not installed) as well as the folder that contains the package or
    module as a tuple.  The package path returned is the module that would
    have to be added to the pythonpath in order to make it possible to
    import the module.  The prefix is the path below which a UNIX like
    folder structure exists (lib, share etc.).
    "
176,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,total_seconds,"Returns the total seconds from a timedelta object.

    :param timedelta td: the timedelta to be converted in seconds

    :returns: number of seconds
    :rtype: int
    "
177,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,is_ip,"Determine if the given string is an IP address.

    Python 2 on Windows doesn't provide ``inet_pton``, so this only
    checks IPv4 addresses in that environment.

    :param value: value to check
    :type value: str

    :return: True if string is an IP address
    :rtype: bool
    "
178,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,has_static_folder,"This is ``True`` if the package bound object's container has a
        folder for static files.

        .. versionadded:: 0.5
        "
179,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,jinja_loader,"The Jinja loader for this package bound object.

        .. versionadded:: 0.5
        "
180,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,get_send_file_max_age,"Provides default cache_timeout for the :func:`send_file` functions.

        By default, this function returns ``SEND_FILE_MAX_AGE_DEFAULT`` from
        the configuration of :data:`~flask.current_app`.

        Static file functions such as :func:`send_from_directory` use this
        function, and :func:`send_file` calls this function on
        :data:`~flask.current_app` when the given cache_timeout is ``None``. If a
        cache_timeout is given in :func:`send_file`, that timeout is used;
        otherwise, this method is called.

        This allows subclasses to change the behavior when sending files based
        on the filename.  For example, to set the cache timeout for .js files
        to 60 seconds::

            class MyFlask(flask.Flask):
                def get_send_file_max_age(self, name):
                    if name.lower().endswith('.js'):
                        return 60
                    return flask.Flask.get_send_file_max_age(self, name)

        .. versionadded:: 0.9
        "
181,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,send_static_file,"Function used internally to send static files from the static
        folder to the browser.

        .. versionadded:: 0.5
        "
182,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\helpers.py,open_resource,"Opens a resource from the application's resource folder.  To see
        how this works, consider the following folder structure::

            /myapplication.py
            /schema.sql
            /static
                /style.css
            /templates
                /layout.html
                /index.html

        If you want to open the :file:`schema.sql` file you would do the
        following::

            with app.open_resource('schema.sql') as f:
                contents = f.read()
                do_something_with(contents)

        :param resource: the name of the resource.  To access resources within
                         subfolders use forward slashes as separator.
        :param mode: resource file opening mode, default is 'rb'.
        "
183,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,__init__,Create a tagger for the given serializer.
184,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,check,Check if the given value should be tagged by this tag.
185,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,to_json,"Convert the Python object to an object that is a valid JSON type.
        The tag will be added later."
186,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,to_python,"Convert the JSON representation back to the correct type. The tag
        will already be removed."
187,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,tag,"Convert the value to a valid JSON type and add the tag structure
        around it."
188,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,register,"Register a new tag with this serializer.

        :param tag_class: tag class to register. Will be instantiated with this
            serializer instance.
        :param force: overwrite an existing tag. If false (default), a
            :exc:`KeyError` is raised.
        :param index: index to insert the new tag in the tag order. Useful when
            the new tag is a special case of an existing tag. If ``None``
            (default), the tag is appended to the end of the order.

        :raise KeyError: if the tag key is already registered and ``force`` is
            not true.
        "
189,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,tag,Convert a value to a tagged representation if necessary.
190,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,untag,Convert a tagged representation back to the original type.
191,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,dumps,Tag the value and dump it to a compact JSON string.
192,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\tag.py,loads,Load data from a JSON string and deserialized any tagged objects.
193,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,_dump_arg_defaults,Inject default arguments for dump functions.
194,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,_load_arg_defaults,Inject default arguments for load functions.
195,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,detect_encoding,"Detect which UTF codec was used to encode the given bytes.

    The latest JSON standard (:rfc:`8259`) suggests that only UTF-8 is
    accepted. Older documents allowed 8, 16, or 32. 16 and 32 can be big
    or little endian. Some editors or libraries may prepend a BOM.

    :param data: Bytes in unknown UTF encoding.
    :return: UTF encoding name
    "
196,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,dumps,"Serialize ``obj`` to a JSON formatted ``str`` by using the application's
    configured encoder (:attr:`~flask.Flask.json_encoder`) if there is an
    application on the stack.

    This function can return ``unicode`` strings or ascii-only bytestrings by
    default which coerce into unicode strings automatically.  That behavior by
    default is controlled by the ``JSON_AS_ASCII`` configuration variable
    and can be overridden by the simplejson ``ensure_ascii`` parameter.
    "
197,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,dump,Like :func:`dumps` but writes into a file object.
198,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,loads,"Unserialize a JSON object from a string ``s`` by using the application's
    configured decoder (:attr:`~flask.Flask.json_decoder`) if there is an
    application on the stack.
    "
199,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,load,"Like :func:`loads` but reads from a file object.
    "
200,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,htmlsafe_dumps,"Works exactly like :func:`dumps` but is safe for use in ``<script>``
    tags.  It accepts the same arguments and returns a JSON string.  Note that
    this is available in templates through the ``|tojson`` filter which will
    also mark the result as safe.  Due to how this function escapes certain
    characters this is safe even if used outside of ``<script>`` tags.

    The following characters are escaped in strings:

    -   ``<``
    -   ``>``
    -   ``&``
    -   ``'``

    This makes it safe to embed such strings in any place in HTML with the
    notable exception of double quoted attributes.  In that case single
    quote your attributes or HTML escape it in addition.

    .. versionchanged:: 0.10
       This function's return value is now always safe for HTML usage, even
       if outside of script tags or if used in XHTML.  This rule does not
       hold true when using this function in HTML attributes that are double
       quoted.  Always single quote attributes if you use the ``|tojson``
       filter.  Alternatively use ``|tojson|forceescape``.
    "
201,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,htmlsafe_dump,Like :func:`htmlsafe_dumps` but writes into a file object.
202,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,jsonify,"This function wraps :func:`dumps` to add a few enhancements that make
    life easier.  It turns the JSON output into a :class:`~flask.Response`
    object with the :mimetype:`application/json` mimetype.  For convenience, it
    also converts multiple arguments into an array or multiple keyword arguments
    into a dict.  This means that both ``jsonify(1,2,3)`` and
    ``jsonify([1,2,3])`` serialize to ``[1,2,3]``.

    For clarity, the JSON serialization behavior has the following differences
    from :func:`dumps`:

    1. Single argument: Passed straight through to :func:`dumps`.
    2. Multiple arguments: Converted to an array before being passed to
       :func:`dumps`.
    3. Multiple keyword arguments: Converted to a dict before being passed to
       :func:`dumps`.
    4. Both args and kwargs: Behavior undefined and will throw an exception.

    Example usage::

        from flask import jsonify

        @app.route('/_get_current_user')
        def get_current_user():
            return jsonify(username=g.user.username,
                           email=g.user.email,
                           id=g.user.id)

    This will send a JSON response like this to the browser::

        {
            ""username"": ""admin"",
            ""email"": ""admin@localhost"",
            ""id"": 42
        }


    .. versionchanged:: 0.11
       Added support for serializing top-level arrays. This introduces a
       security risk in ancient browsers. See :ref:`json-security` for details.

    This function's response will be pretty printed if the
    ``JSONIFY_PRETTYPRINT_REGULAR`` config parameter is set to True or the
    Flask app is running in debug mode. Compressed (not pretty) formatting
    currently means no indents and no spaces after separators.

    .. versionadded:: 0.2
    "
203,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\json\__init__.py,default,"Implement this method in a subclass such that it returns a
        serializable object for ``o``, or calls the base implementation (to
        raise a :exc:`TypeError`).

        For example, to support arbitrary iterators, you could implement
        default like this::

            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                return JSONEncoder.default(self, o)
        "
204,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\logging.py,wsgi_errors_stream,"Find the most appropriate error stream for the application. If a request
    is active, log to ``wsgi.errors``, otherwise use ``sys.stderr``.

    If you configure your own :class:`logging.StreamHandler`, you may want to
    use this for the stream. If you are using file or dict configuration and
    can't import this directly, you can refer to it as
    ``ext://flask.logging.wsgi_errors_stream``.
    "
205,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\logging.py,has_level_handler,"Check if there is a handler in the logging chain that will handle the
    given logger's :meth:`effective level <~logging.Logger.getEffectiveLevel>`.
    "
206,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\logging.py,create_logger,"Get the ``'flask.app'`` logger and configure it if needed.

    When :attr:`~flask.Flask.debug` is enabled, set the logger level to
    :data:`logging.DEBUG` if it is not set.

    If there is no handler for the logger's effective level, add a
    :class:`~logging.StreamHandler` for
    :func:`~flask.logging.wsgi_errors_stream` with a basic format.
    "
207,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,permanent,This reflects the ``'_permanent'`` key in the dict.
208,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,make_null_session,"Creates a null session which acts as a replacement object if the
        real session support could not be loaded due to a configuration
        error.  This mainly aids the user experience because the job of the
        null session is to still support lookup without complaining but
        modifications are answered with a helpful error message of what
        failed.

        This creates an instance of :attr:`null_session_class` by default.
        "
209,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,is_null_session,"Checks if a given object is a null session.  Null sessions are
        not asked to be saved.

        This checks if the object is an instance of :attr:`null_session_class`
        by default.
        "
210,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,get_cookie_domain,"Returns the domain that should be set for the session cookie.

        Uses ``SESSION_COOKIE_DOMAIN`` if it is configured, otherwise
        falls back to detecting the domain based on ``SERVER_NAME``.

        Once detected (or if not set at all), ``SESSION_COOKIE_DOMAIN`` is
        updated to avoid re-running the logic.
        "
211,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,get_cookie_path,"Returns the path for which the cookie should be valid.  The
        default implementation uses the value from the ``SESSION_COOKIE_PATH``
        config var if it's set, and falls back to ``APPLICATION_ROOT`` or
        uses ``/`` if it's ``None``.
        "
212,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,get_cookie_httponly,"Returns True if the session cookie should be httponly.  This
        currently just returns the value of the ``SESSION_COOKIE_HTTPONLY``
        config var.
        "
213,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,get_cookie_secure,"Returns True if the cookie should be secure.  This currently
        just returns the value of the ``SESSION_COOKIE_SECURE`` setting.
        "
214,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,get_cookie_samesite,"Return ``'Strict'`` or ``'Lax'`` if the cookie should use the
        ``SameSite`` attribute. This currently just returns the value of
        the :data:`SESSION_COOKIE_SAMESITE` setting.
        "
215,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,get_expiration_time,"A helper method that returns an expiration date for the session
        or ``None`` if the session is linked to the browser session.  The
        default implementation returns now + the permanent session
        lifetime configured on the application.
        "
216,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,should_set_cookie,"Used by session backends to determine if a ``Set-Cookie`` header
        should be set for this session cookie for this response. If the session
        has been modified, the cookie is set. If the session is permanent and
        the ``SESSION_REFRESH_EACH_REQUEST`` config is true, the cookie is
        always set.

        This check is usually skipped if the session was deleted.

        .. versionadded:: 0.11
        "
217,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,open_session,"This method has to be implemented and must either return ``None``
        in case the loading failed because of a configuration error or an
        instance of a session object which implements a dictionary like
        interface + the methods and attributes on :class:`SessionMixin`.
        "
218,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\sessions.py,save_session,"This is called for actual sessions returned by :meth:`open_session`
        at the end of the request.  This is still called during a request
        context so if you absolutely need access to the request you can do
        that.
        "
219,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\templating.py,_default_template_ctx_processor,"Default template context processor.  Injects `request`,
    `session` and `g`.
    "
220,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\templating.py,_render,Renders the template and fires the signal
221,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\templating.py,render_template,"Renders a template from the template folder with the given
    context.

    :param template_name_or_list: the name of the template to be
                                  rendered, or an iterable with template names
                                  the first one existing will be rendered
    :param context: the variables that should be available in the
                    context of the template.
    "
222,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\templating.py,render_template_string,"Renders a template from the given template source string
    with the given context. Template variables will be autoescaped.

    :param source: the source code of the template to be
                   rendered
    :param context: the variables that should be available in the
                    context of the template.
    "
223,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\testing.py,make_test_environ_builder,"Create a :class:`~werkzeug.test.EnvironBuilder`, taking some
    defaults from the application.

    :param app: The Flask application to configure the environment from.
    :param path: URL path being requested.
    :param base_url: Base URL where the app is being served, which
        ``path`` is relative to. If not given, built from
        :data:`PREFERRED_URL_SCHEME`, ``subdomain``,
        :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.
    :param subdomain: Subdomain name to append to :data:`SERVER_NAME`.
    :param url_scheme: Scheme to use instead of
        :data:`PREFERRED_URL_SCHEME`.
    :param json: If given, this is serialized as JSON and passed as
        ``data``. Also defaults ``content_type`` to
        ``application/json``.
    :param args: other positional arguments passed to
        :class:`~werkzeug.test.EnvironBuilder`.
    :param kwargs: other keyword arguments passed to
        :class:`~werkzeug.test.EnvironBuilder`.
    "
224,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\testing.py,session_transaction,"When used in combination with a ``with`` statement this opens a
        session transaction.  This can be used to modify the session that
        the test client uses.  Once the ``with`` block is left the session is
        stored back.

        ::

            with client.session_transaction() as session:
                session['value'] = 42

        Internally this is implemented by going through a temporary test
        request context and since session handling could depend on
        request variables this function accepts the same arguments as
        :meth:`~flask.Flask.test_request_context` which are directly
        passed through.
        "
225,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\testing.py,invoke,"Invokes a CLI command in an isolated environment. See
        :meth:`CliRunner.invoke <click.testing.CliRunner.invoke>` for
        full method documentation. See :ref:`testing-cli` for examples.

        If the ``obj`` argument is not given, passes an instance of
        :class:`~flask.cli.ScriptInfo` that knows how to load the Flask
        app being tested.

        :param cli: Command object to invoke. Default is the app's
            :attr:`~flask.app.Flask.cli` group.
        :param args: List of strings to invoke the command with.

        :return: a :class:`~click.testing.Result` object.
        "
226,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\views.py,dispatch_request,"Subclasses have to override this method to implement the
        actual view function code.  This method is called with all
        the arguments from the URL rule.
        "
227,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\views.py,as_view,"Converts the class into an actual view function that can be used
        with the routing system.  Internally this generates a function on the
        fly which will instantiate the :class:`View` on each request and call
        the :meth:`dispatch_request` method on it.

        The arguments passed to :meth:`as_view` are forwarded to the
        constructor of the class.
        "
228,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,is_json,"Check if the mimetype indicates JSON data, either
        :mimetype:`application/json` or :mimetype:`application/*+json`.

        .. versionadded:: 0.11
        "
229,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,json,"This will contain the parsed JSON data if the mimetype indicates
        JSON (:mimetype:`application/json`, see :meth:`is_json`), otherwise it
        will be ``None``.
        "
230,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,get_json,"Parse and return the data as JSON. If the mimetype does not
        indicate JSON (:mimetype:`application/json`, see
        :meth:`is_json`), this returns ``None`` unless ``force`` is
        true. If parsing fails, :meth:`on_json_loading_failed` is called
        and its return value is used as the return value.

        :param force: Ignore the mimetype and always try to parse JSON.
        :param silent: Silence parsing errors and return ``None``
            instead.
        :param cache: Store the parsed JSON to return for subsequent
            calls.
        "
231,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,on_json_loading_failed,"Called if :meth:`get_json` parsing fails and isn't silenced. If
        this method returns a value, it is used as the return value for
        :meth:`get_json`. The default implementation raises a
        :class:`BadRequest` exception.

        .. versionchanged:: 0.10
           Raise a :exc:`BadRequest` error instead of returning an error
           message as JSON. If you want that behavior you can add it by
           subclassing.

        .. versionadded:: 0.8
        "
232,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,max_content_length,Read-only view of the ``MAX_CONTENT_LENGTH`` config key.
233,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,endpoint,"The endpoint that matched the request.  This in combination with
        :attr:`view_args` can be used to reconstruct the same or a
        modified URL.  If an exception happened when matching, this will
        be ``None``.
        "
234,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,blueprint,The name of the current blueprint
235,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\wrappers.py,max_cookie_size,"Read-only view of the :data:`MAX_COOKIE_SIZE` config key.

        See :attr:`~werkzeug.wrappers.BaseResponse.max_cookie_size` in
        Werkzeug's docs.
        "
236,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\flask\_compat.py,with_metaclass,Create a base class with a metaclass.
237,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\conftest.py,_standard_os_environ,"Set up ``os.environ`` at the start of the test session to have
    standard values. Returns a list of operations that is used by
    :func:`._reset_os_environ` after each test.
    "
238,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\conftest.py,_reset_os_environ,"Reset ``os.environ`` to the standard environ after each test,
    in case a test changed something without cleaning up.
    "
239,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\conftest.py,limit_loader,"Patch pkgutil.get_loader to give loader without get_filename or archive.

    This provides for tests where a system has custom loaders, e.g. Google App
    Engine's HardenedModulesHook, which have neither the `get_filename` method
    nor the `archive` attribute.

    This fixture will run the testcase twice, once with and once without the
    limitation/mock.
    "
240,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\conftest.py,modules_tmpdir,A tmpdir added to sys.path.
241,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\conftest.py,site_packages,Create a fake site-packages.
242,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\conftest.py,install_egg,"Generate egg from package name inside base and put the egg into
    sys.path."
243,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_cli_name,Make sure the CLI object's name is the app's name and not the app itself
244,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_find_best_app,Test if `find_best_app` behaves as expected with different combinations of input.
245,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_prepare_import,"Expect the correct path to be set and the correct import and app names
    to be returned.

    :func:`prepare_exec_for_file` has a side effect where the parent directory
    of the given import is added to :data:`sys.path`. This is reset after the
    test runs.
    "
246,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_get_version,Test of get_version.
247,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_scriptinfo,Test of ScriptInfo.
248,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_with_appcontext,Test of with_appcontext.
249,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_appgroup,Test of with_appcontext.
250,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_flaskgroup,Test FlaskGroup.
251,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_flaskgroup_debug,Test FlaskGroup debug flag behavior.
252,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_cli.py,test_print_exceptions,Print the stacktrace if the CLI.
253,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_helpers.py,test_jsonify_basic_types,Test jsonify with basic types.
254,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_helpers.py,test_jsonify_dicts,Test jsonify with dicts and kwargs unpacking.
255,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_helpers.py,test_jsonify_arrays,Test jsonify of lists and args unpacking.
256,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_helpers.py,test_jsonify_date_types,Test jsonify with datetime.date and datetime.datetime types.
257,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_helpers.py,test_jsonify_aware_datetimes,Test if aware datetime.datetime objects are converted into GMT.
258,flask,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\flask\tests\test_helpers.py,test_jsonify_uuid_types,Test jsonify with uuid.UUID types
index,project,file,func_name,func_doc
1,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\addition_rnn.py,__init__,"Initialize character table.

        # Arguments
            chars: Characters that can appear in the input.
        "
2,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\addition_rnn.py,encode,"One-hot encode given string C.

        # Arguments
            C: string, to be encoded.
            num_rows: Number of rows in the returned one-hot encoding. This is
                used to keep the # of rows for each data the same.
        "
3,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\addition_rnn.py,decode,"Decode the given vector or 2D array to their character output.

        # Arguments
            x: A vector or a 2D array of probabilities or one-hot representations;
                or a vector of character indices (used with `calc_argmax=False`).
            calc_argmax: Whether to find the character index with maximum
                probability, defaults to `True`.
        "
4,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\babi_memnn.py,tokenize,"Return the tokens of a sentence including punctuation.

    >>> tokenize('Bob dropped the apple. Where is the apple?')
    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']
    "
5,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\babi_memnn.py,parse_stories,"Parse stories provided in the bAbi tasks format

    If only_supporting is true, only the sentences
    that support the answer are kept.
    "
6,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\babi_memnn.py,get_stories,"Given a file name, read the file,
    retrieve the stories,
    and then convert the sentences into a single story.

    If max_length is supplied,
    any stories longer than max_length tokens will be discarded.
    "
7,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\babi_rnn.py,tokenize,"Return the tokens of a sentence including punctuation.

    >>> tokenize('Bob dropped the apple. Where is the apple?')
    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']
    "
8,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\babi_rnn.py,parse_stories,"Parse stories provided in the bAbi tasks format

    If only_supporting is true,
    only the sentences that support the answer are kept.
    "
9,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\babi_rnn.py,get_stories,"Given a file name, read the file, retrieve the stories,
    and then convert the sentences into a single story.

    If max_length is supplied,
    any stories longer than max_length tokens will be discarded.
    "
10,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\cifar10_cnn_capsule.py,call,"Following the routing algorithm from Hinton's paper,
        but replace b = b + <u,v> with b = <u,v>.

        This change can improve the feature representation of Capsule.

        However, you can replace
            b = K.batch_dot(outputs, hat_inputs, [2, 3])
        with
            b += K.batch_dot(outputs, hat_inputs, [2, 3])
        to realize a standard routing.
        "
11,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\cifar10_cnn_tfaugment2d.py,augment_2d,"Apply additive augmentation on 2D data.

    # Arguments
      rotation: A float, the degree range for rotation (0 <= rotation < 180),
          e.g. 3 for random image rotation between (-3.0, 3.0).
      horizontal_flip: A boolean, whether to allow random horizontal flip,
          e.g. true for 50% possibility to flip image horizontally.
      vertical_flip: A boolean, whether to allow random vertical flip,
          e.g. true for 50% possibility to flip image vertically.

    # Returns
      input data after augmentation, whose shape is the same as its original.
    "
12,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\cifar10_resnet.py,lr_schedule,"Learning Rate Schedule

    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.

    # Arguments
        epoch (int): The number of epochs

    # Returns
        lr (float32): learning rate
    "
13,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\cifar10_resnet.py,resnet_layer,"2D Convolution-Batch Normalization-Activation stack builder

    # Arguments
        inputs (tensor): input tensor from input image or previous layer
        num_filters (int): Conv2D number of filters
        kernel_size (int): Conv2D square kernel dimensions
        strides (int): Conv2D square stride dimensions
        activation (string): activation name
        batch_normalization (bool): whether to include batch normalization
        conv_first (bool): conv-bn-activation (True) or
            bn-activation-conv (False)

    # Returns
        x (tensor): tensor as input to the next layer
    "
14,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\cifar10_resnet.py,resnet_v1,"ResNet Version 1 Model builder [a]

    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU
    Last ReLU is after the shortcut connection.
    At the beginning of each stage, the feature map size is halved (downsampled)
    by a convolutional layer with strides=2, while the number of filters is
    doubled. Within each stage, the layers have the same number filters and the
    same number of filters.
    Features maps sizes:
    stage 0: 32x32, 16
    stage 1: 16x16, 32
    stage 2:  8x8,  64
    The Number of parameters is approx the same as Table 6 of [a]:
    ResNet20 0.27M
    ResNet32 0.46M
    ResNet44 0.66M
    ResNet56 0.85M
    ResNet110 1.7M

    # Arguments
        input_shape (tensor): shape of input image tensor
        depth (int): number of core convolutional layers
        num_classes (int): number of classes (CIFAR10 has 10)

    # Returns
        model (Model): Keras model instance
    "
15,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\cifar10_resnet.py,resnet_v2,"ResNet Version 2 Model builder [b]

    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as
    bottleneck layer
    First shortcut connection per layer is 1 x 1 Conv2D.
    Second and onwards shortcut connection is identity.
    At the beginning of each stage, the feature map size is halved (downsampled)
    by a convolutional layer with strides=2, while the number of filter maps is
    doubled. Within each stage, the layers have the same number filters and the
    same filter map sizes.
    Features maps sizes:
    conv1  : 32x32,  16
    stage 0: 32x32,  64
    stage 1: 16x16, 128
    stage 2:  8x8,  256

    # Arguments
        input_shape (tensor): shape of input image tensor
        depth (int): number of core convolutional layers
        num_classes (int): number of classes (CIFAR10 has 10)

    # Returns
        model (Model): Keras model instance
    "
16,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\imdb_fasttext.py,create_ngram_set,"
    Extract a set of n-grams from a list of integers.

    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
    {(4, 9), (4, 1), (1, 4), (9, 4)}

    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]
    "
17,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\imdb_fasttext.py,add_ngram,"
    Augment the input list of list (sequences) by appending n-grams values.

    Example: adding bi-gram
    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
    >>> add_ngram(sequences, token_indice, ngram_range=2)
    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]

    Example: adding tri-gram
    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}
    >>> add_ngram(sequences, token_indice, ngram_range=3)
    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]
    "
18,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\lstm_stateful.py,gen_uniform_amp,"Generates uniform random data between
    -amp and +amp
    and of length xn

    Arguments:
        amp: maximum/minimum range of uniform data
        xn: length of series
    "
19,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,wider2net_conv2d,"Get initial weights for a wider conv2d layer with a bigger filters,
    by 'random-padding' or 'net2wider'.

    # Arguments
        teacher_w1: `weight` of conv2d layer to become wider,
          of shape (filters1, num_channel1, kh1, kw1)
        teacher_b1: `bias` of conv2d layer to become wider,
          of shape (filters1, )
        teacher_w2: `weight` of next connected conv2d layer,
          of shape (filters2, num_channel2, kh2, kw2)
        new_width: new `filters` for the wider conv2d layer
        init: initialization algorithm for new weights,
          either 'random-pad' or 'net2wider'
    "
20,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,wider2net_fc,"Get initial weights for a wider fully connected (dense) layer
       with a bigger nout, by 'random-padding' or 'net2wider'.

    # Arguments
        teacher_w1: `weight` of fc layer to become wider,
          of shape (nin1, nout1)
        teacher_b1: `bias` of fc layer to become wider,
          of shape (nout1, )
        teacher_w2: `weight` of next connected fc layer,
          of shape (nin2, nout2)
        new_width: new `nout` for the wider fc layer
        init: initialization algorithm for new weights,
          either 'random-pad' or 'net2wider'
    "
21,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,deeper2net_conv2d,"Get initial weights for a deeper conv2d layer by net2deeper'.

    # Arguments
        teacher_w: `weight` of previous conv2d layer,
          of shape (kh, kw, num_channel, filters)
    "
22,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,copy_weights,"Copy weights from teacher_model to student_model,
     for layers with names listed in layer_names
    "
23,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,make_teacher_model,"Train and benchmark performance of a simple CNN.
    (0) Teacher model
    "
24,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,make_wider_student_model,"Train a wider student model based on teacher_model,
       with either 'random-pad' (baseline) or 'net2wider'
    "
25,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,make_deeper_student_model,"Train a deeper student model based on teacher_model,
       with either 'random-init' (baseline) or 'net2deeper'
    "
26,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,net2wider_experiment,"Benchmark performances of
    (1) a wider student model with `random_pad` initializer
    (2) a wider student model with `Net2WiderNet` initializer
    "
27,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_net2net.py,net2deeper_experiment,"Benchmark performances of
    (3) a deeper student model with `random_init` initializer
    (4) a deeper student model with `Net2DeeperNet` initializer
    "
28,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_siamese.py,contrastive_loss,"Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    "
29,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_siamese.py,create_pairs,"Positive and negative pair creation.
    Alternates between positive and negative pairs.
    "
30,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_siamese.py,create_base_network,"Base network to be shared (eq. to feature extraction).
    "
31,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_siamese.py,compute_accuracy,"Compute classification accuracy with a fixed threshold on distances.
    "
32,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_siamese.py,accuracy,"Compute classification accuracy with a fixed threshold on distances.
    "
33,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_sklearn_wrapper.py,make_model,"Creates model comprised of 2 convolutional layers followed by dense layers

    dense_layer_sizes: List of layer sizes.
        This list has one number for each layer
    filters: Number of convolutional filters in each convolutional layer
    kernel_size: Convolutional kernel size
    pool_size: Size of pooling area for max pooling
    "
34,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_swwae.py,convresblock,"The proposed residual block from [4].

    Running with elu=True will use ELU nonlinearity and running with
    elu=False will use BatchNorm + RELU nonlinearity.  While ELU's are fast
    due to the fact they do not suffer from BatchNorm overhead, they may
    overfit because they do not offer the stochastic element of the batch
    formation process of BatchNorm, which acts as a good regularizer.

    # Arguments
        x: 4D tensor, the tensor to feed through the block
        nfeats: Integer, number of feature maps for conv layers.
        ksize: Integer, width and height of conv kernels in first convolution.
        nskipped: Integer, number of conv layers for the residual function.
        elu: Boolean, whether to use ELU or BN+RELU.

    # Input shape
        4D tensor with shape:
        `(batch, channels, rows, cols)`

    # Output shape
        4D tensor with shape:
        `(batch, filters, rows, cols)`
    "
35,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\mnist_swwae.py,getwhere," Calculate the 'where' mask that contains switches indicating which
    index contained the max value when MaxPool2D was applied.  Using the
    gradient of the sum is a nice trick to keep everything high level."
36,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\neural_doodle.py,load_mask_labels,"Load both target and style masks.
    A mask image (nr x nc) with m labels/colors will be loaded
    as a 4D boolean tensor:
        (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last'
    "
37,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\neural_doodle.py,region_style_loss,"Calculate style loss between style_image and target_image,
    for one common region specified by their (boolean) masks
    "
38,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\neural_doodle.py,style_loss,"Calculate style loss between style_image and target_image,
    in all regions.
    "
39,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\reuters_mlp_relu_vs_selu.py,create_network,"Generic function to create a fully-connected neural network.

    # Arguments
        n_dense: int > 0. Number of dense layers.
        dense_units: int > 0. Number of dense units per layer.
        dropout: keras.layers.Layer. A dropout layer to apply.
        dropout_rate: 0 <= float <= 1. The rate of dropout.
        kernel_initializer: str. The initializer for the weights.
        optimizer: str/keras.optimizers.Optimizer. The optimizer to use.
        num_classes: int > 0. The number of classes to predict.
        max_words: int > 0. The maximum number of words per data point.

    # Returns
        A Keras model instance (compiled).
    "
40,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\variational_autoencoder.py,sampling,"Reparameterization trick by sampling fr an isotropic unit Gaussian.

    # Arguments:
        args (tensor): mean and log of variance of Q(z|X)

    # Returns:
        z (tensor): sampled latent vector
    "
41,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\variational_autoencoder.py,plot_results,"Plots labels and MNIST digits as function of 2-dim latent vector

    # Arguments:
        models (tuple): encoder and decoder models
        data (tuple): test data and label
        batch_size (int): prediction batch size
        model_name (string): which model is using this function
    "
42,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\variational_autoencoder_deconv.py,sampling,"Reparameterization trick by sampling fr an isotropic unit Gaussian.

    # Arguments:
        args (tensor): mean and log of variance of Q(z|X)

    # Returns:
        z (tensor): sampled latent vector
    "
43,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\examples\variational_autoencoder_deconv.py,plot_results,"Plots labels and MNIST digits as function of 2-dim latent vector

    # Arguments:
        models (tuple): encoder and decoder models
        data (tuple): test data and label
        batch_size (int): prediction batch size
        model_name (string): which model is using this function
    "
44,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,softmax,"Softmax activation function.

    # Arguments
        x: Input tensor.
        axis: Integer, axis along which the softmax normalization is applied.

    # Returns
        Tensor, output of softmax transformation.

    # Raises
        ValueError: In case `dim(x) == 1`.
    "
45,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,elu,"Exponential linear unit.

    # Arguments
        x: Input tensor.
        alpha: A scalar, slope of negative section.

    # Returns
        The exponential linear activation: `x` if `x > 0` and
        `alpha * (exp(x)-1)` if `x < 0`.

    # References
        - [Fast and Accurate Deep Network Learning by Exponential
           Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)
    "
46,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,selu,"Scaled Exponential Linear Unit (SELU).

    SELU is equal to: `scale * elu(x, alpha)`, where alpha and scale
    are pre-defined constants. The values of `alpha` and `scale` are
    chosen so that the mean and variance of the inputs are preserved
    between two consecutive layers as long as the weights are initialized
    correctly (see `lecun_normal` initialization) and the number of inputs
    is ""large enough"" (see references for more information).

    # Arguments
        x: A tensor or variable to compute the activation function for.

    # Returns
       The scaled exponential unit activation: `scale * elu(x, alpha)`.

    # Note
        - To be used together with the initialization ""lecun_normal"".
        - To be used together with the dropout variant ""AlphaDropout"".

    # References
        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)
    "
47,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,softplus,"Softplus activation function.

    # Arguments
        x: Input tensor.

    # Returns
        The softplus activation: `log(exp(x) + 1)`.
    "
48,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,softsign,"Softsign activation function.

    # Arguments
        x: Input tensor.

    # Returns
        The softsign activation: `x / (abs(x) + 1)`.
    "
49,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,relu,"Rectified Linear Unit.

    With default values, it returns element-wise `max(x, 0)`.

    Otherwise, it follows:
    `f(x) = max_value` for `x >= max_value`,
    `f(x) = x` for `threshold <= x < max_value`,
    `f(x) = alpha * (x - threshold)` otherwise.

    # Arguments
        x: Input tensor.
        alpha: float. Slope of the negative part. Defaults to zero.
        max_value: float. Saturation threshold.
        threshold: float. Threshold value for thresholded activation.

    # Returns
        A tensor.
    "
50,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,tanh,"Hyperbolic tangent activation function.
    "
51,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,sigmoid,"Sigmoid activation function.
    "
52,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,hard_sigmoid,"Hard sigmoid activation function.

    Faster to compute than sigmoid activation.

    # Arguments
        x: Input tensor.

    # Returns
        Hard sigmoid activation:

        - `0` if `x < -2.5`
        - `1` if `x > 2.5`
        - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`.
    "
53,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,exponential,"Exponential (base e) activation function.
    "
54,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,linear,"Linear (i.e. identity) activation function.
    "
55,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\activations.py,get,"Get the `identifier` activation function.

    # Arguments
        identifier: None or str, name of the function.

    # Returns
        The activation function, `linear` if `identifier` is None.

    # Raises
        ValueError if unknown identifier
    "
56,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\cntk_backend.py,clear_session,"Reset learning phase flag for cntk backend.
    "
57,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\cntk_backend.py,variable,"Instantiates a variable and returns it.

    # Arguments
        value: Numpy array, initial value of the tensor.
        dtype: Tensor type.
        name: Optional name string for the tensor.
        constraint: Optional projection function to be
            applied to the variable after an optimizer update.

    # Returns
        A variable instance (with Keras metadata included).
    "
58,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\cntk_backend.py,is_placeholder,"Returns whether `x` is a placeholder.

    # Arguments
        x: A candidate placeholder.

    # Returns
        Boolean.
    "
59,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,epsilon,"Returns the value of the fuzz factor used in numeric expressions.

    # Returns
        A float.

    # Example
    ```python
        >>> keras.backend.epsilon()
        1e-07
    ```
    "
60,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,set_epsilon,"Sets the value of the fuzz factor used in numeric expressions.

    # Arguments
        e: float. New value of epsilon.

    # Example
    ```python
        >>> from keras import backend as K
        >>> K.epsilon()
        1e-07
        >>> K.set_epsilon(1e-05)
        >>> K.epsilon()
        1e-05
    ```
    "
61,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,floatx,"Returns the default float type, as a string.
    (e.g. 'float16', 'float32', 'float64').

    # Returns
        String, the current default float type.

    # Example
    ```python
        >>> keras.backend.floatx()
        'float32'
    ```
    "
62,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,set_floatx,"Sets the default float type.

    # Arguments
        floatx: String, 'float16', 'float32', or 'float64'.

    # Example
    ```python
        >>> from keras import backend as K
        >>> K.floatx()
        'float32'
        >>> K.set_floatx('float16')
        >>> K.floatx()
        'float16'
    ```
    "
63,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,cast_to_floatx,"Cast a Numpy array to the default Keras float type.

    # Arguments
        x: Numpy array.

    # Returns
        The same Numpy array, cast to its new type.

    # Example
    ```python
        >>> from keras import backend as K
        >>> K.floatx()
        'float32'
        >>> arr = numpy.array([1.0, 2.0], dtype='float64')
        >>> arr.dtype
        dtype('float64')
        >>> new_arr = K.cast_to_floatx(arr)
        >>> new_arr
        array([ 1.,  2.], dtype=float32)
        >>> new_arr.dtype
        dtype('float32')
    ```
    "
64,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,image_data_format,"Returns the default image data format convention.

    # Returns
        A string, either `'channels_first'` or `'channels_last'`

    # Example
    ```python
        >>> keras.backend.image_data_format()
        'channels_first'
    ```
    "
65,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,set_image_data_format,"Sets the value of the data format convention.

    # Arguments
        data_format: string. `'channels_first'` or `'channels_last'`.

    # Example
    ```python
        >>> from keras import backend as K
        >>> K.image_data_format()
        'channels_first'
        >>> K.set_image_data_format('channels_last')
        >>> K.image_data_format()
        'channels_last'
    ```
    "
66,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,normalize_data_format,"Checks that the value correspond to a valid data format.

    # Arguments
        value: String or None. `'channels_first'` or `'channels_last'`.

    # Returns
        A string, either `'channels_first'` or `'channels_last'`

    # Example
    ```python
        >>> from keras import backend as K
        >>> K.normalize_data_format(None)
        'channels_first'
        >>> K.normalize_data_format('channels_last')
        'channels_last'
    ```

    # Raises
        ValueError: if `value` or the global `data_format` invalid.
    "
67,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,set_image_dim_ordering,"Legacy setter for `image_data_format`.

    # Arguments
        dim_ordering: string. `tf` or `th`.

    # Example
    ```python
        >>> from keras import backend as K
        >>> K.image_data_format()
        'channels_first'
        >>> K.set_image_data_format('channels_last')
        >>> K.image_data_format()
        'channels_last'
    ```

    # Raises
        ValueError: if `dim_ordering` is invalid.
    "
68,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\common.py,image_dim_ordering,"Legacy getter for `image_data_format`.

    # Returns
        string, one of `'th'`, `'tf'`
    "
69,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,get_uid,"Get the uid for the default graph.

    # Arguments
        prefix: An optional prefix of the graph.

    # Returns
        A unique identifier for the graph.
    "
70,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,reset_uids,"Resets graph identifiers.
    "
71,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,clear_session,"Destroys the current TF graph and creates a new one.

    Useful to avoid clutter from old models / layers.
    "
72,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,manual_variable_initialization,"Sets the manual variable initialization flag.

    This boolean flag determines whether
    variables should be initialized
    as they are instantiated (default), or if
    the user should handle the initialization
    (e.g. via `tf.initialize_all_variables()`).

    # Arguments
        value: Python boolean.
    "
73,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,learning_phase,"Returns the learning phase flag.

    The learning phase flag is a bool tensor (0 = test, 1 = train)
    to be passed as input to any Keras function
    that uses a different behavior at train time and test time.

    # Returns
        Learning phase (scalar integer tensor or Python integer).
    "
74,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,set_learning_phase,"Sets the learning phase to a fixed value.

    # Arguments
        value: Learning phase value, either 0 or 1 (integers).

    # Raises
        ValueError: if `value` is neither `0` nor `1`.
    "
75,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,get_session,"Returns the TF session to be used by the backend.

    If a default TensorFlow session is available, we will return it.

    Else, we will return the global Keras session.

    If no global Keras session exists at this point:
    we will create a new global session.

    Note that you can manually set the global session
    via `K.set_session(sess)`.

    # Returns
        A TensorFlow session.
    "
76,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,set_session,"Sets the global TensorFlow session.

    # Arguments
        session: A TF Session.
    "
77,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_get_current_tf_device,"Return explicit device of current context, otherwise returns `None`.

    # Returns
        If the current device scope is explicitly set, it returns a string with
        the device (`CPU` or `GPU`). If the scope is not explicitly set, it will
        return `None`.
    "
78,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_is_current_explicit_device,"Check if the current device is explicitly set on the device type specified.

    # Arguments
        device_type: A string containing `GPU` or `CPU` (case-insensitive).

    # Returns
        A boolean indicating if the current device
        scope is explicitly set on the device type.

    # Raises
        ValueError: If the `device_type` string indicates an unsupported device.
    "
79,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_get_available_gpus,"Get a list of available gpu devices (formatted as strings).

    # Returns
        A list of available GPU devices.
    "
80,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_has_nchw_support,"Check whether the current scope supports NCHW ops.

    TensorFlow does not support NCHW on CPU.
    Therefore we check if we are not explicitly put on
    CPU, and have GPUs available.
    In this case there will be soft-placing on the GPU device.

    # Returns
        bool: if the current scope device placement would support nchw
    "
81,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_to_tensor,"Convert the input `x` to a tensor of type `dtype`.

    # Arguments
        x: An object to be converted (numpy array, list, tensors).
        dtype: The destination type.

    # Returns
        A tensor.
    "
82,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,is_sparse,"Returns whether a tensor is a sparse tensor.

    # Arguments
        tensor: A tensor instance.

    # Returns
        A boolean.

    # Example
    ```python
        >>> from keras import backend as K
        >>> a = K.placeholder((2, 2), sparse=False)
        >>> print(K.is_sparse(a))
        False
        >>> b = K.placeholder((2, 2), sparse=True)
        >>> print(K.is_sparse(b))
        True
    ```
    "
83,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,to_dense,"Converts a sparse tensor into a dense tensor and returns it.

    # Arguments
        tensor: A tensor instance (potentially sparse).

    # Returns
        A dense tensor.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> b = K.placeholder((2, 2), sparse=True)
        >>> print(K.is_sparse(b))
        True
        >>> c = K.to_dense(b)
        >>> print(K.is_sparse(c))
        False
    ```
    "
84,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,variable,"Instantiates a variable and returns it.

    # Arguments
        value: Numpy array, initial value of the tensor.
        dtype: Tensor type.
        name: Optional name string for the tensor.
        constraint: Optional projection function to be
            applied to the variable after an optimizer update.

    # Returns
        A variable instance (with Keras metadata included).

    # Examples
    ```python
        >>> from keras import backend as K
        >>> val = np.array([[1, 2], [3, 4]])
        >>> kvar = K.variable(value=val, dtype='float64', name='example_var')
        >>> K.dtype(kvar)
        'float64'
        >>> print(kvar)
        example_var
        >>> K.eval(kvar)
        array([[ 1.,  2.],
               [ 3.,  4.]])
    ```
    "
85,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,constant,"Creates a constant tensor.

    # Arguments
        value: A constant value (or list)
        dtype: The type of the elements of the resulting tensor.
        shape: Optional dimensions of resulting tensor.
        name: Optional name for the tensor.

    # Returns
        A Constant Tensor.
    "
86,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,is_keras_tensor,"Returns whether `x` is a Keras tensor.

    A ""Keras tensor"" is a tensor that was returned by a Keras layer,
    (`Layer` class) or by `Input`.

    # Arguments
        x: A candidate tensor.

    # Returns
        A boolean: Whether the argument is a Keras tensor.

    # Raises
        ValueError: In case `x` is not a symbolic tensor.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> from keras.layers import Input, Dense
        >>> np_var = numpy.array([1, 2])
        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
        ValueError
        >>> k_var = tf.placeholder('float32', shape=(1,1))
        >>> # A variable indirectly created outside of keras is not a Keras tensor.
        >>> K.is_keras_tensor(k_var)
        False
        >>> keras_var = K.variable(np_var)
        >>> # A variable created with the keras backend is not a Keras tensor.
        >>> K.is_keras_tensor(keras_var)
        False
        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))
        >>> # A placeholder is not a Keras tensor.
        >>> K.is_keras_tensor(keras_placeholder)
        False
        >>> keras_input = Input([10])
        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
        True
        >>> keras_layer_output = Dense(10)(keras_input)
        >>> # Any Keras layer output is a Keras tensor.
        >>> K.is_keras_tensor(keras_layer_output)
        True
    ```
    "
87,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,placeholder,"Instantiates a placeholder tensor and returns it.

    # Arguments
        shape: Shape of the placeholder
            (integer tuple, may include `None` entries).
        ndim: Number of axes of the tensor.
            At least one of {`shape`, `ndim`} must be specified.
            If both are specified, `shape` is used.
        dtype: Placeholder type.
        sparse: Boolean, whether the placeholder should have a sparse type.
        name: Optional name string for the placeholder.

    # Returns
        Tensor instance (with Keras metadata included).

    # Examples
    ```python
        >>> from keras import backend as K
        >>> input_ph = K.placeholder(shape=(2, 4, 5))
        >>> input_ph._keras_shape
        (2, 4, 5)
        >>> input_ph
        <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>
    ```
    "
88,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,is_placeholder,"Returns whether `x` is a placeholder.

    # Arguments
        x: A candidate placeholder.

    # Returns
        Boolean.
    "
89,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,shape,"Returns the symbolic shape of a tensor or variable.

    # Arguments
        x: A tensor or variable.

    # Returns
        A symbolic shape (which is itself a tensor).

    # Examples
    ```python
        # TensorFlow example
        >>> from keras import backend as K
        >>> tf_session = K.get_session()
        >>> val = np.array([[1, 2], [3, 4]])
        >>> kvar = K.variable(value=val)
        >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))
        >>> K.shape(kvar)
        <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>
        >>> K.shape(inputs)
        <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>
        # To get integer shape (Instead, you can use K.int_shape(x))
        >>> K.shape(kvar).eval(session=tf_session)
        array([2, 2], dtype=int32)
        >>> K.shape(inputs).eval(session=tf_session)
        array([2, 4, 5], dtype=int32)
    ```
    "
90,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,int_shape,"Returns the shape of tensor or variable as a tuple of int or None entries.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tuple of integers (or None entries).

    # Examples
    ```python
        >>> from keras import backend as K
        >>> inputs = K.placeholder(shape=(2, 4, 5))
        >>> K.int_shape(inputs)
        (2, 4, 5)
        >>> val = np.array([[1, 2], [3, 4]])
        >>> kvar = K.variable(value=val)
        >>> K.int_shape(kvar)
        (2, 2)
    ```

    {{np_implementation}}
    "
91,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,ndim,"Returns the number of axes in a tensor, as an integer.

    # Arguments
        x: Tensor or variable.

    # Returns
        Integer (scalar), number of axes.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> inputs = K.placeholder(shape=(2, 4, 5))
        >>> val = np.array([[1, 2], [3, 4]])
        >>> kvar = K.variable(value=val)
        >>> K.ndim(inputs)
        3
        >>> K.ndim(kvar)
        2
    ```

    {{np_implementation}}
    "
92,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,dtype,"Returns the dtype of a Keras tensor or variable, as a string.

    # Arguments
        x: Tensor or variable.

    # Returns
        String, dtype of `x`.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> K.dtype(K.placeholder(shape=(2,4,5)))
        'float32'
        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))
        'float32'
        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))
        'float64'
        # Keras variable
        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))
        >>> K.dtype(kvar)
        'float32_ref'
        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
        >>> K.dtype(kvar)
        'float32_ref'
    ```
    "
93,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,eval,"Evaluates the value of a variable.

    # Arguments
        x: A variable.

    # Returns
        A Numpy array.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
        >>> K.eval(kvar)
        array([[ 1.,  2.],
               [ 3.,  4.]], dtype=float32)
    ```
    "
94,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,zeros,"Instantiates an all-zeros variable and returns it.

    # Arguments
        shape: Tuple of integers, shape of returned Keras variable
        dtype: String, data type of returned Keras variable
        name: String, name of returned Keras variable

    # Returns
        A variable (including Keras metadata), filled with `0.0`.
        Note that if `shape` was symbolic, we cannot return a variable,
        and will return a dynamically-shaped tensor instead.

    # Example
    ```python
        >>> from keras import backend as K
        >>> kvar = K.zeros((3,4))
        >>> K.eval(kvar)
        array([[ 0.,  0.,  0.,  0.],
               [ 0.,  0.,  0.,  0.],
               [ 0.,  0.,  0.,  0.]], dtype=float32)
    ```
    "
95,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,ones,"Instantiates an all-ones variable and returns it.

    # Arguments
        shape: Tuple of integers, shape of returned Keras variable.
        dtype: String, data type of returned Keras variable.
        name: String, name of returned Keras variable.

    # Returns
        A Keras variable, filled with `1.0`.
        Note that if `shape` was symbolic, we cannot return a variable,
        and will return a dynamically-shaped tensor instead.

    # Example
    ```python
        >>> from keras import backend as K
        >>> kvar = K.ones((3,4))
        >>> K.eval(kvar)
        array([[ 1.,  1.,  1.,  1.],
               [ 1.,  1.,  1.,  1.],
               [ 1.,  1.,  1.,  1.]], dtype=float32)
    ```
    "
96,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,eye,"Instantiate an identity matrix and returns it.

    # Arguments
        size: Integer, number of rows/columns.
        dtype: String, data type of returned Keras variable.
        name: String, name of returned Keras variable.

    # Returns
        A Keras variable, an identity matrix.

    # Example
    ```python
        >>> from keras import backend as K
        >>> kvar = K.eye(3)
        >>> K.eval(kvar)
        array([[ 1.,  0.,  0.],
               [ 0.,  1.,  0.],
               [ 0.,  0.,  1.]], dtype=float32)
    ```

    "
97,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,zeros_like,"Instantiates an all-zeros variable of the same shape as another tensor.

    # Arguments
        x: Keras variable or Keras tensor.
        dtype: String, dtype of returned Keras variable.
             None uses the dtype of x.
        name: String, name for the variable to create.

    # Returns
        A Keras variable with the shape of x filled with zeros.

    # Example
    ```python
        >>> from keras import backend as K
        >>> kvar = K.variable(np.random.random((2,3)))
        >>> kvar_zeros = K.zeros_like(kvar)
        >>> K.eval(kvar_zeros)
        array([[ 0.,  0.,  0.],
               [ 0.,  0.,  0.]], dtype=float32)
    ```
    "
98,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,ones_like,"Instantiates an all-ones variable of the same shape as another tensor.

    # Arguments
        x: Keras variable or tensor.
        dtype: String, dtype of returned Keras variable.
             None uses the dtype of x.
        name: String, name for the variable to create.

    # Returns
        A Keras variable with the shape of x filled with ones.

    # Example
    ```python
        >>> from keras import backend as K
        >>> kvar = K.variable(np.random.random((2,3)))
        >>> kvar_ones = K.ones_like(kvar)
        >>> K.eval(kvar_ones)
        array([[ 1.,  1.,  1.],
               [ 1.,  1.,  1.]], dtype=float32)
    ```
    "
99,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,identity,"Returns a tensor with the same content as the input tensor.

    # Arguments
        x: The input tensor.
        name: String, name for the variable to create.

    # Returns
        A tensor of the same shape, type and content.
    "
100,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,random_uniform_variable,"Instantiates a variable with values drawn from a uniform distribution.

    # Arguments
        shape: Tuple of integers, shape of returned Keras variable.
        low: Float, lower boundary of the output interval.
        high: Float, upper boundary of the output interval.
        dtype: String, dtype of returned Keras variable.
        name: String, name of returned Keras variable.
        seed: Integer, random seed.

    # Returns
        A Keras variable, filled with drawn samples.

    # Example
    ```python
        # TensorFlow example
        >>> kvar = K.random_uniform_variable((2,3), 0, 1)
        >>> kvar
        <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>
        >>> K.eval(kvar)
        array([[ 0.10940075,  0.10047495,  0.476143  ],
               [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)
    ```
    "
101,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,random_normal_variable,"Instantiates a variable with values drawn from a normal distribution.

    # Arguments
        shape: Tuple of integers, shape of returned Keras variable.
        mean: Float, mean of the normal distribution.
        scale: Float, standard deviation of the normal distribution.
        dtype: String, dtype of returned Keras variable.
        name: String, name of returned Keras variable.
        seed: Integer, random seed.

    # Returns
        A Keras variable, filled with drawn samples.

    # Example
    ```python
        # TensorFlow example
        >>> kvar = K.random_normal_variable((2,3), 0, 1)
        >>> kvar
        <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>
        >>> K.eval(kvar)
        array([[ 1.19591331,  0.68685907, -0.63814116],
               [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)
    ```
    "
102,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,count_params,"Returns the static number of elements in a Keras variable or tensor.

    # Arguments
        x: Keras variable or tensor.

    # Returns
        Integer, the number of elements in `x`, i.e., the product of the
        array's static dimensions.

    # Example
    ```python
        >>> kvar = K.zeros((2,3))
        >>> K.count_params(kvar)
        6
        >>> K.eval(kvar)
        array([[ 0.,  0.,  0.],
               [ 0.,  0.,  0.]], dtype=float32)
    ```
    "
103,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,cast,"Casts a tensor to a different dtype and returns it.

    You can cast a Keras variable but it still returns a Keras tensor.

    # Arguments
        x: Keras tensor (or variable).
        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).

    # Returns
        Keras tensor with dtype `dtype`.

    # Example
    ```python
        >>> from keras import backend as K
        >>> input = K.placeholder((2, 3), dtype='float32')
        >>> input
        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>
        # It doesn't work in-place as below.
        >>> K.cast(input, dtype='float16')
        <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>
        >>> input
        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>
        # you need to assign it.
        >>> input = K.cast(input, dtype='float16')
        >>> input
        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>
    ```
    "
104,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,update,"Update the value of `x` to `new_x`.

    # Arguments
        x: A `Variable`.
        new_x: A tensor of same shape as `x`.

    # Returns
        The variable `x` updated.
    "
105,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,update_add,"Update the value of `x` by adding `increment`.

    # Arguments
        x: A `Variable`.
        increment: A tensor of same shape as `x`.

    # Returns
        The variable `x` updated.
    "
106,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,update_sub,"Update the value of `x` by subtracting `decrement`.

    # Arguments
        x: A `Variable`.
        decrement: A tensor of same shape as `x`.

    # Returns
        The variable `x` updated.
    "
107,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,moving_average_update,"Compute the moving average of a variable.

    # Arguments
        x: A `Variable`.
        value: A tensor with the same shape as `x`.
        momentum: The moving average momentum.

    # Returns
        An operation to update the variable.
    "
108,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,dot,"Multiplies 2 tensors (and/or variables) and returns a *tensor*.

    When attempting to multiply a nD tensor
    with a nD tensor, it reproduces the Theano behavior.
    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A tensor, dot product of `x` and `y`.

    # Examples
    ```python
        # dot product between tensors
        >>> x = K.placeholder(shape=(2, 3))
        >>> y = K.placeholder(shape=(3, 4))
        >>> xy = K.dot(x, y)
        >>> xy
        <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>
    ```

    ```python
        # dot product between tensors
        >>> x = K.placeholder(shape=(32, 28, 3))
        >>> y = K.placeholder(shape=(3, 4))
        >>> xy = K.dot(x, y)
        >>> xy
        <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>
    ```

    ```python
        # Theano-like behavior example
        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
        >>> y = K.ones((4, 3, 5))
        >>> xy = K.dot(x, y)
        >>> K.int_shape(xy)
        (2, 4, 5)
    ```
    "
109,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,batch_dot,"Batchwise dot product.

    `batch_dot` is used to compute dot product of `x` and `y` when
    `x` and `y` are data in batches, i.e. in a shape of
    `(batch_size, :)`.
    `batch_dot` results in a tensor or variable with less dimensions
    than the input. If the number of dimensions is reduced to 1,
    we use `expand_dims` to make sure that ndim is at least 2.

    # Arguments
        x: Keras tensor or variable with `ndim >= 2`.
        y: Keras tensor or variable with `ndim >= 2`.
        axes: int or tuple(int, int). Target dimensions to be reduced.

    # Returns
        A tensor with shape equal to the concatenation of `x`'s shape
        (less the dimension that was summed over) and `y`'s shape
        (less the batch dimension and the dimension that was summed over).
        If the final rank is 1, we reshape it to `(batch_size, 1)`.

    # Examples
        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`
        `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal
        of `x.dot(y.T)`, although we never have to calculate the off-diagonal
        elements.

        Pseudocode:
        ```
        inner_products = []
        for xi, yi in zip(x, y):
            inner_products.append(xi.dot(yi))
        result = stack(inner_products)
        ```

        Shape inference:
        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.
        If `axes` is (1, 2), to find the output shape of resultant tensor,
            loop through each dimension in `x`'s shape and `y`'s shape:

        * `x.shape[0]` : 100 : append to output shape
        * `x.shape[1]` : 20 : do not append to output shape,
            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)
        * `y.shape[0]` : 100 : do not append to output shape,
            always ignore first dimension of `y`
        * `y.shape[1]` : 30 : append to output shape
        * `y.shape[2]` : 20 : do not append to output shape,
            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)
        `output_shape` = `(100, 30)`

    ```python
        >>> x_batch = K.ones(shape=(32, 20, 1))
        >>> y_batch = K.ones(shape=(32, 30, 20))
        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))
        >>> K.int_shape(xy_batch_dot)
        (32, 1, 30)
    ```

    {{np_implementation}}
    "
110,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,transpose,"Transposes a tensor and returns it.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.

    # Examples
    ```python
        >>> var = K.variable([[1, 2, 3], [4, 5, 6]])
        >>> K.eval(var)
        array([[ 1.,  2.,  3.],
               [ 4.,  5.,  6.]], dtype=float32)
        >>> var_transposed = K.transpose(var)
        >>> K.eval(var_transposed)
        array([[ 1.,  4.],
               [ 2.,  5.],
               [ 3.,  6.]], dtype=float32)
    ```

    ```python
        >>> inputs = K.placeholder((2, 3))
        >>> inputs
        <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>
        >>> input_transposed = K.transpose(inputs)
        >>> input_transposed
        <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>

    ```
    "
111,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,gather,"Retrieves the elements of indices `indices` in the tensor `reference`.

    # Arguments
        reference: A tensor.
        indices: An integer tensor of indices.

    # Returns
        A tensor of same type as `reference`.

    {{np_implementation}}
    "
112,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,max,"Maximum value in a tensor.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to find maximum values. If `None` (default), finds the
            maximum over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`,
            the reduced dimension is retained with length 1.

    # Returns
        A tensor with maximum values of `x`.

    {{np_implementation}}
    "
113,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,min,"Minimum value in a tensor.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to find minimum values. If `None` (default), finds the
            minimum over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`,
            the reduced dimension is retained with length 1.

    # Returns
        A tensor with miminum values of `x`.

    {{np_implementation}}
    "
114,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,sum,"Sum of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to sum over. If `None` (default), sums over all
            dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`,
            the reduced dimension is retained with length 1.

    # Returns
        A tensor with sum of `x`.

    {{np_implementation}}
    "
115,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,prod,"Multiplies the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the product. If `None` (default), computes
            the product over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`,
            the reduced dimension is retained with length 1.

    # Returns
        A tensor with the product of elements of `x`.

    {{np_implementation}}
    "
116,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,cumsum,"Cumulative sum of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to compute the sum.

    # Returns
        A tensor of the cumulative sum of values of `x` along `axis`.
    "
117,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,cumprod,"Cumulative product of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to compute the product.

    # Returns
        A tensor of the cumulative product of values of `x` along `axis`.
    "
118,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,var,"Variance of a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the variance. If `None` (default), computes
            the variance over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`,
            the reduced dimension is retained with length 1.

    # Returns
        A tensor with the variance of elements of `x`.
    "
119,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,std,"Standard deviation of a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the standard deviation. If `None` (default),
            computes the standard deviation over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`,
            the reduced dimension is retained with length 1.

    # Returns
        A tensor with the standard deviation of elements of `x`.
    "
120,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,mean,"Mean of a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the mean. If `None` (default), computes
            the mean over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1 for each entry in `axis`. If `keepdims` is `True`,
            the reduced dimensions are retained with length 1.

    # Returns
        A tensor with the mean of elements of `x`.
    "
121,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,any,"Bitwise reduction (logical OR).

    # Arguments
        x: Tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the logical or. If `None` (default), computes
            the logical or over all dimensions.
        keepdims: whether the drop or broadcast the reduction axes.

    # Returns
        A uint8 tensor (0s and 1s).
    "
122,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,all,"Bitwise reduction (logical AND).

    # Arguments
        x: Tensor or variable.
        axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the logical and. If `None` (default), computes
            the logical and over all dimensions.
        keepdims: whether the drop or broadcast the reduction axes.

    # Returns
        A uint8 tensor (0s and 1s).
    "
123,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,argmax,"Returns the index of the maximum value along an axis.

    # Arguments
        x: Tensor or variable.
        axis: axis along which to perform the reduction.

    # Returns
        A tensor.
    "
124,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,argmin,"Returns the index of the minimum value along an axis.

    # Arguments
        x: Tensor or variable.
        axis: axis along which to perform the reduction.

    # Returns
        A tensor.
    "
125,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,square,"Element-wise square.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
126,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,abs,"Element-wise absolute value.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
127,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,sqrt,"Element-wise square root.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
128,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,exp,"Element-wise exponential.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
129,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,log,"Element-wise log.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
130,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,logsumexp,"Computes log(sum(exp(elements across dimensions of a tensor))).

    This function is more numerically stable than log(sum(exp(x))).
    It avoids overflows caused by taking the exp of large inputs and
    underflows caused by taking the log of small inputs.

    # Arguments
        x: A tensor or variable.
        axis: axis: An integer or list of integers in [-rank(x), rank(x)),
            the axes to compute the logsumexp. If `None` (default), computes
            the logsumexp over all dimensions.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`, the reduced dimension is
            retained with length 1.

    # Returns
        The reduced tensor.
    "
131,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,round,"Element-wise rounding to the closest integer.

    In case of tie, the rounding mode used is ""half to even"".

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
132,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,sign,"Element-wise sign.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
133,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,pow,"Element-wise exponentiation.

    # Arguments
        x: Tensor or variable.
        a: Python integer.

    # Returns
        A tensor.
    "
134,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,clip,"Element-wise value clipping.

    # Arguments
        x: Tensor or variable.
        min_value: Python float, integer or tensor.
        max_value: Python float, integer or tensor.

    # Returns
        A tensor.
    "
135,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,equal,"Element-wise equality between two tensors.

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A bool tensor.

    {{np_implementation}}
    "
136,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,not_equal,"Element-wise inequality between two tensors.

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A bool tensor.

    {{np_implementation}}
    "
137,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,greater,"Element-wise truth value of (x > y).

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A bool tensor.

    {{np_implementation}}
    "
138,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,greater_equal,"Element-wise truth value of (x >= y).

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A bool tensor.

    {{np_implementation}}
    "
139,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,less,"Element-wise truth value of (x < y).

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A bool tensor.

    {{np_implementation}}
    "
140,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,less_equal,"Element-wise truth value of (x <= y).

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A bool tensor.

    {{np_implementation}}
    "
141,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,maximum,"Element-wise maximum of two tensors.

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
142,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,minimum,"Element-wise minimum of two tensors.

    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
143,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,sin,"Computes sin of x element-wise.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
144,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,cos,"Computes cos of x element-wise.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tensor.
    "
145,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_regular_normalize_batch_in_training,"Non-fused version of `normalize_batch_in_training`.

    # Arguments
        x: Input tensor or variable.
        gamma: Tensor by which to scale the input.
        beta: Tensor with which to center the input.
        reduction_axes: iterable of integers,
            axes over which to normalize.
        epsilon: Fuzz factor.

    # Returns
        A tuple length of 3, `(normalized_tensor, mean, variance)`.
    "
146,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_broadcast_normalize_batch_in_training,"Non-fused, broadcast version of `normalize_batch_in_training`.

    # Arguments
        x: Input tensor or variable.
        gamma: Tensor by which to scale the input.
        beta: Tensor with which to center the input.
        reduction_axes: iterable of integers,
            axes over which to normalize.
        epsilon: Fuzz factor.

    # Returns
        A tuple length of 3, `(normalized_tensor, mean, variance)`.
    "
147,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_fused_normalize_batch_in_training,"Fused version of `normalize_batch_in_training`.

    # Arguments
        x: Input tensor or variable.
        gamma: Tensor by which to scale the input.
        beta: Tensor with which to center the input.
        reduction_axes: iterable of integers,
            axes over which to normalize.
        epsilon: Fuzz factor.

    # Returns
        A tuple length of 3, `(normalized_tensor, mean, variance)`.
    "
148,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,normalize_batch_in_training,"Computes mean and std for batch then apply batch_normalization on batch.

    # Arguments
        x: Input tensor or variable.
        gamma: Tensor by which to scale the input.
        beta: Tensor with which to center the input.
        reduction_axes: iterable of integers,
            axes over which to normalize.
        epsilon: Fuzz factor.

    # Returns
        A tuple length of 3, `(normalized_tensor, mean, variance)`.
    "
149,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,batch_normalization,"Applies batch normalization on x given mean, var, beta and gamma.

    I.e. returns:
    `output = (x - mean) / sqrt(var + epsilon) * gamma + beta`

    # Arguments
        x: Input tensor or variable.
        mean: Mean of batch.
        var: Variance of batch.
        beta: Tensor with which to center the input.
        gamma: Tensor by which to scale the input.
        axis: Integer, the axis that should be normalized.
            (typically the features axis).
        epsilon: Fuzz factor.

    # Returns
        A tensor.
    "
150,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,concatenate,"Concatenates a list of tensors alongside the specified axis.

    # Arguments
        tensors: list of tensors to concatenate.
        axis: concatenation axis.

    # Returns
        A tensor.
    "
151,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,reshape,"Reshapes a tensor to the specified shape.

    # Arguments
        x: Tensor or variable.
        shape: Target shape tuple.

    # Returns
        A tensor.
    "
152,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,permute_dimensions,"Permutes axes in a tensor.

    # Arguments
        x: Tensor or variable.
        pattern: A tuple of
            dimension indices, e.g. `(0, 2, 1)`.

    # Returns
        A tensor.
    "
153,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,resize_images,"Resizes the images contained in a 4D tensor.

    # Arguments
        x: Tensor or variable to resize.
        height_factor: Positive integer.
        width_factor: Positive integer.
        data_format: string, `""channels_last""` or `""channels_first""`.
        interpolation: A string, one of `nearest` or `bilinear`.

    # Returns
        A tensor.

    # Raises
        ValueError: if `data_format` is
        neither `""channels_last""` or `""channels_first""`.
    "
154,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,resize_volumes,"Resizes the volume contained in a 5D tensor.

    # Arguments
        x: Tensor or variable to resize.
        depth_factor: Positive integer.
        height_factor: Positive integer.
        width_factor: Positive integer.
        data_format: string, `""channels_last""` or `""channels_first""`.

    # Returns
        A tensor.

    # Raises
        ValueError: if `data_format` is
        neither `""channels_last""` or `""channels_first""`.
    "
155,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,repeat_elements,"Repeats the elements of a tensor along an axis, like `np.repeat`.

    If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output
    will have shape `(s1, s2 * rep, s3)`.

    # Arguments
        x: Tensor or variable.
        rep: Python integer, number of times to repeat.
        axis: Axis along which to repeat.

    # Returns
        A tensor.
    "
156,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,repeat,"Repeats a 2D tensor.

    if `x` has shape (samples, dim) and `n` is `2`,
    the output will have shape `(samples, 2, dim)`.

    # Arguments
        x: Tensor or variable.
        n: Python integer, number of times to repeat.

    # Returns
        A tensor.
    "
157,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,arange,"Creates a 1D tensor containing a sequence of integers.

    The function arguments use the same convention as
    Theano's arange: if only one argument is provided,
    it is in fact the ""stop"" argument and ""start"" is 0.

    The default type of the returned tensor is `'int32'` to
    match TensorFlow's default.

    # Arguments
        start: Start value.
        stop: Stop value.
        step: Difference between two successive values.
        dtype: Integer dtype to use.

    # Returns
        An integer tensor.

    "
158,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,tile,"Creates a tensor by tiling `x` by `n`.

    # Arguments
        x: A tensor or variable
        n: A list of integer. The length must be the same as the number of
            dimensions in `x`.

    # Returns
        A tiled tensor.
    "
159,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,flatten,"Flatten a tensor.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor, reshaped into 1-D
    "
160,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,batch_flatten,"Turn a nD tensor into a 2D tensor with same 0th dimension.

    In other words, it flattens each data samples of a batch.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor.
    "
161,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,expand_dims,"Adds a 1-sized dimension at index ""axis"".

    # Arguments
        x: A tensor or variable.
        axis: Position where to add a new axis.

    # Returns
        A tensor with expanded dimensions.
    "
162,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,squeeze,"Removes a 1-dimension from the tensor at index ""axis"".

    # Arguments
        x: A tensor or variable.
        axis: Axis to drop.

    # Returns
        A tensor with the same data as `x` but reduced dimensions.
    "
163,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,temporal_padding,"Pads the middle dimension of a 3D tensor.

    # Arguments
        x: Tensor or variable.
        padding: Tuple of 2 integers, how many zeros to
            add at the start and end of dim 1.

    # Returns
        A padded 3D tensor.
    "
164,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,spatial_2d_padding,"Pads the 2nd and 3rd dimensions of a 4D tensor.

    # Arguments
        x: Tensor or variable.
        padding: Tuple of 2 tuples, padding pattern.
        data_format: string, `""channels_last""` or `""channels_first""`.

    # Returns
        A padded 4D tensor.

    # Raises
        ValueError: if `data_format` is
        neither `""channels_last""` or `""channels_first""`.
    "
165,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,spatial_3d_padding,"Pads 5D tensor with zeros along the depth, height, width dimensions.

    Pads these dimensions with respectively
    ""padding[0]"", ""padding[1]"" and ""padding[2]"" zeros left and right.

    For 'channels_last' data_format,
    the 2nd, 3rd and 4th dimension will be padded.
    For 'channels_first' data_format,
    the 3rd, 4th and 5th dimension will be padded.

    # Arguments
        x: Tensor or variable.
        padding: Tuple of 3 tuples, padding pattern.
        data_format: string, `""channels_last""` or `""channels_first""`.

    # Returns
        A padded 5D tensor.

    # Raises
        ValueError: if `data_format` is
        neither `""channels_last""` or `""channels_first""`.

    "
166,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,stack,"Stacks a list of rank `R` tensors into a rank `R+1` tensor.

    # Arguments
        x: List of tensors.
        axis: Axis along which to perform stacking.

    # Returns
        A tensor.

    {{np_implementation}}
    "
167,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,one_hot,"Computes the one-hot representation of an integer tensor.

    # Arguments
        indices: nD integer tensor of shape
            `(batch_size, dim1, dim2, ... dim(n-1))`
        num_classes: Integer, number of classes to consider.

    # Returns
        (n + 1)D one hot representation of the input
        with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`
    "
168,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,reverse,"Reverses a tensor along the specified axes.

    # Arguments
        x: Tensor to reverse.
        axes: Integer or iterable of integers.
            Axes to reverse.

    # Returns
        A tensor.

    {{np_implementation}}
    "
169,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,slice,"Extracts a slice from a tensor.

    # Arguments
        x: Input tensor.
        start: Integer list/tuple or tensor
            indicating the start indices of the slice
            along each axis.
        size: Integer list/tuple or tensor
            indicating how many dimensions to slice
            along each axis.

    # Returns
        A sliced tensor:
        ```python
        new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]]
        ```

    {{np_implementation}}
    "
170,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,get_value,"Returns the value of a variable.

    # Arguments
        x: input variable.

    # Returns
        A Numpy array.
    "
171,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,batch_get_value,"Returns the value of more than one tensor variable.

    # Arguments
        ops: list of ops to run.

    # Returns
        A list of Numpy arrays.
    "
172,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,set_value,"Sets the value of a variable, from a Numpy array.

    # Arguments
        x: Tensor to set to a new value.
        value: Value to set the tensor to, as a Numpy array
            (of the same shape).
    "
173,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,batch_set_value,"Sets the values of many tensor variables at once.

    # Arguments
        tuples: a list of tuples `(tensor, value)`.
            `value` should be a Numpy array.
    "
174,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,get_variable_shape,"Returns the shape of a variable.

    # Arguments
        x: A variable.

    # Returns
        A tuple of integers.
    "
175,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,print_tensor,"Prints `message` and the tensor value when evaluated.

     Note that `print_tensor` returns a new tensor identical to `x`
     which should be used in the following code. Otherwise the
     print operation is not taken into account during evaluation.

     # Example
     ```python
         >>> x = K.print_tensor(x, message=""x is: "")
     ```

    # Arguments
        x: Tensor to print.
        message: Message to print jointly with the tensor.

    # Returns
        The same tensor `x`, unchanged.
    "
176,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,function,"Instantiates a Keras function.

    # Arguments
        inputs: List of placeholder tensors.
        outputs: List of output tensors.
        updates: List of update ops.
        **kwargs: Passed to `tf.Session.run`.

    # Returns
        Output values as Numpy arrays.

    # Raises
        ValueError: if invalid kwargs are passed in.
    "
177,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,gradients,"Returns the gradients of `loss` w.r.t. `variables`.

    # Arguments
        loss: Scalar tensor to minimize.
        variables: List of variables.

    # Returns
        A gradients tensor.
    "
178,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,stop_gradient,"Returns `variables` but with zero gradient w.r.t. every other variable.

    # Arguments
        variables: tensor or list of tensors to consider constant with respect
            to any other variable.

    # Returns
        A single tensor or a list of tensors (depending on the passed argument)
            that has constant gradient with respect to any other variable.
    "
179,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,rnn,"Iterates over the time dimension of a tensor.

    # Arguments
        step_function:
            Parameters:
                inputs: Tensor with shape (samples, ...) (no time dimension),
                    representing input for the batch of samples at a certain
                    time step.
                states: List of tensors.
            Returns:
                outputs: Tensor with shape (samples, ...) (no time dimension),
                new_states: List of tensors, same length and shapes
                    as 'states'.
        inputs: Tensor of temporal data of shape (samples, time, ...)
            (at least 3D).
        initial_states: Tensor with shape (samples, ...) (no time dimension),
            containing the initial values for the states used in
            the step function.
        go_backwards: Boolean. If True, do the iteration over the time
            dimension in reverse order and return the reversed sequence.
        mask: Binary tensor with shape (samples, time),
            with a zero for every element that is masked.
        constants: A list of constant values passed at each step.
        unroll: Whether to unroll the RNN or to use a symbolic loop
            (`while_loop` or `scan` depending on backend).
        input_length: Static number of timesteps in the input.

    # Returns
        A tuple, `(last_output, outputs, new_states)`.

        last_output: The latest output of the rnn, of shape `(samples, ...)`
        outputs: Tensor with shape `(samples, time, ...)` where each
            entry `outputs[s, t]` is the output of the step function
            at time `t` for sample `s`.
        new_states: List of tensors, latest states returned by
            the step function, of shape `(samples, ...)`.

    # Raises
        ValueError: If input dimension is less than 3.
        ValueError: If `unroll` is `True`
            but input timestep is not a fixed number.
        ValueError: If `mask` is provided (not `None`)
            but states is not provided (`len(states)` == 0).

    {{np_implementation}}
    "
180,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,switch,"Switches between two operations depending on a scalar value.

    Note that both `then_expression` and `else_expression`
    should be symbolic tensors of the *same shape*.

    # Arguments
        condition: tensor (`int` or `bool`).
        then_expression: either a tensor, or a callable that returns a tensor.
        else_expression: either a tensor, or a callable that returns a tensor.

    # Returns
        The selected tensor.

    # Raises
        ValueError: If rank of `condition` is greater than rank of expressions.

    {{np_implementation}}
    "
181,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,in_train_phase,"Selects `x` in train phase, and `alt` otherwise.

    Note that `alt` should have the *same shape* as `x`.

    # Arguments
        x: What to return in train phase
            (tensor or callable that returns a tensor).
        alt: What to return otherwise
            (tensor or callable that returns a tensor).
        training: Optional scalar tensor
            (or Python boolean, or Python integer)
            specifying the learning phase.

    # Returns
        Either `x` or `alt` based on the `training` flag.
        the `training` flag defaults to `K.learning_phase()`.
    "
182,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,in_test_phase,"Selects `x` in test phase, and `alt` otherwise.

    Note that `alt` should have the *same shape* as `x`.

    # Arguments
        x: What to return in test phase
            (tensor or callable that returns a tensor).
        alt: What to return otherwise
            (tensor or callable that returns a tensor).
        training: Optional scalar tensor
            (or Python boolean, or Python integer)
            specifying the learning phase.

    # Returns
        Either `x` or `alt` based on `K.learning_phase`.
    "
183,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,relu,"Rectified linear unit.

    With default values, it returns element-wise `max(x, 0)`.

    Otherwise, it follows:
    `f(x) = max_value` for `x >= max_value`,
    `f(x) = x` for `threshold <= x < max_value`,
    `f(x) = alpha * (x - threshold)` otherwise.

    # Arguments
        x: A tensor or variable.
        alpha: A scalar, slope of negative section (default=`0.`).
        max_value: float. Saturation threshold.
        threshold: float. Threshold value for thresholded activation.

    # Returns
        A tensor.

    {{np_implementation}}
    "
184,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,elu,"Exponential linear unit.

    # Arguments
        x: A tensor or variable to compute the activation function for.
        alpha: A scalar, slope of negative section.

    # Returns
        A tensor.

    {{np_implementation}}
    "
185,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,softmax,"Softmax of a tensor.

    # Arguments
        x: A tensor or variable.
        axis: The dimension softmax would be performed on.
            The default is -1 which indicates the last dimension.

    # Returns
        A tensor.

    {{np_implementation}}
    "
186,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,softplus,"Softplus of a tensor.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
187,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,softsign,"Softsign of a tensor.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
188,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,categorical_crossentropy,"Categorical crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor of the same shape as `output`.
        output: A tensor resulting from a softmax
            (unless `from_logits` is True, in which
            case `output` is expected to be the logits).
        from_logits: Boolean, whether `output` is the
            result of a softmax, or is a tensor of logits.
        axis: Int specifying the channels axis. `axis=-1`
            corresponds to data format `channels_last`,
            and `axis=1` corresponds to data format
            `channels_first`.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `axis` is neither -1 nor one of
            the axes of `output`.
    "
189,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,sparse_categorical_crossentropy,"Categorical crossentropy with integer targets.

    # Arguments
        target: An integer tensor.
        output: A tensor resulting from a softmax
            (unless `from_logits` is True, in which
            case `output` is expected to be the logits).
        from_logits: Boolean, whether `output` is the
            result of a softmax, or is a tensor of logits.
        axis: Int specifying the channels axis. `axis=-1`
            corresponds to data format `channels_last`,
            and `axis=1` corresponds to data format
            `channels_first`.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `axis` is neither -1 nor one of
            the axes of `output`.
    "
190,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,binary_crossentropy,"Binary crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor with the same shape as `output`.
        output: A tensor.
        from_logits: Whether `output` is expected to be a logits tensor.
            By default, we consider that `output`
            encodes a probability distribution.

    # Returns
        A tensor.
    "
191,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,sigmoid,"Element-wise sigmoid.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
192,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,hard_sigmoid,"Segment-wise linear approximation of sigmoid.

    Faster than sigmoid.
    Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.
    In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
193,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,tanh,"Element-wise tanh.

    # Arguments
        x: A tensor or variable.

    # Returns
        A tensor.

    {{np_implementation}}
    "
194,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,dropout,"Sets entries in `x` to zero at random, while scaling the entire tensor.

    # Arguments
        x: tensor
        level: fraction of the entries in the tensor
            that will be set to 0.
        noise_shape: shape for randomly generated keep/drop flags,
            must be broadcastable to the shape of `x`
        seed: random seed to ensure determinism.

    # Returns
        A tensor.
    "
195,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,l2_normalize,"Normalizes a tensor wrt the L2 norm alongside the specified axis.

    # Arguments
        x: Tensor or variable.
        axis: axis along which to perform normalization.

    # Returns
        A tensor.

    {{np_implementation}}
    "
196,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,in_top_k,"Returns whether the `targets` are in the top `k` `predictions`.

    # Arguments
        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.
        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.
        k: An `int`, number of top elements to consider.

    # Returns
        A 1D tensor of length `batch_size` and type `bool`.
        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`
        values of `predictions[i]`.
    "
197,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_preprocess_conv1d_input,"Transpose and cast the input before the conv1d.

    # Arguments
        x: input tensor.
        data_format: string, `""channels_last""` or `""channels_first""`.

    # Returns
        A tensor.
    "
198,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_preprocess_conv2d_input,"Transpose and cast the input before the conv2d.

    # Arguments
        x: input tensor.
        data_format: string, `""channels_last""` or `""channels_first""`.
        force_transpose: boolean, whether force to transpose input from NCHW to NHWC
                        if the `data_format` is `""channels_first""`.

    # Returns
        A tensor.
    "
199,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_preprocess_conv3d_input,"Transpose and cast the input before the conv3d.

    # Arguments
        x: input tensor.
        data_format: string, `""channels_last""` or `""channels_first""`.

    # Returns
        A tensor.
    "
200,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_preprocess_padding,"Convert keras' padding to tensorflow's padding.

    # Arguments
        padding: string, `""same""` or `""valid""`.

    # Returns
        a string, `""SAME""` or `""VALID""`.

    # Raises
        ValueError: if `padding` is invalid.
    "
201,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,conv1d,"1D convolution.

    # Arguments
        x: Tensor or variable.
        kernel: kernel tensor.
        strides: stride integer.
        padding: string, `""same""`, `""causal""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: integer dilate rate.

    # Returns
        A tensor, result of 1D convolution.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
202,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,conv2d,"2D convolution.

    # Arguments
        x: Tensor or variable.
        kernel: kernel tensor.
        strides: strides tuple.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
            Whether to use Theano or TensorFlow/CNTK data format
            for inputs/kernels/outputs.
        dilation_rate: tuple of 2 integers.

    # Returns
        A tensor, result of 2D convolution.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
203,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,conv2d_transpose,"2D deconvolution (i.e. transposed convolution).

    # Arguments
        x: Tensor or variable.
        kernel: kernel tensor.
        output_shape: 1D int tensor for the output shape.
        strides: strides tuple.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
            Whether to use Theano or TensorFlow/CNTK data format
            for inputs/kernels/outputs.
        dilation_rate: tuple of 2 integers.

    # Returns
        A tensor, result of transposed 2D convolution.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
204,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,separable_conv1d,"1D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: stride integer.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: integer dilation rate.

    # Returns
        Output tensor.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
205,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,separable_conv2d,"2D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: strides tuple (length 2).
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: tuple of integers,
            dilation rates for the separable convolution.

    # Returns
        Output tensor.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
206,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,depthwise_conv2d,"2D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        strides: strides tuple (length 2).
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: tuple of integers,
            dilation rates for the separable convolution.

    # Returns
        Output tensor.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
207,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,conv3d,"3D convolution.

    # Arguments
        x: Tensor or variable.
        kernel: kernel tensor.
        strides: strides tuple.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
            Whether to use Theano or TensorFlow/CNTK data format
            for inputs/kernels/outputs.
        dilation_rate: tuple of 3 integers.

    # Returns
        A tensor, result of 3D convolution.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
208,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,conv3d_transpose,"3D deconvolution (i.e. transposed convolution).

    # Arguments
        x: input tensor.
        kernel: kernel tensor.
        output_shape: 1D int tensor for the output shape.
        strides: strides tuple.
        padding: string, ""same"" or ""valid"".
        data_format: string, `""channels_last""` or `""channels_first""`.
            Whether to use Theano or TensorFlow/CNTK data format
            for inputs/kernels/outputs.

    # Returns
        A tensor, result of transposed 3D convolution.

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
209,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,pool2d,"2D Pooling.

    # Arguments
        x: Tensor or variable.
        pool_size: tuple of 2 integers.
        strides: tuple of 2 integers.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        pool_mode: string, `""max""` or `""avg""`.

    # Returns
        A tensor, result of 2D pooling.

    # Raises
        ValueError: if `data_format` is
        neither `""channels_last""` or `""channels_first""`.
        ValueError: if `pool_mode` is neither `""max""` or `""avg""`.
    "
210,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,pool3d,"3D Pooling.

    # Arguments
        x: Tensor or variable.
        pool_size: tuple of 3 integers.
        strides: tuple of 3 integers.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        pool_mode: string, `""max""` or `""avg""`.

    # Returns
        A tensor, result of 3D pooling.

    # Raises
        ValueError: if `data_format` is
        neither `""channels_last""` or `""channels_first""`.
        ValueError: if `pool_mode` is neither `""max""` or `""avg""`.
    "
211,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,bias_add,"Adds a bias vector to a tensor.

    # Arguments
        x: Tensor or variable.
        bias: Bias tensor to add.
        data_format: string, `""channels_last""` or `""channels_first""`.

    # Returns
        Output tensor.

    # Raises
        ValueError: In one of the two cases below:
                    1. invalid `data_format` argument.
                    2. invalid bias shape.
                       the bias should be either a vector or
                       a tensor with ndim(x) - 1 dimension
    "
212,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,random_normal,"Returns a tensor with normal distribution of values.

    # Arguments
        shape: A tuple of integers, the shape of tensor to create.
        mean: A float, mean of the normal distribution to draw samples.
        stddev: A float, standard deviation of the normal distribution
            to draw samples.
        dtype: String, dtype of returned tensor.
        seed: Integer, random seed.

    # Returns
        A tensor.
    "
213,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,random_uniform,"Returns a tensor with uniform distribution of values.

    # Arguments
        shape: A tuple of integers, the shape of tensor to create.
        minval: A float, lower boundary of the uniform distribution
            to draw samples.
        maxval: A float, upper boundary of the uniform distribution
            to draw samples.
        dtype: String, dtype of returned tensor.
        seed: Integer, random seed.

    # Returns
        A tensor.
    "
214,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,random_binomial,"Returns a tensor with random binomial distribution of values.

    # Arguments
        shape: A tuple of integers, the shape of tensor to create.
        p: A float, `0. <= p <= 1`, probability of binomial distribution.
        dtype: String, dtype of returned tensor.
        seed: Integer, random seed.

    # Returns
        A tensor.
    "
215,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,truncated_normal,"Returns a tensor with truncated random normal distribution of values.

    The generated values follow a normal distribution
    with specified mean and standard deviation,
    except that values whose magnitude is more than
    two standard deviations from the mean are dropped and re-picked.

    # Arguments
        shape: A tuple of integers, the shape of tensor to create.
        mean: Mean of the values.
        stddev: Standard deviation of the values.
        dtype: String, dtype of returned tensor.
        seed: Integer, random seed.

    # Returns
        A tensor.
    "
216,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,ctc_label_dense_to_sparse,"Converts CTC labels from dense to sparse.

    # Arguments
        labels: dense CTC labels.
        label_lengths: length of the labels.

    # Returns
        A sparse tensor representation of the labels.
    "
217,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,ctc_batch_cost,"Runs CTC loss algorithm on each batch element.

    # Arguments
        y_true: tensor `(samples, max_string_length)`
            containing the truth labels.
        y_pred: tensor `(samples, time_steps, num_categories)`
            containing the prediction, or output of the softmax.
        input_length: tensor `(samples, 1)` containing the sequence length for
            each batch item in `y_pred`.
        label_length: tensor `(samples, 1)` containing the sequence length for
            each batch item in `y_true`.

    # Returns
        Tensor with shape (samples,1) containing the
            CTC loss of each element.
    "
218,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,ctc_decode,"Decodes the output of a softmax.

    Can use either greedy search (also known as best path)
    or a constrained dictionary search.

    # Arguments
        y_pred: tensor `(samples, time_steps, num_categories)`
            containing the prediction, or output of the softmax.
        input_length: tensor `(samples, )` containing the sequence length for
            each batch item in `y_pred`.
        greedy: perform much faster best-path search if `true`.
            This does not use a dictionary.
        beam_width: if `greedy` is `false`: a beam search decoder will be used
            with a beam of this width.
        top_paths: if `greedy` is `false`,
            how many of the most probable paths will be returned.

    # Returns
        Tuple:
            List: if `greedy` is `true`, returns a list of one element that
                contains the decoded sequence.
                If `false`, returns the `top_paths` most probable
                decoded sequences.
                Important: blank labels are returned as `-1`.
            Tensor `(top_paths, )` that contains
                the log probability of each decoded sequence.
    "
219,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,map_fn,"Map the function fn over the elements elems and return the outputs.

    # Arguments
        fn: Callable that will be called upon each element in elems
        elems: tensor
        name: A string name for the map node in the graph
        dtype: Output data type.

    # Returns
        Tensor with dtype `dtype`.
    "
220,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,foldl,"Reduce elems using fn to combine them from left to right.

    # Arguments
        fn: Callable that will be called upon each element in elems and an
            accumulator, for instance `lambda acc, x: acc + x`
        elems: tensor
        initializer: The first value used (`elems[0]` in case of None)
        name: A string name for the foldl node in the graph

    # Returns
        Tensor with same type and shape as `initializer`.
    "
221,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,foldr,"Reduce elems using fn to combine them from right to left.

    # Arguments
        fn: Callable that will be called upon each element in elems and an
            accumulator, for instance `lambda acc, x: acc + x`
        elems: tensor
        initializer: The first value used (`elems[-1]` in case of None)
        name: A string name for the foldr node in the graph

    # Returns
        Tensor with same type and shape as `initializer`.
    "
222,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,local_conv1d,"Apply 1D conv with un-shared weights.

    # Arguments
        inputs: 3D tensor with shape: (batch_size, steps, input_dim)
        kernel: the unshared weight for convolution,
                with shape (output_length, feature_dim, filters)
        kernel_size: a tuple of a single integer,
                     specifying the length of the 1D convolution window
        strides: a tuple of a single integer,
                 specifying the stride length of the convolution
        data_format: the data format, channels_first or channels_last

    # Returns
        the tensor after 1d conv with un-shared weights,
        with shape (batch_size, output_length, filters)

    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    "
223,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,local_conv2d,"Apply 2D conv with un-shared weights.

    # Arguments
        inputs: 4D tensor with shape:
                (batch_size, filters, new_rows, new_cols)
                if data_format='channels_first'
                or 4D tensor with shape:
                (batch_size, new_rows, new_cols, filters)
                if data_format='channels_last'.
        kernel: the unshared weight for convolution,
                with shape (output_items, feature_dim, filters)
        kernel_size: a tuple of 2 integers, specifying the
                     width and height of the 2D convolution window.
        strides: a tuple of 2 integers, specifying the strides
                 of the convolution along the width and height.
        output_shape: a tuple with (output_row, output_col)
        data_format: the data format, channels_first or channels_last

    # Returns
        A 4d tensor with shape:
        (batch_size, filters, new_rows, new_cols)
        if data_format='channels_first'
        or 4D tensor with shape:
        (batch_size, new_rows, new_cols, filters)
        if data_format='channels_last'.

    # Raises
        ValueError: if `data_format` is neither
                    `channels_last` or `channels_first`.
    "
224,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_set_device,This method captures TF's explicit device scope setting.
225,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\tensorflow_backend.py,_make_callable,"Generates a callable that runs the graph.

        # Arguments
            feed_arrays: List of input tensors to be fed
                Numpy arrays at runtime.
            feed_symbols: List of input tensors to be fed
                symbolic tensors at runtime.
            symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.
            session: Session to use to generate the callable.

        # Returns
            Function that runs the graph according to the above options.
        "
226,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,get_uid,"Provides a unique UID given a string prefix.

    # Arguments
        prefix: string.

    # Returns
        An integer.

    # Example
    ```python
        >>> keras.backend.get_uid('dense')
        1
        >>> keras.backend.get_uid('dense')
        2
    ```

    "
227,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,variable,"Instantiates a variable and returns it.

    # Arguments
        value: Numpy array, initial value of the tensor.
        dtype: Tensor type.
        name: Optional name string for the tensor.
        constraint: Optional projection function to be
            applied to the variable after an optimizer update.

    # Returns
        A variable instance (with Keras metadata included).
    "
228,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,is_keras_tensor,"Returns whether `x` is a Keras tensor.

    A ""Keras tensor"" is a tensor that was returned by a Keras layer,
    (`Layer` class) or by `Input`.

    # Arguments
        x: A candidate tensor.

    # Returns
        A boolean: Whether the argument is a Keras tensor.

    # Raises
        ValueError: In case `x` is not a symbolic tensor.

    # Examples
    ```python
        >>> from keras import backend as K
        >>> from keras.layers import Input, Dense
        >>> np_var = numpy.array([1, 2])
        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
        ValueError
        >>> k_var = tf.placeholder('float32', shape=(1,1))
        >>> # A variable indirectly created outside of keras is not a Keras tensor.
        >>> K.is_keras_tensor(k_var)
        False
        >>> keras_var = K.variable(np_var)
        >>> # A variable created with the keras backend is not a Keras tensor.
        >>> K.is_keras_tensor(keras_var)
        False
        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))
        >>> # A placeholder is not a Keras tensor.
        >>> K.is_keras_tensor(keras_placeholder)
        False
        >>> keras_input = Input([10])
        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
        True
        >>> keras_layer_output = Dense(10)(keras_input)
        >>> # Any Keras layer output is a Keras tensor.
        >>> K.is_keras_tensor(keras_layer_output)
        True
    ```
    "
229,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,placeholder,"Instantiate an input data placeholder variable.
    "
230,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,is_placeholder,"Returns whether `x` is a placeholder.

    # Arguments
        x: A candidate placeholder.

    # Returns
        Boolean.
    "
231,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,shape,"Returns the shape of a tensor.

    Warning: type returned will be different for
    Theano backend (Theano tensor type) and TF backend (TF TensorShape).
    "
232,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,int_shape,"Returns the shape of a Keras tensor or a Keras variable as a tuple of
    integers or None entries.

    # Arguments
        x: Tensor or variable.

    # Returns
        A tuple of integers (or None entries).
    "
233,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,eval,"Returns the value of a tensor.
    "
234,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,zeros,"Instantiates an all-zeros variable.
    "
235,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,ones,"Instantiates an all-ones variable.
    "
236,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,eye,"Instantiates an identity matrix.
    "
237,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,identity,"Returns a tensor with the same content as the input tensor.

    # Arguments
        x: The input tensor.
        name: String, name for the variable to create.

    # Returns
        A tensor of the same shape, type and content.
    "
238,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,count_params,"Returns the number of scalars in a tensor.

    Return: numpy integer.
    "
239,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,batch_dot,"Batchwise dot product.

    batch_dot results in a tensor with less dimensions than the input.
    If the number of dimensions is reduced to 1, we use `expand_dims` to
    make sure that ndim is at least 2.

    # Arguments
        x, y: tensors with ndim >= 2
        axes: list (or single) int with target dimensions

    # Returns
        A tensor with shape equal to the concatenation of x's shape
        (less the dimension that was summed over) and y's shape
        (less the batch dimension and the dimension that was summed over).
        If the final rank is 1, we reshape it to (batch_size, 1).

    # Examples
        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]
        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal
        of x.dot(y.T), although we never have to calculate the off-diagonal
        elements.

        Shape inference:
        Let x's shape be (100, 20) and y's shape be (100, 30, 20).
        If dot_axes is (1, 2), to find the output shape of resultant tensor,
            loop through each dimension in x's shape and y's shape:
        x.shape[0] : 100 : append to output shape
        x.shape[1] : 20 : do not append to output shape,
            dimension 1 of x has been summed over. (dot_axes[0] = 1)
        y.shape[0] : 100 : do not append to output shape,
            always ignore first dimension of y
        y.shape[1] : 30 : append to output shape
        y.shape[2] : 20 : do not append to output shape,
            dimension 2 of y has been summed over. (dot_axes[1] = 2)

        output_shape = (100, 30)
    "
240,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,gather,"Retrieves the elements of indices `indices` in the tensor `reference`.

    # Arguments
        reference: A tensor.
        indices: An integer tensor of indices.

    # Returns
        A tensor of same type as `reference`.
    "
241,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,sum,"Sum of the values in a tensor, alongside the specified axis.
    "
242,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,prod,"Multiply the values in a tensor, alongside the specified axis.
    "
243,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,cumsum,"Cumulative sum of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to compute the sum.

    # Returns
        A tensor of the cumulative sum of values of `x` along `axis`.
    "
244,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,cumprod,"Cumulative product of the values in a tensor, alongside the specified axis.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to compute the product.

    # Returns
        A tensor of the cumulative product of values of `x` along `axis`.
    "
245,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,mean,"Mean of a tensor, alongside the specified axis.
    "
246,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,any,"Bitwise reduction (logical OR).
    "
247,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,all,"Bitwise reduction (logical AND).
    "
248,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,logsumexp,"Computes log(sum(exp(elements across dimensions of a tensor))).

    This function is more numerically stable than log(sum(exp(x))).
    It avoids overflows caused by taking the exp of large inputs and
    underflows caused by taking the log of small inputs.

    # Arguments
        x: A tensor or variable.
        axis: An integer, the axis to reduce over.
        keepdims: A boolean, whether to keep the dimensions or not.
            If `keepdims` is `False`, the rank of the tensor is reduced
            by 1. If `keepdims` is `True`, the reduced dimension is
            retained with length 1.

    # Returns
        The reduced tensor.
    "
249,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,normalize_batch_in_training,"Computes mean and std for batch then apply batch_normalization on batch.
    "
250,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,batch_normalization,"Apply batch normalization on x given mean, var, beta and gamma.
    "
251,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,_old_normalize_batch_in_training,"Computes mean and std for batch then apply batch_normalization on batch.
    "
252,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,_old_batch_normalization,"Apply batch normalization on x given mean, var, beta and gamma.
    "
253,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,permute_dimensions,"Transpose dimensions.

    pattern should be a tuple or list of
    dimension indices, e.g. [0, 2, 1].
    "
254,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,repeat_elements,"Repeat the elements of a tensor along an axis, like np.repeat.

    If x has shape (s1, s2, s3) and axis=1, the output
    will have shape (s1, s2 * rep, s3).
    "
255,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,resize_images,"Resize the images contained in a 4D tensor of shape
    - [batch, channels, height, width] (for 'channels_first' data_format)
    - [batch, height, width, channels] (for 'channels_last' data_format)
    by a factor of (height_factor, width_factor). Both factors should be
    positive integers.
    "
256,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,resize_volumes,"Resize the volume contained in a 5D tensor of shape
    - [batch, channels, depth, height, width] (for 'channels_first' data_format)
    - [batch, depth, height, width, channels] (for 'channels_last' data_format)
    by a factor of (depth_factor, height_factor, width_factor).
    Both factors should be positive integers.
    "
257,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,repeat,"Repeat a 2D tensor.

    If x has shape (samples, dim) and n=2,
    the output will have shape (samples, 2, dim).
    "
258,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,arange,"Creates a 1-D tensor containing a sequence of integers.

    The function arguments use the same convention as
    Theano's arange: if only one argument is provided,
    it is in fact the ""stop"" argument.

    The default type of the returned tensor is 'int32' to
    match TensorFlow's default.
    "
259,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,batch_flatten,"Turn a n-D tensor into a 2D tensor where
    the first dimension is conserved.
    "
260,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,expand_dims,"Add a 1-sized dimension at index ""dim"".
    "
261,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,squeeze,"Remove a 1-dimension from the tensor at index ""axis"".
    "
262,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,temporal_padding,"Pad the middle dimension of a 3D tensor
    with ""padding"" zeros left and right.

    Apologies for the inane API, but Theano makes this
    really hard.
    "
263,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,spatial_2d_padding,"Pad the 2nd and 3rd dimensions of a 4D tensor
    with ""padding[0]"" and ""padding[1]"" (resp.) zeros left and right.
    "
264,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,spatial_3d_padding,"Pad the 2nd, 3rd and 4th dimensions of a 5D tensor
    with ""padding[0]"", ""padding[1]"" and ""padding[2]"" (resp.) zeros left and right.
    "
265,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,one_hot,"Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))
    Output: (n + 1)D one hot representation of the input
    with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)
    "
266,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,reverse,"Reverse a tensor along the specified axes
    "
267,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,batch_get_value,"Returns the value of more than one tensor variable,
    as a list of Numpy arrays.
    "
268,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,print_tensor,"Print the message and the tensor when evaluated and return the same
    tensor.
    "
269,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,stop_gradient,"Returns `variables` but with zero gradient w.r.t. every other variable.

    # Arguments
        variables: tensor or list of tensors to consider constant with respect
            to any other variable.

    # Returns
        A single tensor or a list of tensors (depending on the passed argument)
            that has constant gradient with respect to any other variable.
    "
270,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,rnn,"Iterates over the time dimension of a tensor.

    # Arguments
        step_function:
            Parameters:
                inputs: Tensor with shape (samples, ...) (no time dimension),
                    representing input for the batch of samples at a certain
                    time step.
                states: List of tensors.
            Returns:
                outputs: Tensor with shape (samples, ...) (no time dimension),
                new_states: List of tensors, same length and shapes
                    as 'states'.
        inputs: Tensor of temporal data of shape (samples, time, ...)
            (at least 3D).
        initial_states: Tensor with shape (samples, ...) (no time dimension),
            containing the initial values for the states used in
            the step function.
        go_backwards: Boolean. If True, do the iteration over the time
            dimension in reverse order and return the reversed sequence.
        mask: Binary tensor with shape (samples, time),
            with a zero for every element that is masked.
        constants: A list of constant values passed at each step.
        unroll: Whether to unroll the RNN or to use a symbolic loop
            (`while_loop` or `scan` depending on backend).
        input_length: Static number of timesteps in the input.
            Must be specified if using `unroll`.

    # Returns
        A tuple (last_output, outputs, new_states).

        last_output: The latest output of the rnn, of shape `(samples, ...)`
        outputs: Tensor with shape `(samples, time, ...)` where each
            entry `outputs[s, t]` is the output of the step function
            at time `t` for sample `s`.
        new_states: List of tensors, latest states returned by
            the step function, of shape `(samples, ...)`.
    "
271,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,switch,"Switches between two operations depending on a scalar value.

    Note that both `then_expression` and `else_expression`
    should be symbolic tensors of the *same shape*.

    # Arguments
        condition: scalar tensor (`int` or `bool`).
        then_expression: either a tensor, or a callable that returns a tensor.
        else_expression: either a tensor, or a callable that returns a tensor.

    # Returns
        The selected tensor.
    "
272,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,in_train_phase,"Selects `x` in train phase, and `alt` otherwise.

    Note that `alt` should have the *same shape* as `x`.

    # Returns
        Either `x` or `alt` based on the `training` flag.
        the `training` flag defaults to `K.learning_phase()`.
    "
273,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,in_test_phase,"Selects `x` in test phase, and `alt` otherwise.
    Note that `alt` should have the *same shape* as `x`.

    # Returns
        Either `x` or `alt` based on `K.learning_phase`.
    "
274,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,elu," Exponential linear unit

    # Arguments
        x: Tensor to compute the activation function for.
        alpha: scalar
    "
275,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,dropout,"Sets entries in `x` to zero at random,
    while scaling the entire tensor.

    # Arguments
        x: tensor
        level: fraction of the entries in the tensor
            that will be set to 0.
        noise_shape: shape for randomly generated keep/drop flags,
            must be broadcastable to the shape of `x`
        seed: random seed to ensure determinism.
    "
276,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,in_top_k,"Returns whether the `targets` are in the top `k` `predictions`.

    # Arguments
        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.
        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.
        k: An `int`, number of top elements to consider.

    # Returns
        A 1D tensor of length `batch_size` and type `bool`.
        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`
        values of `predictions[i]`.
    "
277,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,conv1d,"1D convolution.

    # Arguments
        kernel: kernel tensor.
        strides: stride integer.
        padding: string, `""same""`, `""causal""` or `""valid""`.
        data_format: string, one of ""channels_last"", ""channels_first""
        dilation_rate: integer.
    "
278,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,conv2d,"2D convolution.

    # Arguments
        kernel: kernel tensor.
        strides: strides tuple.
        padding: string, ""same"" or ""valid"".
        data_format: ""channels_last"" or ""channels_first"".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.
    "
279,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,conv2d_transpose,"2D deconvolution (transposed convolution).

    # Arguments
        kernel: kernel tensor.
        output_shape: desired dimensions of output.
        strides: strides tuple.
        padding: string, ""same"" or ""valid"".
        data_format: ""channels_last"" or ""channels_first"".
            Whether to use Theano or TensorFlow data format
            in inputs/kernels/outputs.
        dilation_rate: tuple of 2 integers.

    # Raises
        ValueError: if using an even kernel size with padding 'same'.
    "
280,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,separable_conv1d,"1D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: strides integer.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: integer dilation rate.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `data_format` is neither `""channels_last""` or
        `""channels_first""`.
    "
281,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,separable_conv2d,"2D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: strides tuple (length 2).
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: tuple of integers,
            dilation rates for the separable convolution.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `data_format` is neither `""channels_last""` or
        `""channels_first""`.
    "
282,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,depthwise_conv2d,"2D convolution with separable filters.

    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        strides: strides tuple (length 2).
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: tuple of integers,
            dilation rates for the separable convolution.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `data_format` is neither `""channels_last""` or
        `""channels_first""`.
    "
283,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,conv3d,"3D convolution.

    # Arguments
        kernel: kernel tensor.
        strides: strides tuple.
        padding: string, ""same"" or ""valid"".
        data_format: ""channels_last"" or ""channels_first"".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.
    "
284,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,conv3d_transpose,"3D deconvolution (transposed convolution).

    # Arguments
        kernel: kernel tensor.
        output_shape: desired dimensions of output.
        strides: strides tuple.
        padding: string, ""same"" or ""valid"".
        data_format: ""channels_last"" or ""channels_first"".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.

    # Raises
        ValueError: if using an even kernel size with padding 'same'.
    "
285,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,ctc_batch_cost,"Runs CTC loss algorithm on each batch element.

    # Arguments
        y_true: tensor (samples, max_string_length) containing the truth labels
        y_pred: tensor (samples, time_steps, num_categories) containing the
                prediction, or output of the softmax
        input_length: tensor (samples,1) containing the sequence length for
                each batch item in y_pred
        label_length: tensor (samples,1) containing the sequence length for
                each batch item in y_true

    # Returns
        Tensor with shape (samples,1) containing the
            CTC loss of each element
    "
286,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,map_fn,"Map the function fn over the elements elems and return the outputs.

    # Arguments
        fn: Callable that will be called upon each element in elems
        elems: tensor, at least 2 dimensional
        name: A string name for the map node in the graph

    # Returns
        Tensor with first dimension equal to the elems and second depending on
        fn
    "
287,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,foldl,"Reduce elems using fn to combine them from left to right.

    # Arguments
        fn: Callable that will be called upon each element in elems and an
            accumulator, for instance lambda acc, x: acc + x
        elems: tensor
        initializer: The first value used (elems[0] in case of None)
        name: A string name for the foldl node in the graph

    # Returns
        Same type and shape as initializer
    "
288,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\theano_backend.py,foldr,"Reduce elems using fn to combine them from right to left.

    # Arguments
        fn: Callable that will be called upon each element in elems and an
            accumulator, for instance lambda acc, x: acc + x
        elems: tensor
        initializer: The first value used (elems[-1] in case of None)
        name: A string name for the foldr node in the graph

    # Returns
        Same type and shape as initializer
    "
289,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\backend\__init__.py,backend,"Publicly accessible method
    for determining the current backend.

    # Returns
        String, the name of the backend Keras is currently using.

    # Example
    ```python
        >>> keras.backend.backend()
        'tensorflow'
    ```
    "
290,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,_call_batch_hook,Helper function for all batch_{begin | end} methods.
291,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,_call_begin_hook,Helper function for on_{train|test|predict}_begin methods.
292,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,_call_end_hook,Helper function for on_{train|test|predict}_end methods.
293,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_epoch_begin,"Calls the `on_epoch_begin` methods of its callbacks.

        This function should only be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, Currently no data is passed to this argument for this method
                but that may change in the future.
        "
294,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_epoch_end,"Calls the `on_epoch_end` methods of its callbacks.

        This function should only be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, metric results for this training epoch, and for the
                validation epoch if validation is performed. Validation result keys
                are prefixed with `val_`.
        "
295,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_batch_begin,"Calls the `on_train_batch_begin` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        "
296,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_batch_end,"Calls the `on_train_batch_end` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        "
297,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_batch_begin,"Calls the `on_test_batch_begin` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        "
298,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_batch_end,"Calls the `on_test_batch_end` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        "
299,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_batch_begin,"Calls the `on_predict_batch_begin` methods of its callbacks.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        "
300,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_batch_end,"Calls the `on_predict_batch_end` methods of its callbacks.

        # Argument
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        "
301,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_begin,"Calls the `on_train_begin` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
302,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_end,"Calls the `on_train_end` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
303,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_begin,"Calls the `on_test_begin` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
304,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_end,"Calls the `on_test_end` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
305,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_begin,"Calls the `on_predict_begin` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
306,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_end,"Calls the `on_predict_end` methods of its callbacks.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
307,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_batch_begin,A backwards compatibility alias for `on_train_batch_begin`.
308,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_batch_end,A backwards compatibility alias for `on_train_batch_end`.
309,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_epoch_begin,"Called at the start of an epoch.

        Subclasses should override for any actions to run. This function should only
        be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
310,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_epoch_end,"Called at the end of an epoch.

        Subclasses should override for any actions to run. This function should only
        be called during train mode.

        # Arguments
            epoch: integer, index of epoch.
            logs: dict, metric results for this training epoch, and for the
                validation epoch if validation is performed. Validation result keys
                are prefixed with `val_`.
        "
311,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_batch_begin,"Called at the beginning of a training batch in `fit` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        "
312,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_batch_end,"Called at the end of a training batch in `fit` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        "
313,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_batch_begin,"Called at the beginning of a batch in `evaluate` methods.

        Also called at the beginning of a validation batch in the `fit` methods,
        if validation data is provided.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        "
314,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_batch_end,"Called at the end of a batch in `evaluate` methods.

        Also called at the end of a validation batch in the `fit` methods,
        if validation data is provided.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        "
315,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_batch_begin,"Called at the beginning of a batch in `predict` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, has keys `batch` and `size` representing the current
                batch number and the size of the batch.
        "
316,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_batch_end,"Called at the end of a batch in `predict` methods.

        Subclasses should override for any actions to run.

        # Arguments
            batch: integer, index of batch within the current epoch.
            logs: dict, metric results for this batch.
        "
317,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_begin,"Called at the beginning of training.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
318,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_train_end,"Called at the end of training.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
319,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_begin,"Called at the beginning of evaluation or validation.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
320,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_test_end,"Called at the end of evaluation or validation.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
321,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_begin,"Called at the beginning of prediction.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
322,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,on_predict_end,"Called at the end of prediction.

        Subclasses should override for any actions to run.

        # Arguments
            logs: dict, currently no data is passed to this argument for this method
                but that may change in the future.
        "
323,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\callbacks.py,_reset,"Resets wait counter and cooldown counter.
        "
324,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\boston_housing.py,load_data,"Loads the Boston Housing dataset.

    # Arguments
        path: path where to cache the dataset locally
            (relative to ~/.keras/datasets).
        test_split: fraction of the data to reserve as test set.
        seed: Random seed for shuffling the data
            before computing the test split.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
    "
325,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\cifar.py,load_batch,"Internal utility for parsing CIFAR data.

    # Arguments
        fpath: path the file to parse.
        label_key: key for label data in the retrieve
            dictionary.

    # Returns
        A tuple `(data, labels)`.
    "
326,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\cifar10.py,load_data,"Loads CIFAR10 dataset.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
    "
327,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\cifar100.py,load_data,"Loads CIFAR100 dataset.

    # Arguments
        label_mode: one of ""fine"", ""coarse"".

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.

    # Raises
        ValueError: in case of invalid `label_mode`.
    "
328,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\fashion_mnist.py,load_data,"Loads the Fashion-MNIST dataset.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
    "
329,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\imdb.py,load_data,"Loads the IMDB dataset.

    # Arguments
        path: where to cache the data (relative to `~/.keras/dataset`).
        num_words: max number of words to include. Words are ranked
            by how often they occur (in the training set) and only
            the most frequent words are kept
        skip_top: skip the top N most frequently occurring words
            (which may not be informative).
        maxlen: sequences longer than this will be filtered out.
        seed: random seed for sample shuffling.
        start_char: The start of a sequence will be marked with this character.
            Set to 1 because 0 is usually the padding character.
        oov_char: words that were cut out because of the `num_words`
            or `skip_top` limit will be replaced with this character.
        index_from: index actual words with this index and higher.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.

    # Raises
        ValueError: in case `maxlen` is so low
            that no input sequence could be kept.

    Note that the 'out of vocabulary' character is only used for
    words that were present in the training set but are not included
    because they're not making the `num_words` cut here.
    Words that were not seen in the training set but are in the test set
    have simply been skipped.
    "
330,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\imdb.py,get_word_index,"Retrieves the dictionary mapping words to word indices.

    # Arguments
        path: where to cache the data (relative to `~/.keras/dataset`).

    # Returns
        The word index dictionary.
    "
331,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\mnist.py,load_data,"Loads the MNIST dataset.

    # Arguments
        path: path where to cache the dataset locally
            (relative to ~/.keras/datasets).

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
    "
332,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\reuters.py,load_data,"Loads the Reuters newswire classification dataset.

    # Arguments
        path: where to cache the data (relative to `~/.keras/dataset`).
        num_words: max number of words to include. Words are ranked
            by how often they occur (in the training set) and only
            the most frequent words are kept
        skip_top: skip the top N most frequently occurring words
            (which may not be informative).
        maxlen: truncate sequences after this length.
        test_split: Fraction of the dataset to be used as test data.
        seed: random seed for sample shuffling.
        start_char: The start of a sequence will be marked with this character.
            Set to 1 because 0 is usually the padding character.
        oov_char: words that were cut out because of the `num_words`
            or `skip_top` limit will be replaced with this character.
        index_from: index actual words with this index and higher.

    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.

    Note that the 'out of vocabulary' character is only used for
    words that were present in the training set but are not included
    because they're not making the `num_words` cut here.
    Words that were not seen in the training set but are in the test set
    have simply been skipped.
    "
333,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\datasets\reuters.py,get_word_index,"Retrieves the dictionary mapping words to word indices.

    # Arguments
        path: where to cache the data (relative to `~/.keras/dataset`).

    # Returns
        The word index dictionary.
    "
334,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,_collect_previous_mask,"Retrieves the output mask(s) of the previous node.

    # Arguments
        input_tensors: A tensor or list of tensors.

    # Returns
        A mask tensor or list of mask tensors.
    "
335,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,_collect_input_shape,"Collects the output shape(s) of a list of Keras tensors.

    # Arguments
        input_tensors: list of input tensors (or single input tensor).

    # Returns
        List of shape tuples (or single tuple), one tuple per input.
    "
336,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,_node_key,"Converts a layer and its index to a unique (immutable type) name.

        This function is used internally with `self._network_nodes`.

        # Arguments
            layer: The layer.
            node_index: The layer's position (e.g. via enumerate) in a list of
                nodes.

        # Returns
            The unique name.
        "
337,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,add_weight,"Adds a weight variable to the layer.

        # Arguments
            name: String, the name for the weight variable.
            shape: The shape tuple of the weight.
            dtype: The dtype of the weight.
            initializer: An Initializer instance (callable).
            regularizer: An optional Regularizer instance.
            trainable: A boolean, whether the weight should
                be trained via backprop or not (assuming
                that the layer itself is also trainable).
            constraint: An optional Constraint instance.

        # Returns
            The created weight variable.
        "
338,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,assert_input_compatibility,"Checks compatibility between the layer and provided inputs.

        This checks that the tensor(s) `input`
        verify the input assumptions of the layer
        (if any). If not, exceptions are raised.

        # Arguments
            inputs: input tensor or list of input tensors.

        # Raises
            ValueError: in case of mismatch between
                the provided inputs and the expectations of the layer.
        "
339,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,call,"This is where the layer's logic lives.

        # Arguments
            inputs: Input tensor, or list/tuple of input tensors.
            **kwargs: Additional keyword arguments.

        # Returns
            A tensor or list/tuple of tensors.
        "
340,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,__call__,"Wrapper around self.call(), for handling internal references.

        If a Keras tensor is passed:
            - We call self._add_inbound_node().
            - If necessary, we `build` the layer to match
                the _keras_shape of the input(s).
            - We update the _keras_shape of every input tensor with
                its new shape (obtained via self.compute_output_shape).
                This is done as part of _add_inbound_node().
            - We update the _keras_history of the output tensor(s)
                with the current layer.
                This is done as part of _add_inbound_node().

        # Arguments
            inputs: Can be a tensor or list/tuple of tensors.
            **kwargs: Additional keyword arguments to be passed to `call()`.

        # Returns
            Output of the layer's `call` method.

        # Raises
            ValueError: in case the layer is missing shape information
                for its `build` call.
        "
341,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,_add_inbound_node,"Internal method to create an inbound node for the layer.

        # Arguments
            input_tensors: list of input tensors.
            output_tensors: list of output tensors.
            input_masks: list of input masks (a mask can be a tensor, or None).
            output_masks: list of output masks
                (a mask can be a tensor, or None).
            input_shapes: list of input shape tuples.
            output_shapes: list of output shape tuples.
            arguments: dictionary of keyword arguments that were passed to the
                `call` method of the layer at the call that created the node.
        "
342,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,compute_output_shape,"Computes the output shape of the layer.

        Assumes that the layer will be built
        to match that input shape provided.

        # Arguments
            input_shape: Shape tuple (tuple of integers)
                or list of shape tuples (one per output tensor of the layer).
                Shape tuples can include None for free dimensions,
                instead of an integer.

        # Returns
            An input shape tuple.
        "
343,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,compute_mask,"Computes an output mask tensor.

        # Arguments
            inputs: Tensor or list of tensors.
            mask: Tensor or list of tensors.

        # Returns
            None or a tensor (or list of tensors,
                one per output tensor of the layer).
        "
344,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,build,"Creates the layer weights.

        Must be implemented on all layers that have weights.

        # Arguments
            input_shape: Keras tensor (future input to layer)
                or list/tuple of Keras tensors to reference
                for weight shape computations.
        "
345,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,_get_node_attribute_at_index,"Retrieves an attribute (e.g. input_tensors) from a node.

        This is used to implement the methods:
            - get_input_shape_at
            - get_output_shape_at
            - get_input_at
            etc...

        # Arguments
            node_index: Integer index of the node from which
                to retrieve the attribute.
            attr: Exact node attribute name.
            attr_name: Human-readable attribute name, for error messages.

        # Returns
            The layer's attribute `attr` at the node of index `node_index`.

        # Raises
            RuntimeError: If the layer has no inbound nodes.
            ValueError: If the index is does not match any node.
        "
346,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_input_shape_at,"Retrieves the input shape(s) of a layer at a given node.

        # Arguments
            node_index: Integer, index of the node
                from which to retrieve the attribute.
                E.g. `node_index=0` will correspond to the
                first time the layer was called.

        # Returns
            A shape tuple
            (or list of shape tuples if the layer has multiple inputs).
        "
347,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_output_shape_at,"Retrieves the output shape(s) of a layer at a given node.

        # Arguments
            node_index: Integer, index of the node
                from which to retrieve the attribute.
                E.g. `node_index=0` will correspond to the
                first time the layer was called.

        # Returns
            A shape tuple
            (or list of shape tuples if the layer has multiple outputs).
        "
348,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_input_at,"Retrieves the input tensor(s) of a layer at a given node.

        # Arguments
            node_index: Integer, index of the node
                from which to retrieve the attribute.
                E.g. `node_index=0` will correspond to the
                first time the layer was called.

        # Returns
            A tensor (or list of tensors if the layer has multiple inputs).
        "
349,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_output_at,"Retrieves the output tensor(s) of a layer at a given node.

        # Arguments
            node_index: Integer, index of the node
                from which to retrieve the attribute.
                E.g. `node_index=0` will correspond to the
                first time the layer was called.

        # Returns
            A tensor (or list of tensors if the layer has multiple outputs).
        "
350,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_input_mask_at,"Retrieves the input mask tensor(s) of a layer at a given node.

        # Arguments
            node_index: Integer, index of the node
                from which to retrieve the attribute.
                E.g. `node_index=0` will correspond to the
                first time the layer was called.

        # Returns
            A mask tensor
            (or list of tensors if the layer has multiple inputs).
        "
351,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_output_mask_at,"Retrieves the output mask tensor(s) of a layer at a given node.

        # Arguments
            node_index: Integer, index of the node
                from which to retrieve the attribute.
                E.g. `node_index=0` will correspond to the
                first time the layer was called.

        # Returns
            A mask tensor
            (or list of tensors if the layer has multiple outputs).
        "
352,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,input,"Retrieves the input tensor(s) of a layer.

        Only applicable if the layer has exactly one inbound node,
        i.e. if it is connected to one incoming layer.

        # Returns
            Input tensor or list of input tensors.

        # Raises
            AttributeError: if the layer is connected to
            more than one incoming layers.
        "
353,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,output,"Retrieves the output tensor(s) of a layer.

        Only applicable if the layer has exactly one inbound node,
        i.e. if it is connected to one incoming layer.

        # Returns
            Output tensor or list of output tensors.

        # Raises
            AttributeError: if the layer is connected to
            more than one incoming layers.
        "
354,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,input_mask,"Retrieves the input mask tensor(s) of a layer.

        Only applicable if the layer has exactly one inbound node,
        i.e. if it is connected to one incoming layer.

        # Returns
            Input mask tensor (potentially None) or list of input
            mask tensors.

        # Raises
            AttributeError: if the layer is connected to
            more than one incoming layers.
        "
355,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,output_mask,"Retrieves the output mask tensor(s) of a layer.

        Only applicable if the layer has exactly one inbound node,
        i.e. if it is connected to one incoming layer.

        # Returns
            Output mask tensor (potentially None) or list of output
            mask tensors.

        # Raises
            AttributeError: if the layer is connected to
            more than one incoming layers.
        "
356,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,input_shape,"Retrieves the input shape tuple(s) of a layer.

        Only applicable if the layer has exactly one inbound node,
        i.e. if it is connected to one incoming layer.

        # Returns
            Input shape tuple
            (or list of input shape tuples, one tuple per input tensor).

        # Raises
            AttributeError: if the layer is connected to
            more than one incoming layers.
        "
357,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,output_shape,"Retrieves the output shape tuple(s) of a layer.

        Only applicable if the layer has one inbound node,
        or if all inbound nodes have the same output shape.

        # Returns
            Output shape tuple
            (or list of input shape tuples, one tuple per output tensor).

        # Raises
            AttributeError: if the layer is connected to
            more than one incoming layers.
        "
358,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,add_loss,"Adds losses to the layer.

        The loss may potentially be conditional on some inputs tensors,
        for instance activity losses are conditional on the layer's inputs.

        # Arguments
            losses: loss tensor or list of loss tensors
                to add to the layer.
            inputs: input tensor or list of inputs tensors to mark
                the losses as conditional on these inputs.
                If None is passed, the loss is assumed unconditional
                (e.g. L2 weight regularization, which only depends
                on the layer's weights variables, not on any inputs tensors).
        "
359,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,add_update,"Adds updates to the layer.

        The updates may potentially be conditional on some inputs tensors,
        for instance batch norm updates are conditional on the layer's inputs.

        # Arguments
            updates: update op or list of update ops
                to add to the layer.
            inputs: input tensor or list of inputs tensors to mark
                the updates as conditional on these inputs.
                If None is passed, the updates are assumed unconditional.
        "
360,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,set_weights,"Sets the weights of the layer, from Numpy arrays.

        # Arguments
            weights: a list of Numpy arrays. The number
                of arrays and their shape must match
                number of the dimensions of the weights
                of the layer (i.e. it should match the
                output of `get_weights`).

        # Raises
            ValueError: If the provided weights list does not match the
                layer's specifications.
        "
361,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_weights,"Returns the current weights of the layer.

        # Returns
            Weights values as a list of numpy arrays.
        "
362,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,get_config,"Returns the config of the layer.

        A layer config is a Python dictionary (serializable)
        containing the configuration of a layer.
        The same layer can be reinstantiated later
        (without its trained weights) from this configuration.

        The config of a layer does not include connectivity
        information, nor the layer class name. These are handled
        by `Network` (one layer of abstraction above).

        # Returns
            Python dictionary.
        "
363,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,from_config,"Creates a layer from its config.

        This method is the reverse of `get_config`,
        capable of instantiating the same layer from the config
        dictionary. It does not handle layer connectivity
        (handled by Network), nor weights (handled by `set_weights`).

        # Arguments
            config: A Python dictionary, typically the
                output of get_config.

        # Returns
            A layer instance.
        "
364,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\base_layer.py,count_params,"Counts the total number of scalars composing the weights.

        # Returns
            An integer count.

        # Raises
            RuntimeError: if the layer isn't yet built
                (in which case its weights aren't yet defined).
        "
365,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\input_layer.py,Input,"`Input()` is used to instantiate a Keras tensor.

    A Keras tensor is a tensor object from the underlying backend
    (Theano, TensorFlow or CNTK), which we augment with certain
    attributes that allow us to build a Keras model
    just by knowing the inputs and outputs of the model.

    For instance, if a, b and c are Keras tensors,
    it becomes possible to do:
    `model = Model(input=[a, b], output=c)`

    The added Keras attributes are:
        `_keras_shape`: Integer shape tuple propagated
            via Keras-side shape inference.
        `_keras_history`: Last layer applied to the tensor.
            the entire layer graph is retrievable from that layer,
            recursively.

    # Arguments
        shape: A shape tuple (integer), not including the batch size.
            For instance, `shape=(32,)` indicates that the expected input
            will be batches of 32-dimensional vectors.
        batch_shape: A shape tuple (integer), including the batch size.
            For instance, `batch_shape=(10, 32)` indicates that
            the expected input will be batches of 10 32-dimensional vectors.
            `batch_shape=(None, 32)` indicates batches of an arbitrary number
            of 32-dimensional vectors.
        name: An optional name string for the layer.
            Should be unique in a model (do not reuse the same name twice).
            It will be autogenerated if it isn't provided.
        dtype: The data type expected by the input, as a string
            (`float32`, `float64`, `int32`...)
        sparse: A boolean specifying whether the placeholder
            to be created is sparse.
        tensor: Optional existing tensor to wrap into the `Input` layer.
            If set, the layer will not create a placeholder tensor.

    # Returns
        A tensor.

    # Example

    ```python
    # this is a logistic regression in Keras
    x = Input(shape=(32,))
    y = Dense(16, activation='softmax')(x)
    model = Model(x, y)
    ```
    "
366,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,_map_graph_network,"Validates a network's topology and gather its layers and nodes.

    # Arguments
        inputs: List of input tensors.
        outputs: List of outputs tensors.

    # Returns
        A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.
        - nodes: list of Node instances.
        - nodes_by_depth: dict mapping ints (depth) to lists of node instances.
        - layers: list of Layer instances.
        - layers_by_depth: dict mapping ints (depth)
            to lists of layer instances.

    # Raises
        ValueError: In case the network is not valid (e.g. disconnected graph).
    "
367,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,get_layer,"Retrieves a layer based on either its name (unique) or index.

        If `name` and `index` are both provided, `index` will take precedence.

        Indices are based on order of horizontal graph traversal (bottom-up).

        # Arguments
            name: String, name of layer.
            index: Integer, index of layer.

        # Returns
            A layer instance.

        # Raises
            ValueError: In case of invalid layer name or index.
        "
368,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,updates,"Retrieves the model's updates.

        Will only include updates that are either
        unconditional, or conditional on inputs to this model
        (e.g. will not include updates that depend on tensors
        that aren't inputs to this model).

        # Returns
            A list of update ops.
        "
369,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,losses,"Retrieves the model's losses.

        Will only include losses that are either
        unconditional, or conditional on inputs to this model
        (e.g. will not include losses that depend on tensors
        that aren't inputs to this model).

        # Returns
            A list of loss tensors.
        "
370,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,state_updates,"Returns the `updates` from all layers that are stateful.

        This is useful for separating training updates and
        state updates, e.g. when we need to update a layer's internal state
        during prediction.

        # Returns
            A list of update ops.
        "
371,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,get_weights,"Retrieves the weights of the model.

        # Returns
            A flat list of Numpy arrays.
        "
372,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,set_weights,"Sets the weights of the model.

        # Arguments
            weights: A list of Numpy arrays with shapes and types matching
                the output of `model.get_weights()`.
        "
373,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,input_spec,"Gets the model's input specs.

        # Returns
            A list of `InputSpec` instances (one per input to the model)
                or a single instance if the model has only one input.
        "
374,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,call,"Calls the model on new inputs.

        In this case `call` just reapplies
        all ops in the graph to the new inputs
        (e.g. build a new computational graph from the provided inputs).

        A model is callable on non-Keras tensors.

        # Arguments
            inputs: A tensor or list of tensors.
            mask: A mask or list of masks. A mask can be
                either a tensor or None (no mask).

        # Returns
            A tensor if there is a single output, or
            a list of tensors if there are more than one outputs.
        "
375,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,run_internal_graph,"Computes output tensors for new inputs.

        # Note:
            - Expects `inputs` to be a list (potentially with 1 element).
            - Can be run on non-Keras tensors.

        # Arguments
            inputs: List of tensors
            masks: List of masks (tensors or None).

        # Returns
            Three lists: output_tensors, output_masks, output_shapes
        "
376,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,from_config,"Instantiates a Model from its config (output of `get_config()`).

        # Arguments
            config: Model config dictionary.
            custom_objects: Optional dictionary mapping names
                (strings) to custom classes or functions to be
                considered during deserialization.

        # Returns
            A model instance.

        # Raises
            ValueError: In case of improperly formatted config dict.
        "
377,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,save,"Saves the model to a single HDF5 file.

        The savefile includes:
            - The model architecture, allowing to re-instantiate the model.
            - The model weights.
            - The state of the optimizer, allowing to resume training
                exactly where you left off.

        This allows you to save the entirety of the state of a model
        in a single file.

        Saved models can be reinstantiated via `keras.models.load_model`.
        The model returned by `load_model`
        is a compiled model ready to be used (unless the saved model
        was never compiled in the first place).

        # Arguments
            filepath: String, path to the file to save the weights to.
            overwrite: Whether to silently overwrite any existing file at the
                target location, or provide the user with a manual prompt.
            include_optimizer: If True, save optimizer's state together.

        # Example

        ```python
        from keras.models import load_model

        model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
        del model  # deletes the existing model

        # returns a compiled model
        # identical to the previous one
        model = load_model('my_model.h5')
        ```
        "
378,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,save_weights,"Dumps all layer weights to a HDF5 file.

        The weight file has:
            - `layer_names` (attribute), a list of strings
                (ordered names of model layers).
            - For every layer, a `group` named `layer.name`
                - For every such layer group, a group attribute `weight_names`,
                    a list of strings
                    (ordered names of weights tensor of the layer).
                - For every weight in the layer, a dataset
                    storing the weight value, named after the weight tensor.

        # Arguments
            filepath: String, path to the file to save the weights to.
            overwrite: Whether to silently overwrite any existing file at the
                target location, or provide the user with a manual prompt.

        # Raises
            ImportError: If h5py is not available.
        "
379,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,load_weights,"Loads all layer weights from a HDF5 save file.

        If `by_name` is False (default) weights are loaded
        based on the network's topology, meaning the architecture
        should be the same as when the weights were saved.
        Note that layers that don't have weights are not taken
        into account in the topological ordering, so adding or
        removing layers is fine as long as they don't have weights.

        If `by_name` is True, weights are loaded into layers
        only if they share the same name. This is useful
        for fine-tuning or transfer-learning models where
        some of the layers have changed.

        # Arguments
            filepath: String, path to the weights file to load.
            by_name: Boolean, whether to load weights by name
                or by topological order.
            skip_mismatch: Boolean, whether to skip loading of layers
                where there is a mismatch in the number of weights,
                or a mismatch in the shape of the weight
                (only valid when `by_name`=True).
            reshape: Reshape weights to fit the layer when the correct number
                of weight arrays is present but their shape does not match.


        # Raises
            ImportError: If h5py is not available.
        "
380,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,_updated_config,"Util hared between different serialization methods.

        # Returns
            Model config with Keras version information added.
        "
381,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,to_json,"Returns a JSON string containing the network configuration.

        To load a network from a JSON save file, use
        `keras.models.model_from_json(json_string, custom_objects={})`.

        # Arguments
            **kwargs: Additional keyword arguments
                to be passed to `json.dumps()`.

        # Returns
            A JSON string.
        "
382,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,to_yaml,"Returns a yaml string containing the network configuration.

        To load a network from a yaml save file, use
        `keras.models.model_from_yaml(yaml_string, custom_objects={})`.

        `custom_objects` should be a dictionary mapping
        the names of custom losses / layers / etc to the corresponding
        functions / classes.

        # Arguments
            **kwargs: Additional keyword arguments
                to be passed to `yaml.dump()`.

        # Returns
            A YAML string.
        "
383,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\network.py,summary,"Prints a string summary of the network.

        # Arguments
            line_length: Total length of printed lines
                (e.g. set this to adapt the display to different
                terminal window sizes).
            positions: Relative or absolute positions of log elements
                in each line. If not provided,
                defaults to `[.33, .55, .67, 1.]`.
            print_fn: Print function to use.
                It will be called on each line of the summary.
                You can set it to a custom function
                in order to capture the string summary.
                It defaults to `print` (prints to stdout).
        "
384,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,_serialize_model,"Model serialization logic.

    This method is used for both writing to HDF5 file/group,
    as well as pickling. This is achieved via a
    `keras.utils.hdf5_utls.H5Dict` object, which can wrap HDF5
    files, groups and dicts with a common API.

    # Arguments
        model: Keras model instance to be serialized.
        h5dict: keras.utils.io_utils.HD5Dict instance.
        include_optimizer: If True, serialize optimizer's state together.

    "
385,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,_deserialize_model,"De-serializes a model serialized via _serialize_model

    # Arguments
        h5dict: `keras.utils.hdf5_utils.HFDict` instance.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.
        compile: Boolean, whether to compile the model
            after loading.

    # Returns
        A Keras model instance. If an optimizer was found
        as part of the saved model, the model is already
        compiled. Otherwise, the model is uncompiled and
        a warning will be displayed. When `compile` is set
        to False, the compilation is omitted without any
        warning.
    "
386,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,_gcs_copy,"Copies a file to/from/within Google Cloud Storage (GCS).

    # Arguments
        source_filepath: String, path to the file on filesystem or object on GCS to
            copy from.
        target_filepath: String, path to the file on filesystem or object on GCS to
            copy to.
        overwrite: Whether we should overwrite an existing file/object at the target
            location, or instead ask the user with a manual prompt.
    "
387,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,_is_gcs_location,"Checks if `filepath` is referencing a google storage bucket.

    # Arguments
        filepath: The location to check.
    "
388,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,allow_write_to_gcs,"Function decorator to support saving to Google Cloud Storage (GCS).

    This decorator parses the `filepath` argument of the `save_function` and
    transfers the file to GCS if `filepath` starts with ""gs://"".

    Note: the file is temporarily writen to local filesystem before copied to GSC.

    # Arguments
        save_function: The function to wrap, with requirements:
            - second positional argument should indicate the location to save to.
            - third positional argument should be the `overwrite` option indicating
            whether we should overwrite an existing file/object at the target
            location, or instead ask the user with a manual prompt.
    "
389,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,allow_read_from_gcs,"Function decorator to support loading from Google Cloud Storage (GCS).

    This decorator parses the `filepath` argument of the `load_function` and
    fetches the required object from GCS if `filepath` starts with ""gs://"".

    Note: the file is temporarily copied to local filesystem from GCS before loaded.

    # Arguments
        load_function: The function to wrap, with requirements:
            - should have one _named_ argument `filepath` indicating the location to
            load from.
    "
390,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,save_model,"Save a model to a HDF5 file.

    Note: Please also see
    [How can I install HDF5 or h5py to save my models in Keras?](
        /getting-started/faq/
        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)
    in the FAQ for instructions on how to install `h5py`.

    The saved model contains:
        - the model's configuration (topology)
        - the model's weights
        - the model's optimizer's state (if any)

    Thus the saved model can be reinstantiated in
    the exact same state, without any of the code
    used for model definition or training.

    # Arguments
        model: Keras model instance to be saved.
        filepath: one of the following:
            - string, path where to save the model, or
            - h5py.File or h5py.Group object where to save the model
        overwrite: Whether we should overwrite any existing
            model at the target location, or instead
            ask the user with a manual prompt.
        include_optimizer: If True, save optimizer's state together.

    # Raises
        ImportError: if h5py is not available.
    "
391,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,load_model,"Loads a model saved via `save_model`.

    # Arguments
        filepath: one of the following:
            - string, path to the saved model, or
            - h5py.File or h5py.Group object from which to load the model
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.
        compile: Boolean, whether to compile the model
            after loading.

    # Returns
        A Keras model instance. If an optimizer was found
        as part of the saved model, the model is already
        compiled. Otherwise, the model is uncompiled and
        a warning will be displayed. When `compile` is set
        to False, the compilation is omitted without any
        warning.

    # Raises
        ImportError: if h5py is not available.
        ValueError: In case of an invalid savefile.
    "
392,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,model_from_config,"Instantiates a Keras model from its config.

    # Arguments
        config: Configuration dictionary.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.

    # Returns
        A Keras model instance (uncompiled).

    # Raises
        TypeError: if `config` is not a dictionary.
    "
393,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,model_from_yaml,"Parses a yaml model configuration file and returns a model instance.

    # Arguments
        yaml_string: YAML string encoding a model configuration.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.

    # Returns
        A Keras model instance (uncompiled).
    "
394,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,model_from_json,"Parses a JSON model configuration file and returns a model instance.

    # Arguments
        json_string: JSON string encoding a model configuration.
        custom_objects: Optional dictionary mapping names
            (strings) to custom classes or functions to be
            considered during deserialization.

    # Returns
        A Keras model instance (uncompiled).
    "
395,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,save_attributes_to_hdf5_group,"Saves attributes (data) of the specified name into the HDF5 group.

    This method deals with an inherent problem of HDF5 file which is not
    able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.

    # Arguments
        group: A pointer to a HDF5 group.
        name: A name of the attributes to save.
        data: Attributes data to store.
    "
396,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,load_attributes_from_hdf5_group,"Loads attributes of the specified name from the HDF5 group.

    This method deals with an inherent problem
    of HDF5 file which is not able to store
    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.

    # Arguments
        group: A pointer to a HDF5 group.
        name: A name of the attributes to load.

    # Returns
        data: Attributes data.
    "
397,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,save_weights_to_hdf5_group,"Saves weights into the HDF5 group.

    # Arguments
        group: A pointer to a HDF5 group.
        layers: Layers to load.
    "
398,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,preprocess_weights_for_loading,"Converts layers weights from Keras 1 format to Keras 2.

    # Arguments
        layer: Layer instance.
        weights: List of weights values (Numpy arrays).
        original_keras_version: Keras version for the weights, as a string.
        original_backend: Keras backend the weights were trained with,
            as a string.
        reshape: Reshape weights to fit the layer when the correct number
            of values are present but the shape does not match.

    # Returns
        A list of weights values (Numpy arrays).
    "
399,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,_convert_rnn_weights,"Converts weights for RNN layers between native and CuDNN format.

    Input kernels for each gate are transposed and converted between Fortran
    and C layout, recurrent kernels are transposed. For LSTM biases are summed/
    split in half, for GRU biases are reshaped.

    Weights can be converted in both directions between `LSTM` and`CuDNNSLTM`
    and between `CuDNNGRU` and `GRU(reset_after=True)`. Default `GRU` is not
    compatible with `CuDNNGRU`.

    For missing biases in `LSTM`/`GRU` (`use_bias=False`),
    no conversion is made.

    # Arguments
        layer: Target layer instance.
        weights: List of source weights values (input kernels, recurrent
            kernels, [biases]) (Numpy arrays).

    # Returns
        A list of converted weights values (Numpy arrays).

    # Raises
        ValueError: for incompatible GRU layer/weights or incompatible biases
    "
400,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,_need_convert_kernel,"Checks if conversion on kernel matrices is required during weight loading.

    The convolution operation is implemented differently in different backends.
    While TH implements convolution, TF and CNTK implement the correlation operation.
    So the channel axis needs to be flipped when TF weights are loaded on a TH model,
    or vice versa. However, there's no conversion required between TF and CNTK.

    # Arguments
        original_backend: Keras backend the weights were trained with, as a string.

    # Returns
        `True` if conversion on kernel matrices is required, otherwise `False`.
    "
401,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,load_weights_from_hdf5_group,"Implements topological (order-based) weight loading.

    # Arguments
        f: A pointer to a HDF5 group.
        layers: a list of target layers.
        reshape: Reshape weights to fit the layer when the correct number
            of values are present but the shape does not match.

    # Raises
        ValueError: in case of mismatch between provided layers
            and weights file.
    "
402,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\saving.py,load_weights_from_hdf5_group_by_name,"Implements name-based weight loading.

    (instead of topological weight loading).

    Layers that have no matching name are skipped.

    # Arguments
        f: A pointer to a HDF5 group.
        layers: A list of target layers.
        skip_mismatch: Boolean, whether to skip loading of layers
            where there is a mismatch in the number of weights,
            or a mismatch in the shape of the weights.
        reshape: Reshape weights to fit the layer when the correct number
            of values are present but the shape does not match.

    # Raises
        ValueError: in case of mismatch between provided layers
            and weights file and skip_mismatch=False.
    "
403,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\sequential.py,add,"Adds a layer instance on top of the layer stack.

        # Arguments
            layer: layer instance.

        # Raises
            TypeError: If `layer` is not a layer instance.
            ValueError: In case the `layer` argument does not
                know its input shape.
            ValueError: In case the `layer` argument has
                multiple output tensors, or is already connected
                somewhere else (forbidden in `Sequential` models).
        "
404,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\sequential.py,pop,"Removes the last layer in the model.

        # Raises
            TypeError: if there are no layers in the model.
        "
405,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\sequential.py,predict_proba,"Generates class probability predictions for the input samples.

        The input samples are processed batch by batch.

        # Arguments
            x: input data, as a Numpy array or list of Numpy arrays
                (if the model has multiple inputs).
            batch_size: integer.
            verbose: verbosity mode, 0 or 1.

        # Returns
            A Numpy array of probability predictions.
        "
406,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\sequential.py,predict_classes,"Generate class predictions for the input samples.

        The input samples are processed batch by batch.

        # Arguments
            x: input data, as a Numpy array or list of Numpy arrays
                (if the model has multiple inputs).
            batch_size: integer.
            verbose: verbosity mode, 0 or 1.

        # Returns:
            A numpy array of class predictions.
        "
407,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,compile,"Configures the model for training.

        # Arguments
            optimizer: String (name of optimizer) or optimizer instance.
                See [optimizers](/optimizers).
            loss: String (name of objective function) or objective function.
                See [losses](/losses).
                If the model has multiple outputs, you can use a different loss
                on each output by passing a dictionary or a list of losses.
                The loss value that will be minimized by the model
                will then be the sum of all individual losses.
            metrics: List of metrics to be evaluated by the model
                during training and testing.
                Typically you will use `metrics=['accuracy']`.
                To specify different metrics for different outputs of a
                multi-output model, you could also pass a dictionary,
                such as `metrics={'output_a': 'accuracy'}`.
            loss_weights: Optional list or dictionary specifying scalar
                coefficients (Python floats) to weight the loss contributions
                of different model outputs.
                The loss value that will be minimized by the model
                will then be the *weighted sum* of all individual losses,
                weighted by the `loss_weights` coefficients.
                If a list, it is expected to have a 1:1 mapping
                to the model's outputs. If a tensor, it is expected to map
                output names (strings) to scalar coefficients.
            sample_weight_mode: If you need to do timestep-wise
                sample weighting (2D weights), set this to `""temporal""`.
                `None` defaults to sample-wise weights (1D).
                If the model has multiple outputs, you can use a different
                `sample_weight_mode` on each output by passing a
                dictionary or a list of modes.
            weighted_metrics: List of metrics to be evaluated and weighted
                by sample_weight or class_weight during training and testing.
            target_tensors: By default, Keras will create placeholders for the
                model's target, which will be fed with the target data during
                training. If instead you would like to use your own
                target tensors (in turn, Keras will not expect external
                Numpy data for these targets at training time), you
                can specify them via the `target_tensors` argument. It can be
                a single tensor (for a single-output model), a list of tensors,
                or a dict mapping output names to target tensors.
            **kwargs: When using the Theano/CNTK backends, these arguments
                are passed into `K.function`.
                When using the TensorFlow backend,
                these arguments are passed into `tf.Session.run`.

        # Raises
            ValueError: In case of invalid arguments for
                `optimizer`, `loss`, `metrics` or `sample_weight_mode`.
        "
408,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,_check_trainable_weights_consistency,"Check trainable weights count consistency.

        This will raise a warning if `trainable_weights` and
        `_collected_trainable_weights` are inconsistent (i.e. have different
        number of parameters).
        Inconsistency will typically arise when one modifies `model.trainable`
        without calling `model.compile` again.
        "
409,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,_set_inputs,"Set model's input and output specs based on the input data received.

        This is to be used for Model subclasses, which do not know at instantiation
        time what their inputs look like.

        # Arguments
          inputs: Single array, or list of arrays. The arrays could be placeholders,
            Numpy arrays, or data tensors.
            - if placeholders: the model is built on top of these placeholders,
              and we expect Numpy data to be fed for them when calling `fit`/etc.
            - if Numpy data: we create placeholders matching the shape of the Numpy
              arrays. We expect Numpy data to be fed for these placeholders
              when calling `fit`/etc.
            - if data tensors: the model is built on top of these tensors.
              We do not expect any Numpy data to be provided when calling `fit`/etc.
          outputs: Optional output tensors (if already computed by running
            the model).
          training: Boolean or None. Only relevant in symbolic mode. Specifies
            whether to build the model's graph in inference mode (False), training
            mode (True), or using the Keras learning phase (None).
        "
410,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,_get_callback_model,Returns the Callback Model for this Model.
411,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,fit,"Trains the model for a given number of epochs (iterations on a dataset).

        # Arguments
            x: Numpy array of training data (if the model has a single input),
                or list of Numpy arrays (if the model has multiple inputs).
                If input layers in the model are named, you can also pass a
                dictionary mapping input names to Numpy arrays.
                `x` can be `None` (default) if feeding from
                framework-native tensors (e.g. TensorFlow data tensors).
            y: Numpy array of target (label) data
                (if the model has a single output),
                or list of Numpy arrays (if the model has multiple outputs).
                If output layers in the model are named, you can also pass a
                dictionary mapping output names to Numpy arrays.
                `y` can be `None` (default) if feeding from
                framework-native tensors (e.g. TensorFlow data tensors).
            batch_size: Integer or `None`.
                Number of samples per gradient update.
                If unspecified, `batch_size` will default to 32.
            epochs: Integer. Number of epochs to train the model.
                An epoch is an iteration over the entire `x` and `y`
                data provided.
                Note that in conjunction with `initial_epoch`,
                `epochs` is to be understood as ""final epoch"".
                The model is not trained for a number of iterations
                given by `epochs`, but merely until the epoch
                of index `epochs` is reached.
            verbose: Integer. 0, 1, or 2. Verbosity mode.
                0 = silent, 1 = progress bar, 2 = one line per epoch.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training and validation
                (if ).
                See [callbacks](/callbacks).
            validation_split: Float between 0 and 1.
                Fraction of the training data to be used as validation data.
                The model will set apart this fraction of the training data,
                will not train on it, and will evaluate
                the loss and any model metrics
                on this data at the end of each epoch.
                The validation data is selected from the last samples
                in the `x` and `y` data provided, before shuffling.
            validation_data: tuple `(x_val, y_val)` or tuple
                `(x_val, y_val, val_sample_weights)` on which to evaluate
                the loss and any model metrics at the end of each epoch.
                The model will not be trained on this data.
                `validation_data` will override `validation_split`.
            shuffle: Boolean (whether to shuffle the training data
                before each epoch) or str (for 'batch').
                'batch' is a special option for dealing with the
                limitations of HDF5 data; it shuffles in batch-sized chunks.
                Has no effect when `steps_per_epoch` is not `None`.
            class_weight: Optional dictionary mapping class indices (integers)
                to a weight (float) value, used for weighting the loss function
                (during training only).
                This can be useful to tell the model to
                ""pay more attention"" to samples from
                an under-represented class.
            sample_weight: Optional Numpy array of weights for
                the training samples, used for weighting the loss function
                (during training only). You can either pass a flat (1D)
                Numpy array with the same length as the input samples
                (1:1 mapping between weights and samples),
                or in the case of temporal data,
                you can pass a 2D array with shape
                `(samples, sequence_length)`,
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                `sample_weight_mode=""temporal""` in `compile()`.
            initial_epoch: Integer.
                Epoch at which to start training
                (useful for resuming a previous training run).
            steps_per_epoch: Integer or `None`.
                Total number of steps (batches of samples)
                before declaring one epoch finished and starting the
                next epoch. When training with input tensors such as
                TensorFlow data tensors, the default `None` is equal to
                the number of samples in your dataset divided by
                the batch size, or 1 if that cannot be determined.
            validation_steps: Only relevant if `steps_per_epoch`
                is specified. Total number of steps (batches of samples)
                to validate before stopping.

        # Returns
            A `History` object. Its `History.history` attribute is
            a record of training loss values and metrics values
            at successive epochs, as well as validation loss values
            and validation metrics values (if applicable).

        # Raises
            RuntimeError: If the model was never compiled.
            ValueError: In case of mismatch between the provided input data
                and what the model expects.
        "
412,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,evaluate,"Returns the loss value & metrics values for the model in test mode.

        Computation is done in batches.

        # Arguments
            x: Numpy array of test data (if the model has a single input),
                or list of Numpy arrays (if the model has multiple inputs).
                If input layers in the model are named, you can also pass a
                dictionary mapping input names to Numpy arrays.
                `x` can be `None` (default) if feeding from
                framework-native tensors (e.g. TensorFlow data tensors).
            y: Numpy array of target (label) data
                (if the model has a single output),
                or list of Numpy arrays (if the model has multiple outputs).
                If output layers in the model are named, you can also pass a
                dictionary mapping output names to Numpy arrays.
                `y` can be `None` (default) if feeding from
                framework-native tensors (e.g. TensorFlow data tensors).
            batch_size: Integer or `None`.
                Number of samples per evaluation step.
                If unspecified, `batch_size` will default to 32.
            verbose: 0 or 1. Verbosity mode.
                0 = silent, 1 = progress bar.
            sample_weight: Optional Numpy array of weights for
                the test samples, used for weighting the loss function.
                You can either pass a flat (1D)
                Numpy array with the same length as the input samples
                (1:1 mapping between weights and samples),
                or in the case of temporal data,
                you can pass a 2D array with shape
                `(samples, sequence_length)`,
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                `sample_weight_mode=""temporal""` in `compile()`.
            steps: Integer or `None`.
                Total number of steps (batches of samples)
                before declaring the evaluation round finished.
                Ignored with the default value of `None`.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during evaluation.
                See [callbacks](/callbacks).

        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        "
413,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,predict,"Generates output predictions for the input samples.

        Computation is done in batches.

        # Arguments
            x: The input data, as a Numpy array
                (or list of Numpy arrays if the model has multiple inputs).
            batch_size: Integer. If unspecified, it will default to 32.
            verbose: Verbosity mode, 0 or 1.
            steps: Total number of steps (batches of samples)
                before declaring the prediction round finished.
                Ignored with the default value of `None`.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during prediction.
                See [callbacks](/callbacks).

        # Returns
            Numpy array(s) of predictions.

        # Raises
            ValueError: In case of mismatch between the provided
                input data and the model's expectations,
                or in case a stateful model receives a number of samples
                that is not a multiple of the batch size.
        "
414,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,train_on_batch,"Runs a single gradient update on a single batch of data.

        # Arguments
            x: Numpy array of training data,
                or list of Numpy arrays if the model has multiple inputs.
                If all inputs in the model are named,
                you can also pass a dictionary
                mapping input names to Numpy arrays.
            y: Numpy array of target data,
                or list of Numpy arrays if the model has multiple outputs.
                If all outputs in the model are named,
                you can also pass a dictionary
                mapping output names to Numpy arrays.
            sample_weight: Optional array of the same length as x, containing
                weights to apply to the model's loss for each sample.
                In the case of temporal data, you can pass a 2D array
                with shape (samples, sequence_length),
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                sample_weight_mode=""temporal"" in compile().
            class_weight: Optional dictionary mapping
                class indices (integers) to
                a weight (float) to apply to the model's loss for the samples
                from this class during training.
                This can be useful to tell the model to ""pay more attention"" to
                samples from an under-represented class.

        # Returns
            Scalar training loss
            (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        "
415,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,test_on_batch,"Test the model on a single batch of samples.

        # Arguments
            x: Numpy array of test data,
                or list of Numpy arrays if the model has multiple inputs.
                If all inputs in the model are named,
                you can also pass a dictionary
                mapping input names to Numpy arrays.
            y: Numpy array of target data,
                or list of Numpy arrays if the model has multiple outputs.
                If all outputs in the model are named,
                you can also pass a dictionary
                mapping output names to Numpy arrays.
            sample_weight: Optional array of the same length as x, containing
                weights to apply to the model's loss for each sample.
                In the case of temporal data, you can pass a 2D array
                with shape (samples, sequence_length),
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                sample_weight_mode=""temporal"" in compile().

        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        "
416,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,predict_on_batch,"Returns predictions for a single batch of samples.

        # Arguments
            x: Input samples, as a Numpy array.

        # Returns
            Numpy array(s) of predictions.
        "
417,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,fit_generator,"Trains the model on data generated batch-by-batch by a Python generator
        (or an instance of `Sequence`).

        The generator is run in parallel to the model, for efficiency.
        For instance, this allows you to do real-time data augmentation
        on images on CPU in parallel to training your model on GPU.

        The use of `keras.utils.Sequence` guarantees the ordering
        and guarantees the single use of every input per epoch when
        using `use_multiprocessing=True`.

        # Arguments
            generator: A generator or an instance of `Sequence`
                (`keras.utils.Sequence`) object in order to avoid
                duplicate data when using multiprocessing.
                The output of the generator must be either
                - a tuple `(inputs, targets)`
                - a tuple `(inputs, targets, sample_weights)`.
                This tuple (a single output of the generator) makes a single
                batch. Therefore, all arrays in this tuple must have the same
                length (equal to the size of this batch). Different batches may
                have different sizes. For example, the last batch of the epoch
                is commonly smaller than the others, if the size of the dataset
                is not divisible by the batch size.
                The generator is expected to loop over its data
                indefinitely. An epoch finishes when `steps_per_epoch`
                batches have been seen by the model.
            steps_per_epoch: Integer.
                Total number of steps (batches of samples)
                to yield from `generator` before declaring one epoch
                finished and starting the next epoch. It should typically
                be equal to the number of samples of your dataset
                divided by the batch size.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            epochs: Integer. Number of epochs to train the model.
                An epoch is an iteration over the entire data provided,
                as defined by `steps_per_epoch`.
                Note that in conjunction with `initial_epoch`,
                `epochs` is to be understood as ""final epoch"".
                The model is not trained for a number of iterations
                given by `epochs`, but merely until the epoch
                of index `epochs` is reached.
            verbose: Integer. 0, 1, or 2. Verbosity mode.
                0 = silent, 1 = progress bar, 2 = one line per epoch.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training.
                See [callbacks](/callbacks).
            validation_data: This can be either
                - a generator or a `Sequence` object for the validation data
                - tuple `(x_val, y_val)`
                - tuple `(x_val, y_val, val_sample_weights)`
                on which to evaluate
                the loss and any model metrics at the end of each epoch.
                The model will not be trained on this data.
            validation_steps: Only relevant if `validation_data`
                is a generator. Total number of steps (batches of samples)
                to yield from `validation_data` generator before stopping
                at the end of every epoch. It should typically
                be equal to the number of samples of your
                validation dataset divided by the batch size.
                Optional for `Sequence`: if unspecified, will use
                the `len(validation_data)` as a number of steps.
            class_weight: Optional dictionary mapping class indices (integers)
                to a weight (float) value, used for weighting the loss function
                (during training only). This can be useful to tell the model to
                ""pay more attention"" to samples
                from an under-represented class.
            max_queue_size: Integer. Maximum size for the generator queue.
                If unspecified, `max_queue_size` will default to 10.
            workers: Integer. Maximum number of processes to spin up
                when using process-based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: Boolean.
                If `True`, use process-based threading.
                If unspecified, `use_multiprocessing` will default to `False`.
                Note that because this implementation
                relies on multiprocessing,
                you should not pass non-picklable arguments to the generator
                as they can't be passed easily to children processes.
            shuffle: Boolean. Whether to shuffle the order of the batches at
                the beginning of each epoch. Only used with instances
                of `Sequence` (`keras.utils.Sequence`).
                Has no effect when `steps_per_epoch` is not `None`.
            initial_epoch: Integer.
                Epoch at which to start training
                (useful for resuming a previous training run).

        # Returns
            A `History` object. Its `History.history` attribute is
            a record of training loss values and metrics values
            at successive epochs, as well as validation loss values
            and validation metrics values (if applicable).

        # Raises
            ValueError: In case the generator yields data in an invalid format.

        # Example

        ```python
        def generate_arrays_from_file(path):
            while True:
                with open(path) as f:
                    for line in f:
                        # create numpy arrays of input data
                        # and labels, from each line in the file
                        x1, x2, y = process_line(line)
                        yield ({'input_1': x1, 'input_2': x2}, {'output': y})

        model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                            steps_per_epoch=10000, epochs=10)
        ```
        "
418,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,evaluate_generator,"Evaluates the model on a data generator.

        The generator should return the same kind of data
        as accepted by `test_on_batch`.

        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training.
                See [callbacks](/callbacks).
            max_queue_size: maximum size for the generator queue
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.

        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.

        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        "
419,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training.py,predict_generator,"Generates predictions for the input samples from a data generator.

        The generator should return the same kind of data as accepted by
        `predict_on_batch`.

        # Arguments
            generator: Generator yielding batches of input samples
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training.
                See [callbacks](/callbacks).
            max_queue_size: Maximum size for the generator queue.
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: If `True`, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.

        # Returns
            Numpy array(s) of predictions.

        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        "
420,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_arrays.py,fit_loop,"Abstract fit function for `fit_function(fit_inputs)`.

    Assumes that fit_function returns a list, labeled by out_labels.

    # Arguments
        model: Keras model instance.
        fit_function: Keras function returning a list of tensors
        fit_inputs: List of tensors to be fed to `fit_function`
        out_labels: List of strings, display names of
            the outputs of `fit_function`
        batch_size: Integer batch size or None if unknown.
        epochs: Number of times to iterate over the data
        verbose: Verbosity mode, 0, 1 or 2
        callbacks: List of callbacks to be called during training and validation
            (if `val_function` and `val_inputs` are not `None`).
        val_function: Keras function to call for validation
        val_inputs: List of tensors to be fed to `val_function`
        shuffle: Whether to shuffle the data at the beginning of each epoch
        callback_metrics: List of strings, the display names of the metrics
            passed to the callbacks. They should be the
            concatenation of list the display names of the outputs of
             `fit_function` and the list of display names
             of the outputs of `fit_inputs`.
        initial_epoch: Epoch at which to start training
            (useful for resuming a previous training run)
        steps_per_epoch: Total number of steps (batches of samples)
            before declaring one epoch finished and starting the
            next epoch. Ignored with the default value of `None`.
        validation_steps: Number of steps to run validation for
            (only if doing validation from data tensors).
            Ignored with the default value of `None`.

    # Returns
        `History` object.
    "
421,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_arrays.py,predict_loop,"Abstract method to loop over some data in batches.

    # Arguments
        model: Keras model instance.
        f: Keras function returning a list of tensors.
        ins: list of tensors to be fed to `f`.
        batch_size: integer batch size.
        verbose: verbosity mode.
        steps: Total number of steps (batches of samples)
            before declaring `predict_loop` finished.
            Ignored with the default value of `None`.
        callbacks: List of callbacks or an instance of
            `keras.callbacks.CallbackList` to be called during prediction.

    # Returns
        Array of predictions (if the model has a single output)
        or list of arrays of predictions
        (if the model has multiple outputs).
    "
422,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_arrays.py,test_loop,"Abstract method to loop over some data in batches.

    # Arguments
        model: Keras model instance.
        f: Keras function returning a list of tensors.
        ins: list of tensors to be fed to `f`.
        batch_size: integer batch size or `None`.
        verbose: verbosity mode.
        steps: Total number of steps (batches of samples)
            before declaring predictions finished.
            Ignored with the default value of `None`.
        callbacks: List of callbacks or an instance of
            `keras.callbacks.CallbackList` to be called during evaluation.

    # Returns
        Scalar loss (if the model has a single output and no metrics)
        or list of scalars (if the model has multiple outputs
        and/or metrics). The attribute `model.metrics_names` will give you
        the display labels for the scalar outputs.
    "
423,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_generator.py,fit_generator,See docstring for `Model.fit_generator`.
424,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_generator.py,evaluate_generator,See docstring for `Model.evaluate_generator`.
425,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_generator.py,predict_generator,See docstring for `Model.predict_generator`.
426,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,standardize_input_data,"Normalizes inputs and targets provided by users.

    Users may pass data as a list of arrays, dictionary of arrays,
    or as a single array. We normalize this to an ordered list of
    arrays (same order as `names`), while checking that the provided
    arrays have shapes that match the network's expectations.

    # Arguments
        data: User-provided input data (polymorphic).
        names: List of expected array names.
        shapes: Optional list of expected array shapes.
        check_batch_axis: Boolean; whether to check that
            the batch axis of the arrays matches the expected
            value found in `shapes`.
        exception_prefix: String prefix used for exception formatting.

    # Returns
        List of standardized input arrays (one array per model input).

    # Raises
        ValueError: in case of improperly formatted user-provided data.
    "
427,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,standardize_sample_or_class_weights,"Maps `sample_weight` or `class_weight` to model outputs.

    # Arguments
        x_weight: User-provided `sample_weight` or `class_weight` argument.
        output_names: List of output names (strings) in the model.
        weight_type: A string used purely for exception printing.

    # Returns
        A list of `sample_weight` or `class_weight` where there are exactly
            one element per model output.

    # Raises
        ValueError: In case of invalid user-provided argument.
    "
428,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,check_array_length_consistency,"Checks if batch axes are the same for Numpy arrays.

    # Arguments
        inputs: list of Numpy arrays of inputs.
        targets: list of Numpy arrays of targets.
        weights: list of Numpy arrays of sample weights.

    # Raises
        ValueError: in case of incorrectly formatted data.
    "
429,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,check_loss_and_target_compatibility,"Does validation on the compatibility of targets and loss functions.

    This helps prevent users from using loss functions incorrectly.

    # Arguments
        targets: list of Numpy arrays of targets.
        loss_fns: list of loss functions.
        output_shapes: list of shapes of model outputs.

    # Raises
        ValueError: if a loss function or target array
            is incompatible with an output.
    "
430,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,collect_metrics,"Maps metric functions to model outputs.

    # Arguments
        metrics: a list or dict of metric functions.
        output_names: a list of the names (strings) of model outputs.

    # Returns
        A list (one entry per model output) of lists of metric functions.
        For instance, if the model has 2 outputs, and for the first output
        we want to compute ""binary_accuracy"" and ""binary_crossentropy"",
        and just ""binary_accuracy"" for the second output,
        the list would look like:
            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`

    # Raises
        TypeError: if an incorrect type is passed for the `metrics` argument.
    "
431,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,batch_shuffle,"Shuffles an array in a batch-wise fashion.

    Useful for shuffling HDF5 arrays
    (where one cannot access arbitrary indices).

    # Arguments
        index_array: array of indices to be shuffled.
        batch_size: integer.

    # Returns
        The `index_array` array, shuffled in a batch-wise fashion.
    "
432,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,make_batches,"Returns a list of batch indices (tuples of indices).

    # Arguments
        size: Integer, total size of the data to slice into batches.
        batch_size: Integer, batch size.

    # Returns
        A list of tuples of array indices.
    "
433,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,weighted_masked_objective,"Adds support for masking and sample-weighting to an objective function.

    It transforms an objective function `fn(y_true, y_pred)`
    into a sample-weighted, cost-masked objective function
    `fn(y_true, y_pred, weights, mask)`.

    # Arguments
        fn: The objective function to wrap,
            with signature `fn(y_true, y_pred)`.

    # Returns
        A function with signature `fn(y_true, y_pred, weights, mask)`.
    "
434,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,standardize_weights,"Performs sample weight validation and standardization.

    Everything gets normalized to a single sample-wise (or timestep-wise)
    weight array. If both `sample_weights` and `class_weights` are provided,
    the weights are multiplied together.

    # Arguments
        y: Numpy array of model targets to be weighted.
        sample_weight: User-provided `sample_weight` argument.
        class_weight: User-provided `class_weight` argument.
        sample_weight_mode: One of `None` or `""temporal""`.
            `""temporal""` indicated that we expect 2D weight data
            that will be applied to the last 2 dimensions of
            the targets (i.e. we are weighting timesteps, not samples).

    # Returns
        A Numpy array of target weights, one entry per sample to weight.

    # Raises
        ValueError: In case of invalid user-provided arguments.
    "
435,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,check_num_samples,"Checks the number of samples provided for training and evaluation.

    The number of samples is not defined when running with `steps`,
    in which case the number of samples is set to `None`.

    # Arguments
        ins: List of tensors to be fed to the Keras function.
        batch_size: Integer batch size or `None` if not defined.
        steps: Total number of steps (batches of samples)
            before declaring `predict_loop` finished.
            Ignored with the default value of `None`.
        steps_name: The public API's parameter name for `steps`.

    # Raises
        ValueError: when `steps` is `None` and the attribute `ins.shape`
        does not exist. Also raises ValueError when `steps` is not `None`
        and `batch_size` is not `None` because they are mutually
        exclusive.

    # Returns
        When `steps` is `None`, returns the number of samples to be
        processed based on the size of the first dimension of the
        first input Numpy array. When `steps` is not `None` and
        `batch_size` is `None`, returns `None`.

    # Raises
        ValueError: In case of invalid arguments.
    "
436,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,iter_sequence_infinite,"Iterate indefinitely over a Sequence.

    # Arguments
        seq: Sequence object

    # Returns
        Generator yielding batches.
    "
437,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\engine\training_utils.py,is_sequence,"Determine if an object follows the Sequence API.

    # Arguments
        seq: a possible Sequence object

    # Returns
        boolean, whether the object follows the Sequence API.
    "
438,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,lecun_uniform,"LeCun uniform initializer.

    It draws samples from a uniform distribution within [-limit, limit]
    where `limit` is `sqrt(3 / fan_in)`
    where `fan_in` is the number of input units in the weight tensor.

    # Arguments
        seed: A Python integer. Used to seed the random generator.

    # Returns
        An initializer.

    # References
        - [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
    "
439,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,glorot_normal,"Glorot normal initializer, also called Xavier normal initializer.

    It draws samples from a truncated normal distribution centered on 0
    with `stddev = sqrt(2 / (fan_in + fan_out))`
    where `fan_in` is the number of input units in the weight tensor
    and `fan_out` is the number of output units in the weight tensor.

    # Arguments
        seed: A Python integer. Used to seed the random generator.

    # Returns
        An initializer.

    # References
        - [Understanding the difficulty of training deep feedforward neural
           networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
    "
440,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,glorot_uniform,"Glorot uniform initializer, also called Xavier uniform initializer.

    It draws samples from a uniform distribution within [-limit, limit]
    where `limit` is `sqrt(6 / (fan_in + fan_out))`
    where `fan_in` is the number of input units in the weight tensor
    and `fan_out` is the number of output units in the weight tensor.

    # Arguments
        seed: A Python integer. Used to seed the random generator.

    # Returns
        An initializer.

    # References
        - [Understanding the difficulty of training deep feedforward neural
           networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
    "
441,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,he_normal,"He normal initializer.

    It draws samples from a truncated normal distribution centered on 0
    with `stddev = sqrt(2 / fan_in)`
    where `fan_in` is the number of input units in the weight tensor.

    # Arguments
        seed: A Python integer. Used to seed the random generator.

    # Returns
        An initializer.

    # References
        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on
           ImageNet Classification](http://arxiv.org/abs/1502.01852)
    "
442,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,lecun_normal,"LeCun normal initializer.

    It draws samples from a truncated normal distribution centered on 0
    with `stddev = sqrt(1 / fan_in)`
    where `fan_in` is the number of input units in the weight tensor.

    # Arguments
        seed: A Python integer. Used to seed the random generator.

    # Returns
        An initializer.

    # References
        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)
        - [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
    "
443,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,he_uniform,"He uniform variance scaling initializer.

    It draws samples from a uniform distribution within [-limit, limit]
    where `limit` is `sqrt(6 / fan_in)`
    where `fan_in` is the number of input units in the weight tensor.

    # Arguments
        seed: A Python integer. Used to seed the random generator.

    # Returns
        An initializer.

    # References
        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on
           ImageNet Classification](http://arxiv.org/abs/1502.01852)
    "
444,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\initializers.py,_compute_fans,"Computes the number of input and output units for a weight shape.

    # Arguments
        shape: Integer shape tuple.
        data_format: Image data format to use for convolution kernels.
            Note that all kernels in Keras are standardized on the
            `channels_last` ordering (even when inputs are set
            to `channels_first`).

    # Returns
        A tuple of scalars, `(fan_in, fan_out)`.

    # Raises
        ValueError: in case of invalid `data_format` argument.
    "
445,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\core.py,_fix_unknown_dimension,"Finds and replaces a missing dimension in an output shape.

        This is a near direct port of the internal Numpy function
        `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`

        # Arguments
            input_shape: original shape of array being reshaped
            output_shape: target shape of the array, with at most
                a single -1 which indicates a dimension that should be
                derived from the input shape.

        # Returns
            The new output shape with a `-1` replaced with its computed value.

        # Raises
            ValueError: if `input_shape` and `output_shape` do not match.
        "
446,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,add,"Functional interface to the `Add` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the sum of the inputs.

    # Examples

    ```python
        import keras

        input1 = keras.layers.Input(shape=(16,))
        x1 = keras.layers.Dense(8, activation='relu')(input1)
        input2 = keras.layers.Input(shape=(32,))
        x2 = keras.layers.Dense(8, activation='relu')(input2)
        added = keras.layers.add([x1, x2])

        out = keras.layers.Dense(4)(added)
        model = keras.models.Model(inputs=[input1, input2], outputs=out)
    ```
    "
447,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,subtract,"Functional interface to the `Subtract` layer.

    # Arguments
        inputs: A list of input tensors (exactly 2).
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the difference of the inputs.

    # Examples

    ```python
        import keras

        input1 = keras.layers.Input(shape=(16,))
        x1 = keras.layers.Dense(8, activation='relu')(input1)
        input2 = keras.layers.Input(shape=(32,))
        x2 = keras.layers.Dense(8, activation='relu')(input2)
        subtracted = keras.layers.subtract([x1, x2])

        out = keras.layers.Dense(4)(subtracted)
        model = keras.models.Model(inputs=[input1, input2], outputs=out)
    ```
    "
448,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,multiply,"Functional interface to the `Multiply` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the element-wise product of the inputs.
    "
449,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,average,"Functional interface to the `Average` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the average of the inputs.
    "
450,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,maximum,"Functional interface to the `Maximum` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the element-wise maximum of the inputs.
    "
451,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,minimum,"Functional interface to the `Minimum` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the element-wise minimum of the inputs.
    "
452,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,concatenate,"Functional interface to the `Concatenate` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        axis: Concatenation axis.
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the concatenation of the inputs alongside axis `axis`.
    "
453,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,dot,"Functional interface to the `Dot` layer.

    # Arguments
        inputs: A list of input tensors (at least 2).
        axes: Integer or tuple of integers,
            axis or axes along which to take the dot product.
        normalize: Whether to L2-normalize samples along the
            dot product axis before taking the dot product.
            If set to True, then the output of the dot product
            is the cosine proximity between the two samples.
        **kwargs: Standard layer keyword arguments.

    # Returns
        A tensor, the dot product of the samples from the inputs.
    "
454,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\merge.py,_compute_elemwise_op_output_shape,"Computes the shape of the resultant of an elementwise operation.

        # Arguments
            shape1: tuple or None. Shape of the first tensor
            shape2: tuple or None. Shape of the second tensor

        # Returns
            expected output shape when an element-wise operation is
            carried out on 2 tensors with shapes shape1 and shape2.
            tuple or None.

        # Raises
            ValueError: if shape1 and shape2 are not compatible for
                element-wise operations.
        "
455,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\recurrent.py,_standardize_args,"Standardize `__call__` to a single list of tensor inputs.

    When running a model loaded from file, the input tensors
    `initial_state` and `constants` can be passed to `RNN.__call__` as part
    of `inputs` instead of by the dedicated keyword arguments. This method
    makes sure the arguments are separated and that `initial_state` and
    `constants` are lists of tensors (or None).

    # Arguments
        inputs: tensor or list/tuple of tensors
        initial_state: tensor or list of tensors or None
        constants: tensor or list of tensors or None

    # Returns
        inputs: tensor
        initial_state: list of tensors or None
        constants: list of tensors or None
    "
456,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\recurrent.py,get_weights,"Retrieves the weights of the model.

        # Returns
            A flat list of Numpy arrays.
        "
457,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\recurrent.py,set_weights,"Sets the weights of the model.

        # Arguments
            weights: A list of Numpy arrays with shapes and types matching
                the output of `model.get_weights()`.
        "
458,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\wrappers.py,_get_shape_tuple,"Finds non-specific dimensions in the static shapes
        and replaces them by the corresponding dynamic shapes of the tensor.

        # Arguments
            init_tuple: a tuple, the first part of the output shape
            tensor: the tensor from which to get the (static and dynamic) shapes
                as the last part of the output shape
            start_idx: int, which indicate the first dimension to take from
                the static shape of the tensor
            int_shape: an alternative static shape to take as the last part
                of the output shape

        # Returns
            The new int_shape with the first part from init_tuple
            and the last part from either `int_shape` (if provided)
            or K.int_shape(tensor), where every `None` is replaced by
            the corresponding dimension from K.shape(tensor)
        "
459,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\wrappers.py,compute_mask,"Computes an output mask tensor for Embedding layer
        based on the inputs, mask, and the inner layer.

        If batch size is specified:
        Simply return the input `mask`. (An rnn-based implementation with
        more than one rnn inputs is required but not supported in Keras yet.)

        Otherwise we call `compute_mask` of the inner layer at each time step.
        If the output mask at each time step is not `None`:
        (E.g., inner layer is Masking or RNN)
        Concatenate all of them and return the concatenation.
        If the output mask at each time step is `None` and
        the input mask is not `None`:
        (E.g., inner layer is Dense)
        Reduce the input_mask to 2 dimensions and return it.
        Otherwise (both the output mask and the input mask are `None`):
        (E.g., `mask` is not used at all)
        Return `None`.

        # Arguments
            inputs: Tensor
            mask: Tensor
        # Returns
            None or a tensor
        "
460,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\__init__.py,serialize,"Serialize a layer.

    # Arguments
        layer: a Layer object.

    # Returns
        dictionary with config.
    "
461,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\layers\__init__.py,deserialize,"Instantiate a layer from a config dictionary.

    # Arguments
        config: dict of the form {'class_name': str, 'config': dict}
        custom_objects: dict mapping class names (or function names)
            of custom (non-Keras) objects to class/functions

    # Returns
        Layer instance (may be Model, Sequential, Layer...)
    "
462,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\losses.py,logcosh,"Logarithm of the hyperbolic cosine of the prediction error.

    `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and
    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly
    like the mean squared error, but will not be so strongly affected by the
    occasional wildly incorrect prediction.

    # Arguments
        y_true: tensor of true targets.
        y_pred: tensor of predicted targets.

    # Returns
        Tensor with one scalar loss entry per sample.
    "
463,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\losses.py,get,"Get the `identifier` loss function.

    # Arguments
        identifier: None or str, name of the function.

    # Returns
        The loss function or None if `identifier` is None.

    # Raises
        ValueError if unknown identifier.
    "
464,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\models.py,_clone_functional_model,"Clone a functional `Model` instance.

    Model cloning is similar to calling a model on new inputs,
    except that it creates new layers (and thus new weights) instead
    of sharing the weights of the existing layers.

    # Arguments
        model: Instance of `Model`.
        input_tensors: optional list of input tensors
            to build the model upon. If not provided,
            placeholders will be created.

    # Returns
        An instance of `Model` reproducing the behavior
        of the original model, on top of new inputs tensors,
        using newly instantiated weights.

    # Raises
        ValueError: in case of invalid `model` argument value.
    "
465,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\models.py,_clone_sequential_model,"Clone a `Sequential` model instance.

    Model cloning is similar to calling a model on new inputs,
    except that it creates new layers (and thus new weights) instead
    of sharing the weights of the existing layers.

    # Arguments
        model: Instance of `Sequential`.
        input_tensors: optional list of input tensors
            to build the model upon. If not provided,
            placeholders will be created.

    # Returns
        An instance of `Sequential` reproducing the behavior
        of the original model, on top of new inputs tensors,
        using newly instantiated weights.

    # Raises
        ValueError: in case of invalid `model` argument value.
    "
466,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\models.py,clone_model,"Clone any `Model` instance.

    Model cloning is similar to calling a model on new inputs,
    except that it creates new layers (and thus new weights) instead
    of sharing the weights of the existing layers.

    # Arguments
        model: Instance of `Model`
            (could be a functional model or a Sequential model).
        input_tensors: optional list of input tensors
            to build the model upon. If not provided,
            placeholders will be created.

    # Returns
        An instance of `Model` reproducing the behavior
        of the original model, on top of new inputs tensors,
        using newly instantiated weights.

    # Raises
        ValueError: in case of invalid `model` argument value.
    "
467,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\optimizers.py,clip_norm,"Clip the gradient `g` if the L2 norm `n` exceeds `c`.

    # Arguments
        g: Tensor, the gradient tensor
        c: float >= 0. Gradients will be clipped
            when their L2 norm exceeds this value.
        n: Tensor, actual norm of `g`.

    # Returns
        Tensor, the gradient clipped if required.
    "
468,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\optimizers.py,deserialize,"Inverse of the `serialize` function.

    # Arguments
        config: Optimizer configuration dictionary.
        custom_objects: Optional dictionary mapping
            names (strings) to custom objects
            (classes and functions)
            to be considered during deserialization.

    # Returns
        A Keras Optimizer instance.
    "
469,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\optimizers.py,get,"Retrieves a Keras Optimizer instance.

    # Arguments
        identifier: Optimizer identifier, one of
            - String: name of an optimizer
            - Dictionary: configuration dictionary.
            - Keras Optimizer instance (it will be returned unchanged).
            - TensorFlow Optimizer instance
                (it will be wrapped as a Keras Optimizer).

    # Returns
        A Keras Optimizer instance.

    # Raises
        ValueError: If `identifier` cannot be interpreted.
    "
470,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\optimizers.py,set_weights,"Sets the weights of the optimizer, from Numpy arrays.

        Should only be called after computing the gradients
        (otherwise the optimizer has no weights).

        # Arguments
            weights: a list of Numpy arrays. The number
                of arrays and their shape must match
                number of the dimensions of the weights
                of the optimizer (i.e. it should match the
                output of `get_weights`).

        # Raises
            ValueError: in case of incompatible weight shapes.
        "
471,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\optimizers.py,get_weights,"Returns the current value of the weights of the optimizer.

        # Returns
            A list of numpy arrays.
        "
472,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\conv_utils.py,normalize_tuple,"Transforms a single int or iterable of ints into an int tuple.

    # Arguments
        value: The value to validate and convert. Could be an int, or any iterable
          of ints.
        n: The size of the tuple to be returned.
        name: The name of the argument being validated, e.g. `strides` or
          `kernel_size`. This is only used to format error messages.

    # Returns
        A tuple of n integers.

    # Raises
        ValueError: If something else than an int/long or iterable thereof was
        passed.
    "
473,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\conv_utils.py,convert_kernel,"Converts a Numpy kernel matrix from Theano format to TensorFlow format.

    Also works reciprocally, since the transformation is its own inverse.

    # Arguments
        kernel: Numpy array (3D, 4D or 5D).

    # Returns
        The converted kernel.

    # Raises
        ValueError: in case of invalid kernel shape or invalid data_format.
    "
474,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\conv_utils.py,conv_output_length,"Determines output length of a convolution given input length.

    # Arguments
        input_length: integer.
        filter_size: integer.
        padding: one of `""same""`, `""valid""`, `""full""`.
        stride: integer.
        dilation: dilation rate, integer.

    # Returns
        The output length (integer).
    "
475,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\conv_utils.py,conv_input_length,"Determines input length of a convolution given output length.

    # Arguments
        output_length: integer.
        filter_size: integer.
        padding: one of `""same""`, `""valid""`, `""full""`.
        stride: integer.

    # Returns
        The input length (integer).
    "
476,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\conv_utils.py,deconv_length,"Determines output length of a transposed convolution given input length.

    # Arguments
        dim_size: Integer, the input length.
        stride_size: Integer, the stride along the dimension of `dim_size`.
        kernel_size: Integer, the kernel size along the dimension of
            `dim_size`.
        padding: One of `""same""`, `""valid""`, `""full""`.
        output_padding: Integer, amount of padding along the output dimension,
            Can be set to `None` in which case the output length is inferred.
        dilation: dilation rate, integer.

    # Returns
        The output length (integer).
    "
477,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_extract_archive,"Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.

    # Arguments
        file_path: path to the archive file
        path: path to extract the archive file
        archive_format: Archive format to try for extracting the file.
            Options are 'auto', 'tar', 'zip', and None.
            'tar' includes tar, tar.gz, and tar.bz files.
            The default 'auto' is ['tar', 'zip'].
            None or an empty list will return no matches found.

    # Returns
        True if a match was found and an archive extraction was completed,
        False otherwise.
    "
478,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,get_file,"Downloads a file from a URL if it not already in the cache.

    By default the file at the url `origin` is downloaded to the
    cache_dir `~/.keras`, placed in the cache_subdir `datasets`,
    and given the filename `fname`. The final location of a file
    `example.txt` would therefore be `~/.keras/datasets/example.txt`.

    Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.
    Passing a hash will verify the file after download. The command line
    programs `shasum` and `sha256sum` can compute the hash.

    # Arguments
        fname: Name of the file. If an absolute path `/path/to/file.txt` is
            specified the file will be saved at that location.
        origin: Original URL of the file.
        untar: Deprecated in favor of 'extract'.
            boolean, whether the file should be decompressed
        md5_hash: Deprecated in favor of 'file_hash'.
            md5 hash of the file for verification
        file_hash: The expected hash string of the file after download.
            The sha256 and md5 hash algorithms are both supported.
        cache_subdir: Subdirectory under the Keras cache dir where the file is
            saved. If an absolute path `/path/to/folder` is
            specified the file will be saved at that location.
        hash_algorithm: Select the hash algorithm to verify the file.
            options are 'md5', 'sha256', and 'auto'.
            The default 'auto' detects the hash algorithm in use.
        extract: True tries extracting the file as an Archive, like tar or zip.
        archive_format: Archive format to try for extracting the file.
            Options are 'auto', 'tar', 'zip', and None.
            'tar' includes tar, tar.gz, and tar.bz files.
            The default 'auto' is ['tar', 'zip'].
            None or an empty list will return no matches found.
        cache_dir: Location to store cached files, when None it
            defaults to the [Keras Directory](/faq/#where-is-the-keras-configuration-filed-stored).

    # Returns
        Path to the downloaded file
    "
479,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_hash_file,"Calculates a file sha256 or md5 hash.

    # Example

    ```python
        >>> from keras.data_utils import _hash_file
        >>> _hash_file('/path/to/file.zip')
        'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
    ```

    # Arguments
        fpath: path to the file being validated
        algorithm: hash algorithm, one of 'auto', 'sha256', or 'md5'.
            The default 'auto' detects the hash algorithm in use.
        chunk_size: Bytes to read at a time, important for large files.

    # Returns
        The file hash
    "
480,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,validate_file,"Validates a file against a sha256 or md5 hash.

    # Arguments
        fpath: path to the file being validated
        file_hash:  The expected hash string of the file.
            The sha256 and md5 hash algorithms are both supported.
        algorithm: Hash algorithm, one of 'auto', 'sha256', or 'md5'.
            The default 'auto' detects the hash algorithm in use.
        chunk_size: Bytes to read at a time, important for large files.

    # Returns
        Whether the file is valid
    "
481,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,get_index,"Get the value from the Sequence `uid` at index `i`.

    To allow multiple Sequences to be used at the same time, we use `uid` to
    get a specific one. A single Sequence would cause the validation to
    overwrite the training Sequence.

    # Arguments
        uid: int, Sequence identifier
        i: index

    # Returns
        The value at index `i`.
    "
482,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,next_sample,"Get the next value from the generator `uid`.

    To allow multiple generators to be used at the same time, we use `uid` to
    get a specific one. A single generator would cause the validation to
    overwrite the training generator.

    # Arguments
        uid: int, generator identifier

    # Returns
        The next value of generator `uid`.
    "
483,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,__getitem__,"Gets batch at position `index`.

        # Arguments
            index: position of the batch in the Sequence.

        # Returns
            A batch
        "
484,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,__len__,"Number of batch in the Sequence.

        # Returns
            The number of batches in the Sequence.
        "
485,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,on_epoch_end,"Method called at the end of every epoch.
        "
486,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,__iter__,Create a generator that iterate over the Sequence.
487,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,start,"Start the handler's workers.

        # Arguments
            workers: number of worker threads
            max_queue_size: queue size
                (when full, workers could block on `put()`)
        "
488,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_send_sequence,Send current Iterable to all workers.
489,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,stop,"Stops running threads and wait for them to exit, if necessary.

        Should be called by the same thread which called `start()`.

        # Arguments
            timeout: maximum time to wait on `thread.join()`
        "
490,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_run,Submits request to the executor and queue the `Future` objects.
491,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_get_executor_init,"Get the Pool initializer for multiprocessing.

        # Returns
            Function, a Function to initialize the pool
        "
492,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,get,"Creates a generator to extract data from the queue.

        Skip the data if it is `None`.

        # Returns
            Generator yielding tuples `(inputs, targets)`
                or `(inputs, targets, sample_weights)`.
        "
493,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_get_executor_init,"Get the Pool initializer for multiprocessing.

        # Returns
            Function, a Function to initialize the pool
        "
494,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_wait_queue,Wait for the queue to be empty.
495,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_run,Submits request to the executor and queue the `Future` objects.
496,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,get,"Creates a generator to extract data from the queue.

        Skip the data if it is `None`.

        # Yields
            The next element in the queue, i.e. a tuple
            `(inputs, targets)` or
            `(inputs, targets, sample_weights)`.
        "
497,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_get_executor_init,"Get the Pool initializer for multiprocessing.

        # Returns
            Function, a Function to initialize the pool
        "
498,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,_run,Submits request to the executor and queue the `Future` objects.
499,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\data_utils.py,get,"Creates a generator to extract data from the queue.

        Skip the data if it is `None`.

        # Yields
            The next element in the queue, i.e. a tuple
            `(inputs, targets)` or
            `(inputs, targets, sample_weights)`.
        "
500,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,custom_object_scope,"Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.

    Convenience wrapper for `CustomObjectScope`.
    Code within a `with` statement will be able to access custom objects
    by name. Changes to global custom objects persist
    within the enclosing `with` statement. At end of the `with` statement,
    global custom objects are reverted to state
    at beginning of the `with` statement.

    # Example

    Consider a custom object `MyObject`

    ```python
        with custom_object_scope({'MyObject':MyObject}):
            layer = Dense(..., kernel_regularizer='MyObject')
            # save, load, etc. will recognize custom object by name
    ```

    # Arguments
        *args: Variable length list of dictionaries of name,
            class pairs to add to custom objects.

    # Returns
        Object of type `CustomObjectScope`.
    "
501,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,get_custom_objects,"Retrieves a live reference to the global dictionary of custom objects.

    Updating and clearing custom objects using `custom_object_scope`
    is preferred, but `get_custom_objects` can
    be used to directly access `_GLOBAL_CUSTOM_OBJECTS`.

    # Example

    ```python
        get_custom_objects().clear()
        get_custom_objects()['MyObject'] = MyObject
    ```

    # Returns
        Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).
    "
502,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,func_dump,"Serializes a user defined function.

    # Arguments
        func: the function to serialize.

    # Returns
        A tuple `(code, defaults, closure)`.
    "
503,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,func_load,"Deserializes a user defined function.

    # Arguments
        code: bytecode of the function.
        defaults: defaults of the function.
        closure: closure of the function.
        globs: dictionary of global objects.

    # Returns
        A function object.
    "
504,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,getargspec,"Python 2/3 compatible `getargspec`.

    Calls `getfullargspec` and assigns args, varargs,
    varkw, and defaults to a python 2/3 compatible `ArgSpec`.
    The parameter name 'varkw' is changed to 'keywords' to fit the
    `ArgSpec` struct.

    # Arguments
        fn: the target function to inspect.

    # Returns
        An ArgSpec with args, varargs, keywords, and defaults parameters
        from FullArgSpec.
    "
505,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,has_arg,"Checks if a callable accepts a given keyword argument.

    For Python 2, checks if there is an argument with the given name.

    For Python 3, checks if there is an argument with the given name, and
    also whether this argument can be called with a keyword (i.e. if it is
    not a positional-only argument).

    # Arguments
        fn: Callable to inspect.
        name: Check if `fn` can be called with `name` as a keyword argument.
        accept_all: What to return if there is no parameter called `name`
                    but the function accepts a `**kwargs` argument.

    # Returns
        bool, whether `fn` accepts a `name` keyword argument.
    "
506,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,to_list,"Normalizes a list/tensor into a list.

    If a tensor is passed, we return
    a list of size 1 containing the tensor.

    # Arguments
        x: target object to be normalized.
        allow_tuple: If False and x is a tuple,
            it will be converted into a list
            with a single element (the tuple).
            Else converts the tuple to a list.

    # Returns
        A list.
    "
507,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,unpack_singleton,"Gets the first element if the iterable has only one value.

    Otherwise return the iterable.

    # Argument:
        x: A list or tuple.

    # Returns:
        The same iterable or the first element.
    "
508,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,slice_arrays,"Slices an array or list of arrays.

    This takes an array-like, or a list of
    array-likes, and outputs:
        - arrays[start:stop] if `arrays` is an array-like
        - [x[start:stop] for x in arrays] if `arrays` is a list

    Can also work on list/array of indices: `_slice_arrays(x, indices)`

    # Arguments
        arrays: Single array or list of arrays.
        start: can be an integer index (start index)
            or a list/array of indices
        stop: integer (stop index); should be None if
            `start` was a list.

    # Returns
        A slice of the array(s).
    "
509,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,transpose_shape,"Converts a tuple or a list to the correct `data_format`.

    It does so by switching the positions of its elements.

    # Arguments
        shape: Tuple or list, often representing shape,
            corresponding to `'channels_last'`.
        target_format: A string, either `'channels_first'` or `'channels_last'`.
        spatial_axes: A tuple of integers.
            Correspond to the indexes of the spatial axes.
            For example, if you pass a shape
            representing (batch_size, timesteps, rows, cols, channels),
            then `spatial_axes=(2, 3)`.

    # Returns
        A tuple or list, with the elements permuted according
        to `target_format`.

    # Example
    ```python
        >>> from keras.utils.generic_utils import transpose_shape
        >>> transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))
        (16, 32, 128, 128)
        >>> transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))
        (16, 128, 128, 32)
        >>> transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))
        (32, 128, 128)
    ```

    # Raises
        ValueError: if `value` or the global `data_format` invalid.
    "
510,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\generic_utils.py,update,"Updates the progress bar.

        # Arguments
            current: Index of current step.
            values: List of tuples:
                `(name, value_for_last_step)`.
                If `name` is in `stateful_metrics`,
                `value_for_last_step` will be displayed as-is.
                Else, an average of the metric over time will be displayed.
        "
511,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\io_utils.py,ask_to_proceed_with_overwrite,"Produces a prompt asking about overwriting a file.

    # Arguments
        filepath: the path to the file to be overwritten.

    # Returns
        True if we can proceed with overwrite, False otherwise.
    "
512,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\io_utils.py,shape,"Gets a numpy-style shape tuple giving the dataset dimensions.

        # Returns
            A numpy-style shape tuple.
        "
513,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\io_utils.py,dtype,"Gets the datatype of the dataset.

        # Returns
            A numpy dtype string.
        "
514,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\io_utils.py,ndim,"Gets the number of dimensions (rank) of the dataset.

        # Returns
            An integer denoting the number of dimensions (rank) of the dataset.
        "
515,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\io_utils.py,size,"Gets the total dataset size (number of elements).

        # Returns
            An integer denoting the number of elements in the dataset.
        "
516,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\layer_utils.py,count_params,"Count the total number of scalars composing the weights.

    # Arguments
        weights: An iterable containing the weights on which to compute params

    # Returns
        The total number of scalars composing the weights
    "
517,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\layer_utils.py,print_summary,"Prints a summary of a model.

    # Arguments
        model: Keras model instance.
        line_length: Total length of printed lines
            (e.g. set this to adapt the display to different
            terminal window sizes).
        positions: Relative or absolute positions of log elements in each line.
            If not provided, defaults to `[.33, .55, .67, 1.]`.
        print_fn: Print function to use.
            It will be called on each line of the summary.
            You can set it to a custom function
            in order to capture the string summary.
            It defaults to `print` (prints to stdout).
    "
518,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\layer_utils.py,convert_all_kernels_in_model,"Converts all convolution kernels in a model from Theano to TensorFlow.

    Also works from TensorFlow to Theano.

    # Arguments
        model: target model for the conversion.
    "
519,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\layer_utils.py,convert_dense_weights_data_format,"Utility useful when changing a convnet's `data_format`.

    When porting the weights of a convnet from one data format to the other,
    if the convnet includes a `Flatten` layer
    (applied to the last convolutional feature map)
    followed by a `Dense` layer, the weights of that `Dense` layer
    should be updated to reflect the new dimension ordering.

    # Arguments
        dense: The target `Dense` layer.
        previous_feature_map_shape: A shape tuple of 3 integers,
            e.g. `(512, 7, 7)`. The shape of the convolutional
            feature map right before the `Flatten` layer that
            came before the target `Dense` layer.
        target_data_format: One of ""channels_last"", ""channels_first"".
            Set it ""channels_last""
            if converting a ""channels_first"" model to ""channels_last"",
            or reciprocally.
    "
520,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\layer_utils.py,get_source_inputs,"Returns the list of input tensors necessary to compute `tensor`.

    Output will always be a list of tensors
    (potentially with 1 element).

    # Arguments
        tensor: The tensor to start from.
        layer: Origin layer of the tensor. Will be
            determined via tensor._keras_history if not provided.
        node_index: Origin node index of the tensor.

    # Returns
        List of input tensors.
    "
521,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\multi_gpu_utils.py,multi_gpu_model,"Replicates a model on different GPUs.

    Specifically, this function implements single-machine
    multi-GPU data parallelism. It works in the following way:

    - Divide the model's input(s) into multiple sub-batches.
    - Apply a model copy on each sub-batch. Every model copy
        is executed on a dedicated GPU.
    - Concatenate the results (on CPU) into one big batch.

    E.g. if your `batch_size` is 64 and you use `gpus=2`,
    then we will divide the input into 2 sub-batches of 32 samples,
    process each sub-batch on one GPU, then return the full
    batch of 64 processed samples.

    This induces quasi-linear speedup on up to 8 GPUs.

    This function is only available with the TensorFlow backend
    for the time being.

    # Arguments
        model: A Keras model instance. To avoid OOM errors,
            this model could have been built on CPU, for instance
            (see usage example below).
        gpus: Integer >= 2 or list of integers, number of GPUs or
            list of GPU IDs on which to create model replicas.
        cpu_merge: A boolean value to identify whether to force
            merging model weights under the scope of the CPU or not.
        cpu_relocation: A boolean value to identify whether to
            create the model's weights under the scope of the CPU.
            If the model is not defined under any preceding device
            scope, you can still rescue it by activating this option.

    # Returns
        A Keras `Model` instance which can be used just like the initial
        `model` argument, but which distributes its workload on multiple GPUs.

    # Examples

    Example 1 - Training models with weights merge on CPU

    ```python
        import tensorflow as tf
        from keras.applications import Xception
        from keras.utils import multi_gpu_model
        import numpy as np

        num_samples = 1000
        height = 224
        width = 224
        num_classes = 1000

        # Instantiate the base model (or ""template"" model).
        # We recommend doing this with under a CPU device scope,
        # so that the model's weights are hosted on CPU memory.
        # Otherwise they may end up hosted on a GPU, which would
        # complicate weight sharing.
        with tf.device('/cpu:0'):
            model = Xception(weights=None,
                             input_shape=(height, width, 3),
                             classes=num_classes)

        # Replicates the model on 8 GPUs.
        # This assumes that your machine has 8 available GPUs.
        parallel_model = multi_gpu_model(model, gpus=8)
        parallel_model.compile(loss='categorical_crossentropy',
                               optimizer='rmsprop')

        # Generate dummy data.
        x = np.random.random((num_samples, height, width, 3))
        y = np.random.random((num_samples, num_classes))

        # This `fit` call will be distributed on 8 GPUs.
        # Since the batch size is 256, each GPU will process 32 samples.
        parallel_model.fit(x, y, epochs=20, batch_size=256)

        # Save model via the template model (which shares the same weights):
        model.save('my_model.h5')
    ```

    Example 2 - Training models with weights merge on CPU using cpu_relocation

    ```python
         ..
         # Not needed to change the device scope for model definition:
         model = Xception(weights=None, ..)

         try:
             parallel_model = multi_gpu_model(model, cpu_relocation=True)
             print(""Training using multiple GPUs.."")
         except ValueError:
             parallel_model = model
             print(""Training using single GPU or CPU.."")
         parallel_model.compile(..)
         ..
    ```

    Example 3 - Training models with weights merge on GPU (recommended for NV-link)

    ```python
         ..
         # Not needed to change the device scope for model definition:
         model = Xception(weights=None, ..)

         try:
             parallel_model = multi_gpu_model(model, cpu_merge=False)
             print(""Training using multiple GPUs.."")
         except:
             parallel_model = model
             print(""Training using single GPU or CPU.."")

         parallel_model.compile(..)
         ..
    ```

    # On model saving

    To save the multi-gpu model, use `.save(fname)` or `.save_weights(fname)`
    with the template model (the argument you passed to `multi_gpu_model`),
    rather than the model returned by `multi_gpu_model`.
    "
522,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\np_utils.py,to_categorical,"Converts a class vector (integers) to binary class matrix.

    E.g. for use with categorical_crossentropy.

    # Arguments
        y: class vector to be converted into a matrix
            (integers from 0 to num_classes).
        num_classes: total number of classes.
        dtype: The data type expected by the input, as a string
            (`float32`, `float64`, `int32`...)

    # Returns
        A binary matrix representation of the input. The classes axis
        is placed last.

    # Example

    ```python
    # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:
    > labels
    array([0, 2, 1, 2, 0])
    # `to_categorical` converts this into a matrix with as many
    # columns as there are classes. The number of rows
    # stays the same.
    > to_categorical(labels)
    array([[ 1.,  0.,  0.],
           [ 0.,  0.,  1.],
           [ 0.,  1.,  0.],
           [ 0.,  0.,  1.],
           [ 1.,  0.,  0.]], dtype=float32)
    ```
    "
523,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\np_utils.py,normalize,"Normalizes a Numpy array.

    # Arguments
        x: Numpy array to normalize.
        axis: axis along which to normalize.
        order: Normalization order (e.g. 2 for L2 norm).

    # Returns
        A normalized copy of the array.
    "
524,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,get_test_data,"Generates test data to train a model on.

    classification=True overrides output_shape
    (i.e. output_shape is set to (1,)) and the output
    consists in integers in [0, num_classes-1].

    Otherwise: float output with shape output_shape.
    "
525,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,layer_test,"Test routine for a layer with a single input tensor
    and single output tensor.
    "
526,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,bucket_path,Returns the full GCS bucket path
527,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,get_filepath,Returns filename appended to bucketpath
528,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,FileIO,"Proxy for tensorflow.python.lib.io.file_io.FileIO class. Mocks the class
        if a real GCS bucket is not available for testing.
        "
529,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,file_exists,"Proxy for tensorflow.python.lib.io.file_io.file_exists class. Mocks the
        function if a real GCS bucket is not available for testing.
        "
530,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,delete_file,"Proxy for tensorflow.python.lib.io.file_io.delete_file function. Mocks
        the function if a real GCS bucket is not available for testing.
        "
531,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,assert_exists,Convenience method for verifying that a file exists after writing.
532,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,start,"Start mocking of `self.file_io_module` if real bucket not
        available for testing"
533,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\test_utils.py,stop,"Stop mocking of `self.file_io_module` if real bucket not
        available for testing"
534,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\vis_utils.py,_check_pydot,Raise errors if `pydot` or GraphViz unavailable.
535,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\vis_utils.py,model_to_dot,"Convert a Keras model to dot format.

    # Arguments
        model: A Keras model instance.
        show_shapes: whether to display shape information.
        show_layer_names: whether to display layer names.
        rankdir: `rankdir` argument passed to PyDot,
            a string specifying the format of the plot:
            'TB' creates a vertical plot;
            'LR' creates a horizontal plot.
        expand_nested: whether to expand nested models into clusters.
        dpi: dot DPI.
        subgraph: whether to return a pydot.Cluster instance.

    # Returns
        A `pydot.Dot` instance representing the Keras model or
        a `pydot.Cluster` instance representing nested model if
        `subgraph=True`.
    "
536,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\utils\vis_utils.py,plot_model,"Converts a Keras model to dot format and save to a file.

    # Arguments
        model: A Keras model instance
        to_file: File name of the plot image.
        show_shapes: whether to display shape information.
        show_layer_names: whether to display layer names.
        rankdir: `rankdir` argument passed to PyDot,
            a string specifying the format of the plot:
            'TB' creates a vertical plot;
            'LR' creates a horizontal plot.
        expand_nested: whether to expand nested models into clusters.
        dpi: dot DPI.
    "
537,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,check_params,"Checks for user typos in `params`.

        # Arguments
            params: dictionary; the parameters to be checked

        # Raises
            ValueError: if any member of `params` is not a valid argument.
        "
538,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,get_params,"Gets parameters for this estimator.

        # Arguments
            **params: ignored (exists for API compatibility).

        # Returns
            Dictionary of parameter names mapped to their values.
        "
539,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,set_params,"Sets the parameters of this estimator.

        # Arguments
            **params: Dictionary of parameter names mapped to their values.

        # Returns
            self
        "
540,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,fit,"Constructs a new model with `build_fn` & fit the model to `(x, y)`.

        # Arguments
            x : array-like, shape `(n_samples, n_features)`
                Training samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`
                True labels for `x`.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.fit`

        # Returns
            history : object
                details about the training history at each epoch.
        "
541,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,filter_sk_params,"Filters `sk_params` and returns those in `fn`'s arguments.

        # Arguments
            fn : arbitrary function
            override: dictionary, values to override `sk_params`

        # Returns
            res : dictionary containing variables
                in both `sk_params` and `fn`'s arguments.
        "
542,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,fit,"Constructs a new model with `build_fn` & fit the model to `(x, y)`.

        # Arguments
            x : array-like, shape `(n_samples, n_features)`
                Training samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`
                True labels for `x`.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.fit`

        # Returns
            history : object
                details about the training history at each epoch.

        # Raises
            ValueError: In case of invalid shape for `y` argument.
        "
543,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,predict,"Returns the class predictions for the given test data.

        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            **kwargs: dictionary arguments
                Legal arguments are the arguments
                of `Sequential.predict_classes`.

        # Returns
            preds: array-like, shape `(n_samples,)`
                Class predictions.
        "
544,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,predict_proba,"Returns class probability estimates for the given test data.

        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            **kwargs: dictionary arguments
                Legal arguments are the arguments
                of `Sequential.predict_classes`.

        # Returns
            proba: array-like, shape `(n_samples, n_outputs)`
                Class probability estimates.
                In the case of binary classification,
                to match the scikit-learn API,
                will return an array of shape `(n_samples, 2)`
                (instead of `(n_sample, 1)` as in Keras).
        "
545,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,score,"Returns the mean accuracy on the given test data and labels.

        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`
                True labels for `x`.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.evaluate`.

        # Returns
            score: float
                Mean accuracy of predictions on `x` wrt. `y`.

        # Raises
            ValueError: If the underlying model isn't configured to
                compute accuracy. You should pass `metrics=[""accuracy""]` to
                the `.compile()` method of the model.
        "
546,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,predict,"Returns predictions for the given test data.

        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.predict`.

        # Returns
            preds: array-like, shape `(n_samples,)`
                Predictions.
        "
547,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\keras\wrappers\scikit_learn.py,score,"Returns the mean loss on the given test data and labels.

        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            y: array-like, shape `(n_samples,)`
                True labels for `x`.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.evaluate`.

        # Returns
            score: float
                Mean accuracy of predictions on `x` wrt. `y`.
        "
548,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\conftest.py,clear_session_after_test,"Test wrapper to clean up after TensorFlow and CNTK tests.

    This wrapper runs for all the tests in the keras test suite.
    "
549,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\preprocessing\text_test.py,test_tokenizer_oov_flag,"
    Test of Out of Vocabulary (OOV) flag in Tokenizer
    "
550,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_temporal_data_tasks.py,test_temporal_classification,"
    Classify temporal sequences of float numbers
    of length 3 into 2 classes using
    single layer of GRU units and softmax applied
    to the last activations of the units
    "
551,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_temporal_data_tasks.py,test_temporal_classification_functional,"
    Classify temporal sequences of float numbers
    of length 3 into 2 classes using
    single layer of GRU units and softmax applied
    to the last activations of the units
    "
552,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_temporal_data_tasks.py,test_temporal_regression,"
    Predict float numbers (regression) based on sequences
    of float numbers of length 3 using a single layer of GRU units
    "
553,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_temporal_data_tasks.py,test_3d_to_3d,"
    Apply a same Dense layer for each element of time dimension of the input
    and make predictions of the output sequence elements.
    This does not make use of the temporal structure of the sequence
    (see TimeDistributedDense for more details)
    "
554,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_temporal_data_tasks.py,test_stacked_lstm_char_prediction,"
    Learn alphabetical char sequence with stacked LSTM.
    Predict the whole alphabet based on the first two letters ('ab' -> 'ab...z')
    See non-toy example in examples/lstm_text_generation.py
    "
555,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_temporal_data_tasks.py,test_masked_temporal,"
    Confirm that even with masking on both inputs and outputs, cross-entropies are
    of the expected scale.

    In this task, there are variable length inputs of integers from 1-9, and a random
    subset of unmasked outputs. Each of these outputs has a 50% probability of being
    the input number unchanged, and a 50% probability of being 2*input%10.

    The ground-truth best cross-entropy loss should, then be -log(0.5) = 0.69

    "
556,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_vector_data_tasks.py,test_vector_classification,"
    Classify random float vectors into 2 classes with logistic regression
    using 2 layer neural network with ReLU hidden units.
    "
557,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\integration_tests\test_vector_data_tasks.py,test_vector_regression,"
    Perform float data prediction (regression) using 2 layer MLP
    with tanh and sigmoid activations.
    "
558,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,get_standard_values,"A set of floats used for testing the activations.
    "
559,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_get_fn,"Activations has a convenience ""get"" function. All paths of this
    function are tested here, although the behaviour in some instances
    seems potentially surprising (e.g. situation 3)
    "
560,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_softmax_valid,"Test using a reference implementation of softmax.
    "
561,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_softmax_invalid,"Test for the expected exception behaviour on invalid input
    "
562,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_softmax_3d,"Test using a reference implementation of softmax.
    "
563,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_softplus,"Test using a reference softplus implementation.
    "
564,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_softsign,"Test using a reference softsign implementation.
    "
565,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_sigmoid,"Test using a numerically stable reference sigmoid implementation.
    "
566,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\activations_test.py,test_hard_sigmoid,"Test using a reference hard sigmoid implementation.
    "
567,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\backend\backend_test.py,test_logsumexp,"
        Check if K.logsumexp works properly for values close to one.
        "
568,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\backend\backend_test.py,test_logsumexp_optim,"
        Check if optimization works.
        "
569,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\backend\backend_test.py,test_ctc_decode_greedy,Test two batch entries - best path decoder.
570,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\backend\backend_test.py,test_ctc_decode_beam_search,"Test one batch, two beams - hibernating beam search."
571,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\backend\backend_test.py,test_set_floatx,"
        Make sure that changes to the global floatx are effectively
        taken into account by the backend.
        "
572,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\engine\test_topology.py,test_preprocess_weights_for_loading_rnn_should_be_idempotent,"
    Loading weights from a RNN class to itself should not convert the weights.
    "
573,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\engine\test_topology.py,test_multi_output_mask,Fixes #7589
574,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\engine\test_training.py,threadsafe_generator,"A decorator that takes a generator function and makes it thread-safe.
    "
575,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\engine\test_training.py,test_model_with_input_feed_tensor,"We test building a model with a TF variable as input.
    We should be able to call fit, evaluate, predict,
    by only passing them data for the placeholder inputs
    in the model.
    "
576,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\engine\test_training.py,test_trainable_weights_count_consistency,"Tests the trainable weights consistency check of Model.

    This verifies that a warning is shown if model.trainable is modified
    and the model is summarized/run without a new call to .compile()

    Reproduce issue #8121
    "
577,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\engine\test_training.py,test_model_with_crossentropy_losses_channels_first,"Tests use of all crossentropy losses with `channels_first`.

    Tests `sparse_categorical_crossentropy`, `categorical_crossentropy`,
    and `binary_crossentropy`.
    Verifies that evaluate gives the same result with either
    `channels_first` or `channels_last` image_data_format.
    Tests PR #9715.
    "
578,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\layers\normalization_test.py,test_shared_batchnorm,"Test that a BN layer can be shared
    across different data streams.
    "
579,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\layers\recurrent_test.py,test_masking_layer," This test based on a previously failing issue here:
    https://github.com/keras-team/keras/issues/1567
    "
580,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\layers\recurrent_test.py,test_specify_state_with_masking," This test based on a previously failing issue here:
    https://github.com/keras-team/keras/issues/1567
    "
581,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\legacy\conftest.py,clear_session_after_test,"This wrapper runs for all the tests in the legacy directory (recursively).
    "
582,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\metrics_test.py,test_serialize,"This is a mock 'round trip' of serialize and deserialize.
    "
583,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\metrics_test.py,__call__,"Computes the number of true positives in a batch.

            # Arguments
                y_true: Tensor, batch_wise labels
                y_pred: Tensor, batch_wise predictions

            # Returns
                The total number of true positives seen this epoch at the
                    completion of the batch.
            "
584,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\test_callbacks.py,_check_counts,Checks that the counts registered by `counter` are those expected.
585,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\test_sequential_model.py,in_tmpdir,"Runs a function in a temporary directory.

    Checks that the directory is empty afterwards.
    "
586,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\data_utils_test.py,use_spawn,"Decorator which uses `spawn` when possible.
    This is useful on Travis to avoid memory issues.
    "
587,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\data_utils_test.py,in_tmpdir,"Runs a function in a temporary directory.

    Checks that the directory is empty afterwards.
    "
588,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\data_utils_test.py,test_data_utils,"Tests get_file from a url, plus extraction and validation.
    "
589,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\data_utils_test.py,threadsafe_generator,"A decorator that takes a generator function and makes it thread-safe.
    "
590,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\data_utils_test.py,test_generator_enqueuer_threads,"
     Not comparing the order since it is not guaranteed.
     It may get ordered, but not a lot, one thread can take
     the GIL before he was supposed to.
    "
591,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\io_utils_test.py,in_tmpdir,"Runs a function in a temporary directory.

    Checks that the directory is empty afterwards.
    "
592,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\io_utils_test.py,test_io_utils,"Tests the HDF5Matrix code using the sample from @jfsantos at
    https://gist.github.com/jfsantos/e2ef822c744357a4ed16ec0c885100a3
    "
593,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\io_utils_test.py,test_H5Dict_accepts_pathlib_Path,GitHub issue: 11459
594,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\multi_gpu_test.py,multi_gpu_application_folder_generator_benchmark,"Before running this test:

    wget https://s3.amazonaws.com/img-datasets/cats_and_dogs_small.zip
    unzip cats_and_dogs_small.zip
    "
595,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\keras\utils\vis_utils_test.py,test_plot_sequential_embedding,Fixes #11376
596,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_doc_auto_generation.py,test_doc_multiple_sections_code, Checks that we can have code blocks in multiple sections.
597,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_pickling.py,test_pickling_without_compilation,"Test pickling model without compiling.
    "
598,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_saving_without_compilation,"Test saving model without compiling.
    "
599,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_loading_weights_by_name_and_reshape,"
    test loading model weights by name on:
        - sequential model
    "
600,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_loading_weights_by_name_2,"
    test loading model weights by name on:
        - both sequential and functional api models
        - different architecture with shared names
    "
601,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_loading_weights_by_name_skip_mismatch,"
    test skipping layers while loading model weights by name on:
        - sequential model
    "
602,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_saving_constant_initializer_with_numpy,"Test saving and loading model of constant initializer with numpy inputs.
    "
603,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_load_weights_between_noncudnn_rnn_time_distributed,"
    Similar test as  test_load_weights_between_noncudnn_rnn() but has different
    rank of input due to usage of TimeDistributed. Issue: #10356.
    "
604,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_model_saving.py,test_preprocess_weights_for_loading_gru_incompatible,"
    Loading weights between incompatible layers should fail fast with an exception.
    "
605,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_multiprocessing.py,use_spawn,"Decorator which uses `spawn` when possible.
    This is useful on Travis to avoid memory issues.
    "
606,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_multiprocessing.py,threadsafe_generator,"A decorator that takes a generator function and makes it thread-safe.
    "
607,keras,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\keras\tests\test_multiprocessing.py,in_tmpdir,"Runs a function in a temporary directory.

    Checks that the directory is empty afterwards.
    "
index,project,file,func_name,func_doc
index,project,file,func_name,func_doc
1,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,send,"Sends PreparedRequest object. Returns Response object.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple
        :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        "
2,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,close,Cleans up adapter specific items.
3,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,init_poolmanager,"Initializes a urllib3 PoolManager.

        This method should not be called from user code, and is only
        exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param connections: The number of urllib3 connection pools to cache.
        :param maxsize: The maximum number of connections to save in the pool.
        :param block: Block when no free connections are available.
        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
        "
4,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,proxy_manager_for,"Return urllib3 ProxyManager for the given proxy.

        This method should not be called from user code, and is only
        exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param proxy: The proxy to return a urllib3 ProxyManager for.
        :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
        :returns: ProxyManager
        :rtype: urllib3.ProxyManager
        "
5,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,cert_verify,"Verify a SSL certificate. This method should not be called from user
        code, and is only exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param conn: The urllib3 connection object associated with the cert.
        :param url: The requested URL.
        :param verify: Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use
        :param cert: The SSL certificate to verify.
        "
6,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,build_response,"Builds a :class:`Response <requests.Response>` object from a urllib3
        response. This should not be called from user code, and is only exposed
        for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`

        :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
        :param resp: The urllib3 response object.
        :rtype: requests.Response
        "
7,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,get_connection,"Returns a urllib3 connection for the given URL. This should not be
        called from user code, and is only exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param url: The URL to connect to.
        :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
        :rtype: urllib3.ConnectionPool
        "
8,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,close,"Disposes of any internal state.

        Currently, this closes the PoolManager and any active ProxyManager,
        which closes any pooled connections.
        "
9,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,request_url,"Obtain the url to use when making the final request.

        If the message is being sent through a HTTP proxy, the full URL has to
        be used. Otherwise, we should only use the path portion of the URL.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.
        :rtype: str
        "
10,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,add_headers,"Add any headers needed by the connection. As of v2.0 this does
        nothing by default, but is left for overriding by users that subclass
        the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
        :param kwargs: The keyword arguments from the call to send().
        "
11,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,proxy_headers,"Returns a dictionary of the headers to add to any request sent
        through a proxy. This works with urllib3 magic to ensure that they are
        correctly sent to the proxy, rather than in a tunnelled request if
        CONNECT is being used.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param proxy: The url of the proxy being used for this request.
        :rtype: dict
        "
12,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\adapters.py,send,"Sends PreparedRequest object. Returns Response object.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        "
13,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,request,"Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      <Response [200]>
    "
14,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,get,"Sends a GET request.

    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
15,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,options,"Sends an OPTIONS request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
16,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,head,"Sends a HEAD request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
17,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,post,"Sends a POST request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
18,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,put,"Sends a PUT request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
19,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,patch,"Sends a PATCH request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
20,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\api.py,delete,"Sends a DELETE request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    "
21,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\auth.py,_basic_auth_str,Returns a Basic Auth string.
22,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\auth.py,build_digest_header,"
        :rtype: str
        "
23,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\auth.py,handle_redirect,Reset num_401_calls counter on redirects.
24,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\auth.py,handle_401,"
        Takes the given response and tries digest-auth, if needed.

        :rtype: requests.Response
        "
25,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,extract_cookies_to_jar,"Extract the cookies from the response into a CookieJar.

    :param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)
    :param request: our own requests.Request object
    :param response: urllib3.HTTPResponse object
    "
26,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,get_cookie_header,"
    Produce an appropriate Cookie header string to be sent with `request`, or None.

    :rtype: str
    "
27,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,remove_cookie_by_name,"Unsets a cookie by name, by default over all domains and paths.

    Wraps CookieJar.clear(), is O(n).
    "
28,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,create_cookie,"Make a cookie from underspecified parameters.

    By default, the pair of `name` and `value` will be set for the domain ''
    and sent on every request (this is sometimes called a ""supercookie"").
    "
29,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,morsel_to_cookie,Convert a Morsel object into a Cookie containing the one k/v pair.
30,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,cookiejar_from_dict,"Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    "
31,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,merge_cookies,"Add cookies to cookiejar and returns a merged CookieJar.

    :param cookiejar: CookieJar object to add the cookies to.
    :param cookies: Dictionary or CookieJar object to be added.
    :rtype: CookieJar
    "
32,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,add_header,cookielib has no legitimate use for this method; add it back if you find one.
33,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,__init__,"Make a MockResponse for `cookielib` to read.

        :param headers: a httplib.HTTPMessage or analogous carrying the headers
        "
34,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,get,"Dict-like get() that also supports optional domain and path args in
        order to resolve naming collisions from using one cookie jar over
        multiple domains.

        .. warning:: operation is O(n), not O(1).
        "
35,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,set,"Dict-like set() that also supports optional domain and path args in
        order to resolve naming collisions from using one cookie jar over
        multiple domains.
        "
36,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,iterkeys,"Dict-like iterkeys() that returns an iterator of names of cookies
        from the jar.

        .. seealso:: itervalues() and iteritems().
        "
37,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,keys,"Dict-like keys() that returns a list of names of cookies from the
        jar.

        .. seealso:: values() and items().
        "
38,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,itervalues,"Dict-like itervalues() that returns an iterator of values of cookies
        from the jar.

        .. seealso:: iterkeys() and iteritems().
        "
39,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,values,"Dict-like values() that returns a list of values of cookies from the
        jar.

        .. seealso:: keys() and items().
        "
40,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,iteritems,"Dict-like iteritems() that returns an iterator of name-value tuples
        from the jar.

        .. seealso:: iterkeys() and itervalues().
        "
41,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,items,"Dict-like items() that returns a list of name-value tuples from the
        jar. Allows client-code to call ``dict(RequestsCookieJar)`` and get a
        vanilla python dict of key value pairs.

        .. seealso:: keys() and values().
        "
42,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,list_domains,Utility method to list all the domains in the jar.
43,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,list_paths,Utility method to list all the paths in the jar.
44,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,multiple_domains,"Returns True if there are multiple domains in the jar.
        Returns False otherwise.

        :rtype: bool
        "
45,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,get_dict,"Takes as an argument an optional domain and path and returns a plain
        old Python dict of name-value pairs of cookies that meet the
        requirements.

        :rtype: dict
        "
46,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,__getitem__,"Dict-like __getitem__() for compatibility with client code. Throws
        exception if there are more than one cookie with name. In that case,
        use the more explicit get() method instead.

        .. warning:: operation is O(n), not O(1).
        "
47,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,__setitem__,"Dict-like __setitem__ for compatibility with client code. Throws
        exception if there is already a cookie of that name in the jar. In that
        case, use the more explicit set() method instead.
        "
48,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,__delitem__,"Deletes a cookie given a name. Wraps ``cookielib.CookieJar``'s
        ``remove_cookie_by_name()``.
        "
49,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,update,Updates this jar with cookies from another CookieJar or dict-like
50,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,_find,"Requests uses this method internally to get cookie values.

        If there are conflicting cookies, _find arbitrarily chooses one.
        See _find_no_duplicates if you want an exception thrown if there are
        conflicting cookies.

        :param name: a string containing name of cookie
        :param domain: (optional) string containing domain of cookie
        :param path: (optional) string containing path of cookie
        :return: cookie.value
        "
51,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,_find_no_duplicates,"Both ``__get_item__`` and ``get`` call this function: it's never
        used elsewhere in Requests.

        :param name: a string containing name of cookie
        :param domain: (optional) string containing domain of cookie
        :param path: (optional) string containing path of cookie
        :raises KeyError: if cookie is not found
        :raises CookieConflictError: if there are multiple cookies
            that match name and optionally domain and path
        :return: cookie.value
        "
52,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,__getstate__,"Unlike a normal CookieJar, this class is pickleable."
53,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,__setstate__,"Unlike a normal CookieJar, this class is pickleable."
54,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,copy,Return a copy of this RequestsCookieJar.
55,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\cookies.py,get_policy,Return the CookiePolicy instance used.
56,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\exceptions.py,__init__,Initialize RequestException with `request` and `response` objects.
57,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\help.py,_implementation,"Return a dict with the Python implementation and version.

    Provide both the name and the version of the Python implementation
    currently running. For example, on CPython 2.7.5 it will return
    {'name': 'CPython', 'version': '2.7.5'}.

    This function works best on CPython and PyPy: in particular, it probably
    doesn't work for Jython or IronPython. Future investigation should be done
    to work out the correct shape of the code for those platforms.
    "
58,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\help.py,info,Generate information for a bug report.
59,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\help.py,main,Pretty-print the bug information as JSON.
60,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\hooks.py,dispatch_hook,Dispatches a hook dictionary on a given piece of data.
61,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,path_url,Build the path URL to use.
62,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,_encode_params,"Encode parameters in a piece of data.

        Will successfully encode parameters when passed as a dict or a list of
        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
        if parameters are supplied as a dict.
        "
63,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,_encode_files,"Build the body for a multipart/form-data request.

        Will successfully encode files when passed as a dict or a list of
        tuples. Order is retained if data is a list of tuples but arbitrary
        if parameters are supplied as a dict.
        The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)
        or 4-tuples (filename, fileobj, contentype, custom_headers).
        "
64,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,register_hook,Properly register a hook.
65,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,deregister_hook,"Deregister a previously registered hook.
        Returns True if the hook existed, False if not.
        "
66,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare,Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.
67,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare,Prepares the entire request with the given parameters.
68,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_method,Prepares the given HTTP method.
69,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_url,Prepares the given HTTP URL.
70,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_headers,Prepares the given HTTP headers.
71,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_body,Prepares the given HTTP body data.
72,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_content_length,Prepare Content-Length header based on request method and body
73,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_auth,Prepares the given HTTP auth data.
74,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_cookies,"Prepares the given HTTP cookie data.

        This function eventually generates a ``Cookie`` header from the
        given cookies using cookielib. Due to cookielib's design, the header
        will not be regenerated if it already exists, meaning this function
        can only be called once for the life of the
        :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
        to ``prepare_cookies`` will have no actual effect, unless the ""Cookie""
        header is removed beforehand.
        "
75,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,prepare_hooks,Prepares the given hooks.
76,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,__bool__,"Returns True if :attr:`status_code` is less than 400.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code, is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        "
77,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,__nonzero__,"Returns True if :attr:`status_code` is less than 400.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code, is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        "
78,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,__iter__,Allows you to use a response as an iterator.
79,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,ok,"Returns True if :attr:`status_code` is less than 400, False if not.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        "
80,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,is_redirect,"True if this Response is a well-formed HTTP redirect that could have
        been processed automatically (by :meth:`Session.resolve_redirects`).
        "
81,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,is_permanent_redirect,True if this Response one of the permanent versions of redirect.
82,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,next,"Returns a PreparedRequest for the next request in a redirect chain, if there is one."
83,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,apparent_encoding,"The apparent encoding, provided by the chardet library."
84,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,iter_content,"Iterates over the response data.  When stream=True is set on the
        request, this avoids reading the content at once into memory for
        large responses.  The chunk size is the number of bytes it should
        read into memory.  This is not necessarily the length of each item
        returned as decoding can take place.

        chunk_size must be of type int or None. A value of None will
        function differently depending on the value of `stream`.
        stream=True will read data as it arrives in whatever size the
        chunks are received. If stream=False, data is returned as
        a single chunk.

        If decode_unicode is True, content will be decoded using the best
        available encoding based on the response.
        "
85,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,iter_lines,"Iterates over the response data, one line at a time.  When
        stream=True is set on the request, this avoids reading the
        content at once into memory for large responses.

        .. note:: This method is not reentrant safe.
        "
86,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,content,"Content of the response, in bytes."
87,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,text,"Content of the response, in unicode.

        If Response.encoding is None, encoding will be guessed using
        ``chardet``.

        The encoding of the response content is determined based solely on HTTP
        headers, following RFC 2616 to the letter. If you can take advantage of
        non-HTTP knowledge to make a better guess at the encoding, you should
        set ``r.encoding`` appropriately before accessing this property.
        "
88,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,json,"Returns the json-encoded content of a response, if any.

        :param \*\*kwargs: Optional arguments that ``json.loads`` takes.
        :raises ValueError: If the response body does not contain valid json.
        "
89,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,links,"Returns the parsed header links of the response, if any."
90,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,raise_for_status,"Raises stored :class:`HTTPError`, if one occurred."
91,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\models.py,close,"Releases the connection back to the pool. Once this method has been
        called the underlying ``raw`` object must not be accessed again.

        *Note: Should not normally need to be called explicitly.*
        "
92,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,merge_setting,"Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    "
93,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,merge_hooks,"Properly merges both requests and session hooks.

    This is necessary because when request_hooks == {'response': []}, the
    merge breaks Session hooks entirely.
    "
94,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,session,"
    Returns a :class:`Session` for context-management.

    .. deprecated:: 1.0.0

        This method has been deprecated since version 1.0.0 and is only kept for
        backwards compatibility. New code should use :class:`~requests.sessions.Session`
        to create a session. This may be removed at a future date.

    :rtype: Session
    "
95,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,get_redirect_target,Receives a Response. Returns a redirect URI or ``None``
96,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,should_strip_auth,Decide whether Authorization header should be removed when redirecting
97,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,resolve_redirects,Receives a Response. Returns a generator of Responses or Requests.
98,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,rebuild_auth,"When being redirected we may want to strip authentication from the
        request to avoid leaking credentials. This method intelligently removes
        and reapplies authentication where possible to avoid credential loss.
        "
99,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,rebuild_proxies,"This method re-evaluates the proxy configuration by considering the
        environment variables. If we are redirected to a URL covered by
        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing
        proxy keys for this URL (in case they were stripped by a previous
        redirect).

        This method also replaces the Proxy-Authorization header where
        necessary.

        :rtype: dict
        "
100,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,rebuild_method,"When being redirected we may want to change the method of the request
        based on certain specs or browser behavior.
        "
101,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,prepare_request,"Constructs a :class:`PreparedRequest <PreparedRequest>` for
        transmission and returns it. The :class:`PreparedRequest` has settings
        merged from the :class:`Request <Request>` instance and those of the
        :class:`Session`.

        :param request: :class:`Request` instance to prepare with this
            session's settings.
        :rtype: requests.PreparedRequest
        "
102,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,request,"Constructs a :class:`Request <Request>`, prepares it and sends it.
        Returns :class:`Response <Response>` object.

        :param method: method for the new :class:`Request` object.
        :param url: URL for the new :class:`Request` object.
        :param params: (optional) Dictionary or bytes to be sent in the query
            string for the :class:`Request`.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param json: (optional) json to send in the body of the
            :class:`Request`.
        :param headers: (optional) Dictionary of HTTP Headers to send with the
            :class:`Request`.
        :param cookies: (optional) Dict or CookieJar object to send with the
            :class:`Request`.
        :param files: (optional) Dictionary of ``'filename': file-like-objects``
            for multipart encoding upload.
        :param auth: (optional) Auth tuple or callable to enable
            Basic/Digest/Custom HTTP Auth.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple
        :param allow_redirects: (optional) Set to True by default.
        :type allow_redirects: bool
        :param proxies: (optional) Dictionary mapping protocol or protocol and
            hostname to the URL of the proxy.
        :param stream: (optional) whether to immediately download the response
            content. Defaults to ``False``.
        :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
        :param cert: (optional) if String, path to ssl client cert file (.pem).
            If Tuple, ('cert', 'key') pair.
        :rtype: requests.Response
        "
103,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,get,"Sends a GET request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
104,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,options,"Sends a OPTIONS request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
105,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,head,"Sends a HEAD request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
106,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,post,"Sends a POST request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param json: (optional) json to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
107,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,put,"Sends a PUT request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
108,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,patch,"Sends a PATCH request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
109,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,delete,"Sends a DELETE request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        "
110,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,send,"Send a given PreparedRequest.

        :rtype: requests.Response
        "
111,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,merge_environment_settings,"
        Check the environment and merge it with some settings.

        :rtype: dict
        "
112,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,get_adapter,"
        Returns the appropriate connection adapter for the given URL.

        :rtype: requests.adapters.BaseAdapter
        "
113,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,close,Closes all adapters and as such the session
114,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\sessions.py,mount,"Registers a connection adapter to a prefix.

        Adapters are sorted in descending order by prefix length.
        "
115,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\structures.py,lower_items,"Like iteritems(), but with all lowercase keys."
116,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,dict_to_sequence,Returns an internal sequence dictionary update.
117,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,get_netrc_auth,Returns the Requests tuple auth for a given url from netrc.
118,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,guess_filename,Tries to guess the filename of the given object.
119,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,extract_zipped_paths,"Replace nonexistent paths that look like they refer to a member of a zip
    archive with the location of an extracted copy of the target, or else
    just return the provided path unchanged.
    "
120,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,from_key_val_list,"Take an object and test to see if it can be represented as a
    dictionary. Unless it can not be represented as such, return an
    OrderedDict, e.g.,

    ::

        >>> from_key_val_list([('key', 'val')])
        OrderedDict([('key', 'val')])
        >>> from_key_val_list('string')
        ValueError: cannot encode objects that are not 2-tuples
        >>> from_key_val_list({'key': 'val'})
        OrderedDict([('key', 'val')])

    :rtype: OrderedDict
    "
121,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,to_key_val_list,"Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> to_key_val_list([('key', 'val')])
        [('key', 'val')]
        >>> to_key_val_list({'key': 'val'})
        [('key', 'val')]
        >>> to_key_val_list('string')
        ValueError: cannot encode objects that are not 2-tuples.

    :rtype: list
    "
122,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,parse_list_header,"Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> parse_list_header('token, ""quoted value""')
    ['token', 'quoted value']

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    "
123,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,parse_dict_header,"Parse lists of key, value pairs as described by RFC 2068 Section 2 and
    convert them into a python dict:

    >>> d = parse_dict_header('foo=""is a fish"", bar=""as well""')
    >>> type(d) is dict
    True
    >>> sorted(d.items())
    [('bar', 'as well'), ('foo', 'is a fish')]

    If there is no value for a key it will be `None`:

    >>> parse_dict_header('key_without_value')
    {'key_without_value': None}

    To create a header from the :class:`dict` again, use the
    :func:`dump_header` function.

    :param value: a string with a dict header.
    :return: :class:`dict`
    :rtype: dict
    "
124,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,unquote_header_value,"Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    "
125,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,dict_from_cookiejar,"Returns a key/value dictionary from a CookieJar.

    :param cj: CookieJar object to extract cookies from.
    :rtype: dict
    "
126,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,add_dict_to_cookiejar,"Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    "
127,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,get_encodings_from_content,"Returns encodings from given content string.

    :param content: bytestring to extract encodings from.
    "
128,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,_parse_content_type_header,"Returns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    "
129,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,get_encoding_from_headers,"Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    "
130,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,stream_decode_response_unicode,Stream decodes a iterator.
131,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,iter_slices,Iterate over slices of a string.
132,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,get_unicode_from_response,"Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    "
133,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,unquote_unreserved,"Un-escape any percent-escape sequences in a URI that are unreserved
    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.

    :rtype: str
    "
134,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,requote_uri,"Re-quote the given URI.

    This function passes the given URI through an unquote/quote cycle to
    ensure that it is fully and consistently quoted.

    :rtype: str
    "
135,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,address_in_network,"This function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    "
136,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,dotted_netmask,"Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    "
137,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,is_ipv4_address,"
    :rtype: bool
    "
138,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,is_valid_cidr,"
    Very simple check of the cidr format in no_proxy variable.

    :rtype: bool
    "
139,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,set_environ,"Set the environment variable 'env_name' to 'value'

    Save previous value, yield, and then restore the previous value stored in
    the environment variable 'env_name'.

    If 'value' is None, do nothing"
140,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,should_bypass_proxies,"
    Returns whether we should bypass proxies or not.

    :rtype: bool
    "
141,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,get_environ_proxies,"
    Return a dict of environment proxies.

    :rtype: dict
    "
142,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,select_proxy,"Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    "
143,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,default_user_agent,"
    Return a string representing the default user agent.

    :rtype: str
    "
144,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,default_headers,"
    :rtype: requests.structures.CaseInsensitiveDict
    "
145,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,parse_header_links,"Return a list of parsed link headers proxies.

    i.e. Link: <http:/.../front.jpeg>; rel=front; type=""image/jpeg"",<http://.../back.jpeg>; rel=back;type=""image/jpeg""

    :rtype: list
    "
146,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,guess_json_utf,"
    :rtype: str
    "
147,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,prepend_scheme_if_needed,"Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    "
148,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,get_auth_from_url,"Given a url with authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str)
    "
149,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,check_header_validity,"Verifies that header value is a string which doesn't contain
    leading whitespace or return characters. This prevents unintended
    header injection.

    :param header: tuple, in the format (name, value).
    "
150,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,urldefragauth,"
    Given a url remove the fragment and the authentication part.

    :rtype: str
    "
151,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\utils.py,rewind_body,"Move file pointer back to its recorded starting position
    so it can be read again on redirect.
    "
152,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\_internal_utils.py,to_native_string,"Given a string object, regardless of type, returns a representation of
    that string in the native string type, encoding and decoding where
    necessary. This assumes ASCII unless told otherwise.
    "
153,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\requests\_internal_utils.py,unicode_is_ascii,"Determine if unicode string only contains ASCII characters.

    :param str u_string: unicode string to check. Must be unicode
        and not Python 2 `str`.
    :rtype: bool
    "
154,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_help.py,test_system_ssl,Verify we're actually setting system_ssl when it should be available.
155,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_help.py,test_idna_without_version_attribute,"Older versions of IDNA don't provide a __version__ attribute, verify
    that if we have such a package, we don't blow up.
    "
156,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_help.py,test_idna_with_version_attribute,Verify we're actually setting idna version when it should be available.
157,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_lowlevel.py,test_chunked_upload,can safely send generators
158,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_lowlevel.py,test_digestauth_401_count_reset_on_redirect,"Ensure we correctly reset num_401_calls after a successful digest auth,
    followed by a 302 redirect to another digest auth prompt.

    See https://github.com/requests/requests/issues/1979.
    "
159,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_lowlevel.py,test_digestauth_401_only_sent_once,"Ensure we correctly respond to a 401 challenge once, and then
    stop responding if challenged again.
    "
160,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_lowlevel.py,test_digestauth_only_on_4xx,"Ensure we only send digestauth on 4xx challenges.

    See https://github.com/requests/requests/issues/3772.
    "
161,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_lowlevel.py,test_fragment_not_sent_with_request,Verify that the fragment portion of a URI isn't sent to the server.
162,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_lowlevel.py,test_fragment_update_on_redirect,"Verify we only append previous fragment if one doesn't exist on new
    location. If a new fragment is encountered in a Location header, it should
    be added to all subsequent requests.
    "
163,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_data_argument_accepts_tuples,"Ensure that the data argument will accept tuples of strings
    and properly encode them.
    "
164,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_cookielib_cookiejar_on_redirect,"Tests resolve_redirect doesn't fail when merging cookies
        with non-RequestsCookieJar cookiejar.

        See GH #3579
        "
165,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_history_is_always_a_list,"Show that even with redirects, Response.history is always a list."
166,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_headers_on_session_with_None_are_not_sent,Do not send headers in Session.headers with None values.
167,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_headers_preserve_order,Preserve order when headers provided as OrderedDict.
168,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_basicauth_encodes_byte_strings,"Ensure b'test' formats as the byte string ""test"" rather
        than the unicode string ""b'test'"" in Python 3.
        "
169,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_https_warnings,warnings are emitted with requests.get
170,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_certificate_failure,"
        When underlying SSL problems occur, an SSLError is raised.
        "
171,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_response_decode_unicode,"When called with decode_unicode, Response.iter_content should always
        return unicode.
        "
172,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_response_chunk_size_type,"Ensure that chunk_size is passed as None or an integer, otherwise
        raise a TypeError.
        "
173,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_fixes_1329,Ensure that header updates are done case-insensitively.
174,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_header_validation,Ensure prepare_headers regex isn't flagging valid header contents.
175,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_header_value_not_str,"Ensure the header value is of type string or bytes as
        per discussion in GH issue #3386
        "
176,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_header_no_return_chars,"Ensure that a header containing return character sequences raise an
        exception. Otherwise, multiple headers are created from single string.
        "
177,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_header_no_leading_space,"Ensure headers containing leading whitespace raise
        InvalidHeader Error before sending.
        "
178,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_response_iter_lines_reentrant,Response.iter_lines() is not reentrant safe
179,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_response_without_release_conn,"Test `close` call for non-urllib3-like raw objects.
        Should work when `release_conn` attr doesn't exist on `response.raw`.
        "
180,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_empty_stream_with_auth_does_not_set_content_length_header,"Ensure that a byte stream with size 0 will not set both a Content-Length
        and Transfer-Encoding header.
        "
181,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_stream_with_auth_does_not_set_transfer_encoding_header,"Ensure that a byte stream with size > 0 will not set both a Content-Length
        and Transfer-Encoding header.
        "
182,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_chunked_upload_does_not_set_content_length_header,"Ensure that requests with a generator body stream using
        Transfer-Encoding: chunked, not a Content-Length header.
        "
183,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_custom_redirect_mixin,"Tests a custom mixin to overwrite ``get_redirect_target``.

        Ensures a subclassed ``requests.Session`` can handle a certain type of
        malformed redirect responses.

        1. original request receives a proper response: 302 redirect
        2. following the redirect, a malformed response is given:
            status code = HTTP 200
            location = alternate url
        3. the custom session catches the edge case and follows the redirect
        "
184,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_fixes_649,__setitem__ should behave case-insensitively.
185,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_expires_valid_str,Test case where we convert expires from string time.
186,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_expires_invalid_int,Test case where an invalid type is passed for expires.
187,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_expires_none,Test case where expires is None.
188,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_max_age_valid_int,Test case where a valid max age in seconds is passed.
189,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_max_age_invalid_str,Test case where a invalid max age is passed.
190,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_none_timeout,"Check that you can set None as a valid timeout value.

        To actually test this behavior, we'd want to check that setting the
        timeout to None actually lets the request block past the system default
        timeout. However, this would make the test suite unbearably slow.
        Instead we verify that setting the timeout to None does not prevent the
        request from succeeding.
        "
191,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_encoded_methods,See: https://github.com/requests/requests/issues/2316
192,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_url_mutation,"
        This test validates that we correctly exclude some URLs from
        preparation, and that we handle others. Specifically, it tests that
        any URL whose scheme doesn't begin with ""http"" is left alone, and
        those whose scheme *does* begin with ""http"" are mutated.
        "
193,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_requests.py,test_parameters_for_nonstandard_schemes,"
        Setting parameters for nonstandard schemes is allowed if those schemes
        begin with ""http"", and is forbidden otherwise.
        "
194,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_structures.py,setup,"CaseInsensitiveDict instance with ""Accept"" header."
195,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_structures.py,setup,"LookupDict instance with ""bad_gateway"" attribute."
196,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_basic,messages are sent and received properly
197,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_server_closes,the server closes when leaving the context manager
198,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_text_response,the text_response_server sends the given text
199,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_basic_response,the basic response server returns an empty http response
200,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_basic_waiting_server,the server waits for the block_server event to be set before closing
201,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_multiple_requests,multiple requests can be served
202,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_request_recovery,can check the requests content
203,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_requests_after_timeout_are_not_received,the basic response handler times out when receiving requests
204,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_request_recovery_with_bigger_timeout,a biggest timeout can be specified
205,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_server_finishes_on_error,the server thread exits even if an exception exits the context manager
206,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_testserver.py,test_server_finishes_when_no_connections,the server thread exits even if there are no connections
207,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_requote_uri_with_unquoted_percents,See: https://github.com/requests/requests/issues/2356
208,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_select_proxies,Make sure we can select per-host proxies correctly.
209,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_should_bypass_proxies,"Tests for function should_bypass_proxies to check if proxy
    can be bypassed or not
    "
210,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_should_bypass_proxies_pass_only_hostname,"The proxy_bypass function should be called with a hostname or IP without
    a port number or auth credentials.
    "
211,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_add_dict_to_cookiejar,"Ensure add_dict_to_cookiejar works for
    non-RequestsCookieJar CookieJars
    "
212,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_should_bypass_proxies_no_proxy,"Tests for function should_bypass_proxies to check if proxy
    can be bypassed or not using the 'no_proxy' argument
    "
213,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_should_bypass_proxies_win_registry,"Tests for function should_bypass_proxies to check if proxy
    can be bypassed or not with Windows registry settings
    "
214,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_set_environ,Tests set_environ will set environ values and will restore the environ.
215,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_set_environ_raises_exception,"Tests set_environ will raise exceptions in context when the
    value parameter is None."
216,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_io_streams,Ensures that we properly deal with different kinds of IO streams.
217,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_super_len_correctly_calculates_len_of_partially_read_file,Ensure that we handle partially consumed file like objects.
218,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_super_len_handles_files_raising_weird_errors_in_tell,"If tell() raises errors, assume the cursor is at position zero."
219,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_super_len_tell_ioerror,Ensure that if tell gives an IOError super_len doesn't fail
220,requests,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\requests\tests\test_utils.py,test_super_len_with_no_matches,Ensure that objects without any length methods default to 0
index,project,file,func_name,func_doc
1,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_attr_methods.py,to_html,"Doc method extension for saving the current state as a displaCy
    visualization.
    "
2,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_attr_methods.py,overlap_tokens,"Get the tokens from the original Doc that are also in the comparison Doc.
    "
3,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_component_countries_api.py,__init__,"Initialise the pipeline component. The shared nlp instance is used
        to initialise the matcher with the shared vocab, get the label ID and
        generate Doc objects as phrase match patterns.
        "
4,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_component_countries_api.py,__call__,"Apply the pipeline component on a Doc object and modify it if matches
        are found. Return the Doc, so it can be processed by the next component
        in the pipeline, if available.
        "
5,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_component_countries_api.py,has_country,"Getter for Doc and Span attributes. Returns True if one of the tokens
        is a country. Since the getter is only called when we access the
        attribute, we can refer to the Token's 'is_country' attribute here,
        which is already set in the processing step."
6,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_component_entities.py,__init__,"Initialise the pipeline component. The shared nlp instance is used
        to initialise the matcher with the shared vocab, get the label ID and
        generate Doc objects as phrase match patterns.
        "
7,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_component_entities.py,__call__,"Apply the pipeline component on a Doc object and modify it if matches
        are found. Return the Doc, so it can be processed by the next component
        in the pipeline, if available.
        "
8,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\pipeline\custom_component_entities.py,has_tech_org,"Getter for Doc and Span attributes. Returns True if one of the tokens
        is a tech org. Since the getter is only called when we access the
        attribute, we can refer to the Token's 'is_tech_org' attribute here,
        which is already set in the processing step."
9,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\ner_multitask_objective.py,get_position_label,"Return labels indicating the position of the word in the document.
    "
10,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\train_intent_parser.py,main,"Load the model, set up the pipeline and train the parser."
11,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\train_ner.py,main,"Load the model, set up the pipeline and train the entity recognizer."
12,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\train_new_entity_type.py,main,"Set up the pipeline and entity recognizer, and train the new entity."
13,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\train_parser.py,main,"Load the model, set up the pipeline and train the parser."
14,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\train_tagger.py,main,"Create a new model, set up the pipeline and train the tagger. In order to
    train the tagger with a custom tag map, we're creating a new Language
    instance with a custom vocab.
    "
15,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\examples\training\train_textcat.py,load_data,Load data from the IMDB dataset.
16,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\convert.py,convert,"
    Convert files into JSON format for use with train command and other
    experiment management functions.
    "
17,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\conllu2json.py,conllu2json,"
    Convert conllu files into JSON format for use with train cli.
    use_morphology parameter enables appending morphology to tags, which is
    useful for languages such as Spanish, where UD tags are not so rich.
    "
18,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\conllu2json.py,conllu2json,"     
    Extract NER tags if available and convert them so that they follow
    BILUO and the Wikipedia scheme
    "
19,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\conllu2json.py,is_ner," 
    Check the 10th column of the first token to determine if the file contains
    NER tags 
    "
20,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\conllu2json.py,simplify_tags,"
    Simplify tags obtained from the dataset in order to follow Wikipedia
    scheme (PER, LOC, ORG, MISC). 'PER', 'LOC' and 'ORG' keep their tags, while
    'GPE_LOC' is simplified to 'LOC', 'GPE_ORG' to 'ORG' and all remaining tags to
    'MISC'.     
    "
21,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\conllubio2json.py,conllubio2json,"
    Convert conllu files into JSON format for use with train cli.
    use_morphology parameter enables appending morphology to tags, which is
    useful for languages such as Spanish, where UD tags are not so rich.
    "
22,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\conll_ner2json.py,conll_ner2json,"
    Convert files in the CoNLL-2003 NER format into JSON format for use with
    train cli.
    "
23,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\converters\iob2json.py,iob2json,"
    Convert IOB files into JSON format for use with train cli.
    "
24,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\download.py,download,"
    Download compatible model from default download path using pip. Model
    can be shortcut, model name or, if --direct flag is set, full model name
    with version.
    "
25,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\evaluate.py,evaluate,"
    Evaluate a model. To render a sample of parses in a HTML file, set an
    output directory as the displacy_path argument.
    "
26,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\info.py,info,"Print info about spaCy installation. If a model shortcut link is
    speficied as an argument, print model information. Flag --markdown
    prints details in Markdown for easy copy-pasting to GitHub issues.
    "
27,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\init_model.py,init_model,"
    Create a new model from raw data, like word frequencies, Brown clusters
    and word vectors.
    "
28,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\init_model.py,open_file,"Handle .gz, .tar.gz or unzipped files"
29,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\link.py,link,"
    Create a symlink for models within the spacy/data directory. Accepts
    either the name of a pip package, or the local path to the model data
    directory. Linking models allows loading them via spacy.load(link_name).
    "
30,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\package.py,package,"
    Generate Python package for model data, including meta and required
    installation files. A new directory will be created in the specified
    output directory, and model data will be copied over.
    "
31,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\profile.py,profile,"
    Profile a spaCy pipeline, to find out which functions take the most time.
    "
32,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\train.py,train,"
    Train a model. Expects data in spaCy's JSON format.
    "
33,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\validate.py,validate,"Validate that the currently installed version of spaCy is compatible
    with the installed models. Should be run after `pip install -U spacy`.
    "
34,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\validate.py,reformat_version,Hack to reformat old versions ending on '-alpha' to match pip format.
35,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\cli\vocab.py,make_vocab,Compile a vocabulary from a lexicon jsonl file and word vectors.
36,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\compat.py,normalize_string_keys,"Given a dictionary, make sure keys are unicode strings, not bytes."
37,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\compat.py,locale_escape,"
    Mangle non-supported characters, for savages with ascii terminals.
    "
38,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,__init__,"Initialise dependency renderer.

        options (dict): Visualiser-specific options (compact, word_spacing,
            arrow_spacing, arrow_width, arrow_stroke, distance, offset_x,
            color, bg, font)
        "
39,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,render,"Render complete markup.

        parsed (list): Dependency parses to render.
        page (bool): Render parses wrapped as full HTML page.
        minify (bool): Minify HTML markup.
        RETURNS (unicode): Rendered SVG or HTML markup.
        "
40,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,render_svg,"Render SVG.

        render_id (int): Unique ID, typically index of document.
        words (list): Individual words and their tags.
        arcs (list): Individual arcs and their start, end, direction and label.
        RETURNS (unicode): Rendered SVG markup.
        "
41,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,render_word,"Render individual word.

        text (unicode): Word text.
        tag (unicode): Part-of-speech tag.
        i (int): Unique ID, typically word index.
        RETURNS (unicode): Rendered SVG markup.
        "
42,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,render_arrow,"Render indivicual arrow.

        label (unicode): Dependency label.
        start (int): Index of start word.
        end (int): Index of end word.
        direction (unicode): Arrow direction, 'left' or 'right'.
        i (int): Unique ID, typically arrow index.
        RETURNS (unicode): Rendered SVG markup.
        "
43,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,get_arc,"Render individual arc.

        x_start (int): X-coordinate of arrow start point.
        y (int): Y-coordinate of arrow start and end point.
        y_curve (int): Y-corrdinate of Cubic Bézier y_curve point.
        x_end (int): X-coordinate of arrow end point.
        RETURNS (unicode): Definition of the arc path ('d' attribute).
        "
44,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,get_arrowhead,"Render individual arrow head.

        direction (unicode): Arrow direction, 'left' or 'right'.
        x (int): X-coordinate of arrow start point.
        y (int): Y-coordinate of arrow start and end point.
        end (int): X-coordinate of arrow end point.
        RETURNS (unicode): Definition of the arrow head path ('d' attribute).
        "
45,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,get_levels,"Calculate available arc height ""levels"".
        Used to calculate arrow heights dynamically and without wasting space.

        args (list): Individual arcs and their start, end, direction and label.
        RETURNS (list): Arc levels sorted from lowest to highest.
        "
46,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,__init__,"Initialise dependency renderer.

        options (dict): Visualiser-specific options (colors, ents)
        "
47,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,render,"Render complete markup.

        parsed (list): Dependency parses to render.
        page (bool): Render parses wrapped as full HTML page.
        minify (bool): Minify HTML markup.
        RETURNS (unicode): Rendered HTML markup.
        "
48,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\render.py,render_ents,"Render entities in text.

        text (unicode): Original text.
        spans (list): Individual entity spans and their start, end and label.
        title (unicode or None): Document title set in Doc.user_data['title'].
        "
49,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\__init__.py,render,"Render displaCy visualisation.

    docs (list or Doc): Document(s) to visualise.
    style (unicode): Visualisation style, 'dep' or 'ent'.
    page (bool): Render markup as full HTML page.
    minify (bool): Minify HTML markup.
    jupyter (bool): Experimental, use Jupyter's `display()` to output markup.
    options (dict): Visualiser-specific options, e.g. colors.
    manual (bool): Don't parse `Doc` and instead expect a dict/list of dicts.
    RETURNS (unicode): Rendered HTML markup.
    "
50,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\__init__.py,serve,"Serve displaCy visualisation.

    docs (list or Doc): Document(s) to visualise.
    style (unicode): Visualisation style, 'dep' or 'ent'.
    page (bool): Render markup as full HTML page.
    minify (bool): Minify HTML markup.
    options (dict): Visualiser-specific options, e.g. colors.
    manual (bool): Don't parse `Doc` and instead expect a dict/list of dicts.
    port (int): Port to serve visualisation.
    "
51,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\__init__.py,parse_deps,"Generate dependency parse in {'words': [], 'arcs': []} format.

    doc (Doc): Document do parse.
    RETURNS (dict): Generated dependency parse keyed by words and arcs.
    "
52,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\displacy\__init__.py,parse_ents,"Generate named entities in [{start: i, end: i, label: 'label'}] format.

    doc (Doc): Document do parse.
    RETURNS (dict): Generated entities keyed by text (original text) and ents.
    "
53,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\errors.py,add_codes,Add error codes to string messages via class attribute names.
54,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\errors.py,_warn,"
    message (unicode): The message to display.
    category (Warning): The Warning to show.
    "
55,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\glossary.py,explain,"Get a description for a given POS tag, dependency label or entity type.

    term (unicode): The term to explain.
    RETURNS (unicode): The explanation, or `None` if not found in the glossary.

    EXAMPLE:
        >>> spacy.explain(u'NORP')
        >>> doc = nlp(u'Hello world')
        >>> print([w.text, w.tag_, spacy.explain(w.tag_) for w in doc])
    "
56,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\ar\lex_attrs.py,like_num,"
    check if text resembles a number
    "
57,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\de\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
    "
58,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\el\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases. Works on both Doc and Span.
    "
59,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\en\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
    "
60,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\fa\lex_attrs.py,like_num,"
    check if text resembles a number
    "
61,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\fa\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
    "
62,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\fr\lemmatizer\lemmatizer.py,is_base_form,"
        Check whether we're dealing with an uninflected paradigm, so we can
        avoid lemmatization entirely.
        "
63,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\fr\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
    "
64,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\id\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
    "
65,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\ja\__init__.py,try_mecab_import,"Mecab is required for Japanese support, so check for it.

    It it's not available blow up and explain how to fix it."
66,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\ja\__init__.py,resolve_pos,"If necessary, add a field to the POS tag for UD mapping.

    Under Universal Dependencies, sometimes the same Unidic POS tag can
    be mapped differently depending on the literal token or its context
    in the sentence. This function adds information to the POS tag to
    resolve ambiguous mappings.
    "
67,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\ja\__init__.py,detailed_tokens,"Format Mecab output into a nice data structure, based on Janome."
68,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lang\nb\syntax_iterators.py,noun_chunks,"
    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
    "
69,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,__init__,"Initialise a Language object.

        vocab (Vocab): A `Vocab` object. If `True`, a vocab is created via
            `Language.Defaults.create_vocab`.
        make_doc (callable): A function that takes text and returns a `Doc`
            object. Usually a `Tokenizer`.
        meta (dict): Custom meta data for the Language class. Is written to by
            models to add model meta data.
        max_length (int) :
            Maximum number of characters in a single text. The current v2 models
            may run out memory on extremely long texts, due to large internal
            allocations. You should segment these texts into meaningful units,
            e.g. paragraphs, subsections etc, before passing them to spaCy.
            Default maximum length is 1,000,000 characters (1mb). As a rule of
            thumb, if all pipeline components are enabled, spaCy's default
            models currently requires roughly 1GB of temporary memory per
            100,000 characters in one text.
        RETURNS (Language): The newly constructed object.
        "
70,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,pipe_names,"Get names of available pipeline components.

        RETURNS (list): List of component name strings, in order.
        "
71,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,get_pipe,"Get a pipeline component for a given component name.

        name (unicode): Name of pipeline component to get.
        RETURNS (callable): The pipeline component.
        "
72,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,create_pipe,"Create a pipeline component from a factory.

        name (unicode): Factory name to look up in `Language.factories`.
        config (dict): Configuration parameters to initialise component.
        RETURNS (callable): Pipeline component.
        "
73,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,add_pipe,"Add a component to the processing pipeline. Valid components are
        callables that take a `Doc` object, modify it and return it. Only one
        of before/after/first/last can be set. Default behaviour is ""last"".

        component (callable): The pipeline component.
        name (unicode): Name of pipeline component. Overwrites existing
            component.name attribute if available. If no name is set and
            the component exposes no name attribute, component.__name__ is
            used. An error is raised if a name already exists in the pipeline.
        before (unicode): Component name to insert component directly before.
        after (unicode): Component name to insert component directly after.
        first (bool): Insert component first / not first in the pipeline.
        last (bool): Insert component last / not last in the pipeline.

        EXAMPLE:
            >>> nlp.add_pipe(component, before='ner')
            >>> nlp.add_pipe(component, name='custom_name', last=True)
        "
74,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,has_pipe,"Check if a component name is present in the pipeline. Equivalent to
        `name in nlp.pipe_names`.

        name (unicode): Name of the component.
        RETURNS (bool): Whether a component of the name exists in the pipeline.
        "
75,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,replace_pipe,"Replace a component in the pipeline.

        name (unicode): Name of the component to replace.
        component (callable): Pipeline component.
        "
76,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,rename_pipe,"Rename a pipeline component.

        old_name (unicode): Name of the component to rename.
        new_name (unicode): New name of the component.
        "
77,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,remove_pipe,"Remove a component from the pipeline.

        name (unicode): Name of the component to remove.
        RETURNS (tuple): A `(name, component)` tuple of the removed component.
        "
78,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,__call__,"Apply the pipeline to some text. The text can span multiple sentences,
        and can contain arbtrary whitespace. Alignment into the original string
        is preserved.

        text (unicode): The text to be processed.
        disable (list): Names of the pipeline components to disable.
        RETURNS (Doc): A container for accessing the annotations.

        EXAMPLE:
            >>> tokens = nlp('An example sentence. Another example sentence.')
            >>> tokens[0].text, tokens[0].head.tag_
            ('An', 'NN')
        "
79,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,disable_pipes,"Disable one or more pipeline components. If used as a context
        manager, the pipeline will be restored to the initial state at the end
        of the block. Otherwise, a DisabledPipes object is returned, that has
        a `.restore()` method you can use to undo your changes.

        EXAMPLE:
            >>> nlp.add_pipe('parser')
            >>> nlp.add_pipe('tagger')
            >>> with nlp.disable_pipes('parser', 'tagger'):
            >>>     assert not nlp.has_pipe('parser')
            >>> assert nlp.has_pipe('parser')
            >>> disabled = nlp.disable_pipes('parser')
            >>> assert len(disabled) == 1
            >>> assert not nlp.has_pipe('parser')
            >>> disabled.restore()
            >>> assert nlp.has_pipe('parser')
        "
80,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,update,"Update the models in the pipeline.

        docs (iterable): A batch of `Doc` objects.
        golds (iterable): A batch of `GoldParse` objects.
        drop (float): The droput rate.
        sgd (callable): An optimizer.
        RETURNS (dict): Results from the update.

        EXAMPLE:
            >>> with nlp.begin_training(gold) as (trainer, optimizer):
            >>>    for epoch in trainer.epochs(gold):
            >>>        for docs, golds in epoch:
            >>>            state = nlp.update(docs, golds, sgd=optimizer)
        "
81,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,preprocess_gold,"Can be called before training to pre-process gold data. By default,
        it handles nonprojectivity and adds missing tags to the tag map.

        docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.
        YIELDS (tuple): Tuples of preprocessed `Doc` and `GoldParse` objects.
        "
82,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,begin_training,"Allocate models, pre-process training data and acquire a trainer and
        optimizer. Used as a contextmanager.

        get_gold_tuples (function): Function returning gold data
        **cfg: Config parameters.
        RETURNS: An optimizer
        "
83,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,use_params,"Replace weights of models in the pipeline with those provided in the
        params dictionary. Can be used as a contextmanager, in which case,
        models go back to their original weights after the block.

        params (dict): A dictionary of parameters keyed by model ID.
        **cfg: Config parameters.

        EXAMPLE:
            >>> with nlp.use_params(optimizer.averages):
            >>>     nlp.to_disk('/tmp/checkpoint')
        "
84,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,pipe,"Process texts as a stream, and yield `Doc` objects in order.

        texts (iterator): A sequence of texts to process.
        as_tuples (bool):
            If set to True, inputs should be a sequence of
            (text, context) tuples. Output will then be a sequence of
            (doc, context) tuples. Defaults to False.
        n_threads (int): Currently inactive.
        batch_size (int): The number of texts to buffer.
        disable (list): Names of the pipeline components to disable.
        cleanup (bool): If True, unneeded strings are freed,
            to control memory use. Experimental.
        YIELDS (Doc): Documents in the order of the original text.

        EXAMPLE:
            >>> texts = [u'One document.', u'...', u'Lots of documents']
            >>>     for doc in nlp.pipe(texts, batch_size=50, n_threads=4):
            >>>         assert doc.is_parsed
        "
85,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,to_disk,"Save the current state to a directory.  If a model is loaded, this
        will include the model.

        path (unicode or Path): A path to a directory, which will be created if
            it doesn't exist. Paths may be strings or `Path`-like objects.
        disable (list): Names of pipeline components to disable and prevent
            from being saved.

        EXAMPLE:
            >>> nlp.to_disk('/path/to/models')
        "
86,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,from_disk,"Loads state from a directory. Modifies the object in place and
        returns it. If the saved `Language` object contains a model, the
        model will be loaded.

        path (unicode or Path): A path to a directory. Paths may be either
            strings or `Path`-like objects.
        disable (list): Names of the pipeline components to disable.
        RETURNS (Language): The modified `Language` object.

        EXAMPLE:
            >>> from spacy.language import Language
            >>> nlp = Language().from_disk('/path/to/models')
        "
87,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,to_bytes,"Serialize the current state to a binary string.

        disable (list): Nameds of pipeline components to disable and prevent
            from being serialized.
        RETURNS (bytes): The serialized form of the `Language` object.
        "
88,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,from_bytes,"Load state from a binary string.

        bytes_data (bytes): The data to load from.
        disable (list): Names of the pipeline components to disable.
        RETURNS (Language): The `Language` object.
        "
89,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\language.py,restore,Restore the pipeline to its state when DisabledPipes was created.
90,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\lemmatizer.py,is_base_form,"
        Check whether we're dealing with an uninflected paradigm, so we can
        avoid lemmatization entirely.
        "
91,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_doc_api.py,test_doc_api_merge_children,Test that attachments work correctly after merging.
92,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_doc_api.py,test_doc_api_right_edge,"Test for bug occurring from Unshift action, causing incorrect right edge"
93,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_doc_api.py,test_parse_tree,Tests doc.print_tree() method.
94,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_span.py,test_spans_span_sent,Test span.sent property
95,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_span.py,test_spans_lca_matrix,Test span's lca matrix generation
96,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_span.py,test_spans_default_sentiment,Test span.sentiment property's default averaging behaviour
97,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_span.py,test_spans_override_sentiment,Test span.sentiment property's default averaging behaviour
98,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_span.py,test_spans_are_hashable,Test spans can be hashed.
99,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_span.py,test_span_ents_property,Test span.ents for the 
100,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\doc\test_token_api.py,test_tokens_sent,Test token.sent property
101,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\de\test_models.py,example,"
    This is to make sure the model works as expected. The tests make sure that
    values are properly set. Tests are not meant to evaluate the content of the
    output, only make sure the output is formally okay.
    "
102,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_models.py,example,"
    This is to make sure the model works as expected. The tests make sure that
    values are properly set. Tests are not meant to evaluate the content of the
    output, only make sure the output is formally okay.
    "
103,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_ner.py,test_en_ner_consistency_bug,Test an arbitrary sequence-consistency bug encountered during speed test
104,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_ner.py,test_en_ner_unit_end_gazetteer,Test a bug in the interaction between the NER model and the gazetteer
105,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_sbd.py,test_en_sbd_serialization_projective,"Test that before and after serialization, the sentence boundaries are
    the same."
106,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_sbd.py,test_en_sbd_prag,SBD tests from Pragmatic Segmenter
107,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_tagger.py,test_en_tagger_spaces,Ensure spaces are assigned the POS tag SPACE
108,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\lang\en\test_tagger.py,test_en_tagger_return_char,Ensure spaces are assigned the POS tag SPACE
109,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue118.py,test_issue118,Test a bug that arose from having overlapping matches
110,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue118.py,test_issue118_prefix_reorder,Test a bug that arose from having overlapping matches
111,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1305.py,test_issue1305,Test lemmatization of English VBZ
112,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1375.py,test_issue1375,Test that token.nbor() raises IndexError for out-of-bounds access.
113,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1434.py,test_issue1434,Test matches occur when optional element at end of short doc
114,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1450.py,test_issue1450_matcher_end_zero_plus,"Test matcher works when patterns end with * operator.
    
    Original example (rewritten to avoid model usage)

    nlp = spacy.load('en_core_web_sm')
    matcher = Matcher(nlp.vocab)
    matcher.add(
        ""TSTEND"",
        on_match_1,
        [
            {TAG: ""JJ"", LOWER: ""new""},
            {TAG: ""NN"", 'OP': ""*""}
        ]
    )
    doc = nlp(u'Could you create a new ticket for me?')
    print([(w.tag_, w.text, w.lower_) for w in doc])
    matches = matcher(doc)
    print(matches)
    assert len(matches) == 1
    assert matches[0][1] == 4
    assert matches[0][2] == 5
    "
115,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1518.py,test_issue1518,Test vectors.resize() works.
116,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1547.py,test_issue1547,Test that entity labels still match after merging tokens.
117,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1758.py,test_issue1758,"Test that ""would've"" is handled by the English tokenizer exceptions."
118,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1834.py,test_issue1834,"test if sentence boundaries & parse/tag flags are not lost
    during serialization
    "
119,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue1959.py,clean_component, Clean up text. Make lowercase and remove punctuation and stopwords 
120,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue2385.py,test_issue2385_biluo,already biluo format
121,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue2385.py,test_issue2385_iob_bcharacter,fix bug in labels with a 'b' character
122,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue2385.py,test_issue2385_iob1,maintain support for iob1 format
123,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue2385.py,test_issue2385_iob2,maintain support for iob2 format
124,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue242.py,test_issue242,Test overlapping multi-word phrases.
125,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue2626.py,test_issue2626,Check that this sentence doesn't cause an infinite loop in the tokenizer.
126,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue2901.py,test_issue2901,Test that `nlp` doesn't fail.
127,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue309.py,test_issue309,Test Issue #309: SBD fails on empty string
128,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue360.py,test_issue360,Test tokenization of big ellipsis
129,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue361.py,test_issue361,Test Issue #361: Equality of lexemes
130,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue401.py,test_issue401,Text that 's in contractions is not lemmatized as '.
131,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue514.py,test_issue514,Test serializing after adding entity
132,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue587.py,test_issue587,Test that Matcher doesn't segfault on particular input
133,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue590.py,test_issue590,Test overlapping matches
134,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue595.py,test_issue595,Test lemmatization of base forms
135,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue686.py,test_issue686,Test that pronoun lemmas are assigned correctly.
136,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue693.py,test_issue693,Test that doc.noun_chunks parses the complete sentence.
137,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue704.py,test_issue704,Test that sentence boundaries are detected correctly.
138,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue717.py,test_issue717,Test that contractions are assigned the correct lemma.
139,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue719.py,test_issue719,Test that the token 's' is not lemmatized into empty string.
140,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue736.py,test_issue736,"Test that times like ""7am"" are tokenized correctly and that numbers are converted to string."
141,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue740.py,test_issue740,"Test that dates are not split and kept as one token. This behaviour is currently inconsistent, since dates separated by hyphens are still split.
    This will be hard to prevent without causing clashes with numeric ranges."
142,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue744.py,test_issue744,"Test that 'were' and 'Were' are excluded from the contractions
    generated by the English tokenizer exceptions."
143,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue758.py,test_issue758,Test parser transition bug after label added.
144,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue759.py,test_issue759,Test that numbers are recognised correctly.
145,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue768.py,test_issue768,Allow zero-width 'infix' token during the tokenization process.
146,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue775.py,test_issue775,"Test that 'Shell' and 'shell' are excluded from the contractions
    generated by the English tokenizer exceptions."
147,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue792.py,test_issue792,Test for Issue #792: Trailing whitespace is removed after tokenization.
148,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue792.py,test_control_issue792,Test base case for Issue #792: Non-trailing whitespace
149,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue801.py,test_issue801,Test that special characters + hyphens are split correctly.
150,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue834.py,test_issue834,Test that no-break space (U+00A0) is detected as space by the load_vectors function.
151,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue850.py,test_basic_case,Test Matcher matches with '*' operator and Boolean flag
152,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue850.py,test_issue850,"The problem here is that the variable-length pattern matches the
    succeeding token. We then don't handle the ambiguity correctly."
153,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue852.py,test_issue852,Test that French tokenizer exceptions are imported correctly.
154,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue859.py,test_issue859,Test that no extra space is added in doc.text method.
155,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue886.py,test_issue886,Test that token.idx matches the original text index for texts with newlines.
156,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue891.py,test_issue891,Test that / infixes are split correctly.
157,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue903.py,test_issue912,Test base-forms of adjectives are preserved.
158,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue910.py,test_issue910,"Test that adding entities and resuming training works passably OK.
    There are two issues here:

    1) We have to readd labels. This isn't very nice.
    2) There's no way to set the learning rate for the weight update, so we
        end up out-of-scale, causing it to learn too fast.
    "
159,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue912.py,test_issue912,Test base-forms of adjectives are preserved.
160,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue957.py,test_issue957,Test that spaCy doesn't hang on many periods.
161,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue995.py,test_issue955,Test that we don't have any nested noun chunks
162,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\regression\test_issue999.py,test_issue999,"Test that adding entities and resuming training works passably OK.
    There are two issues here:

    1) We have to readd labels. This isn't very nice.
    2) There's no way to set the learning rate for the weight update, so we
        end up out-of-scale, causing it to learn too fast.
    "
163,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\stringstore\test_stringstore.py,test_string_hash,Test that string hashing is stable across platforms
164,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_matcher.py,test_matcher_empty_dict,"Test matcher allows empty token specs, meaning match on any token."
165,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_matcher.py,test_matcher_end_zero_plus,Test matcher works when patterns end with * operator. (issue 1450)
166,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_misc.py,test_util_is_package,Test that an installed package via pip is recognised by util.is_package.
167,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_misc.py,test_util_get_package_path,Test that a Path object is returned for a package name.
168,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_misc.py,test_displacy_parse_ents,Test that named entities on a Doc are converted into displaCy's format.
169,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_misc.py,test_displacy_parse_deps,Test that deps and tags on a Doc are converted into displaCy's format.
170,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\test_misc.py,test_displacy_spans,Test that displaCy can render Spans.
171,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,load_test_model,"Load a model if it's installed as a package, otherwise skip."
172,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,get_doc,"Create Doc object from given vocab, words and annotations."
173,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,apply_transition_sequence,"Perform a series of pre-specified transitions, to put the parser in a
    desired state."
174,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,add_vecs_to_vocab,"Add list of vector tuples to given vocab. All vectors need to have the
    same length. Format: [(""text"", [1, 2, 3])]"
175,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,get_cosine,Get cosine for two given vectors
176,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,assert_docs_equal,"Compare two Doc objects and assert that they're equal. Tests for tokens,
    tags, dependencies and entities."
177,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\util.py,assert_packed_msg_equal,Assert that two packed msgpack messages are equal.
178,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\vocab\test_lexeme.py,test_vocab_lexeme_lt,More frequent is l.t. less frequent
179,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tests\vocab\test_lexeme.py,test_vocab_lexeme_hash,Test that lexemes are hashable.
180,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tokens\printers.py,merge_ents,Helper: merge adjacent entities into single tokens; modifies the doc.
181,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tokens\printers.py,format_POS,Helper: form the POS output for a token.
182,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tokens\printers.py,POS_tree,"Helper: generate a POS tree for a root token. The doc must have
    `merge_ents(doc)` ran on it.
    "
183,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tokens\printers.py,parse_tree,"Make a copy of the doc and construct a syntactic parse tree similar to
    displaCy. Generates the POS tree for all sentences in a doc.

    doc (Doc): The doc for parsing.
    RETURNS (dict): The parse tree.

    EXAMPLE:
        >>> doc = nlp('Bob brought Alice the pizza. Alice ate the pizza.')
        >>> trees = doc.print_tree()
        >>> trees[1]
        {'modifiers': [
            {'modifiers': [], 'NE': 'PERSON', 'word': 'Alice', 'arc': 'nsubj',
             'POS_coarse': 'PROPN', 'POS_fine': 'NNP', 'lemma': 'Alice'},
            {'modifiers': [
                {'modifiers': [], 'NE': '', 'word': 'the', 'arc': 'det',
                 'POS_coarse': 'DET', 'POS_fine': 'DT', 'lemma': 'the'}],
             'NE': '', 'word': 'pizza', 'arc': 'dobj', 'POS_coarse': 'NOUN',
             'POS_fine': 'NN', 'lemma': 'pizza'},
            {'modifiers': [], 'NE': '', 'word': '.', 'arc': 'punct',
             'POS_coarse': 'PUNCT', 'POS_fine': '.', 'lemma': '.'}],
            'NE': '', 'word': 'ate', 'arc': 'ROOT', 'POS_coarse': 'VERB',
            'POS_fine': 'VBD', 'lemma': 'eat'}
    "
184,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\tokens\underscore.py,get_ext_args,"Validate and convert arguments. Reused in Doc, Token and Span."
185,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,get_lang_class,"Import and load a Language class.

    lang (unicode): Two-letter language code, e.g. 'en'.
    RETURNS (Language): Language class.
    "
186,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,set_lang_class,"Set a custom Language class name that can be loaded via get_lang_class.

    name (unicode): Name of Language class.
    cls (Language): Language class.
    "
187,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,get_data_path,"Get path to spaCy data directory.

    require_exists (bool): Only return path if it exists, otherwise None.
    RETURNS (Path or None): Data path or None.
    "
188,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,set_data_path,"Set path to spaCy data directory.

    path (unicode or Path): Path to new data directory.
    "
189,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,ensure_path,"Ensure string is converted to a Path.

    path: Anything. If string, it's converted to Path.
    RETURNS: Path or original argument.
    "
190,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,load_model,"Load a model from a shortcut link, package or data path.

    name (unicode): Package name, shortcut link or model path.
    **overrides: Specific overrides, like pipeline components to disable.
    RETURNS (Language): `Language` class with the loaded model.
    "
191,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,load_model_from_link,"Load a model from a shortcut link, or directory in spaCy data path."
192,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,load_model_from_package,Load a model from an installed package.
193,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,load_model_from_path,"Load a model from a data directory path. Creates Language class with
    pipeline from meta.json and then calls from_disk() with path."
194,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,load_model_from_init_py,"Helper function to use in the `load()` method of a model package's
    __init__.py.

    init_file (unicode): Path to model's __init__.py, i.e. `__file__`.
    **overrides: Specific overrides, like pipeline components to disable.
    RETURNS (Language): `Language` class with loaded model.
    "
195,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,get_model_meta,"Get model meta.json from a directory path and validate its contents.

    path (unicode or Path): Path to model directory.
    RETURNS (dict): The model's meta data.
    "
196,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,is_package,"Check if string maps to a package installed via pip.

    name (unicode): Name of package.
    RETURNS (bool): True if installed package, False if not.
    "
197,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,get_package_path,"Get the path to an installed package.

    name (unicode): Package name.
    RETURNS (Path): Path to installed package.
    "
198,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,is_in_jupyter,"Check if user is running spaCy from a Jupyter notebook by detecting the
    IPython kernel. Mainly used for the displaCy visualizer.

    RETURNS (bool): True if in Jupyter, False if not.
    "
199,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,add_lookups,"Extend an attribute function with special cases. If a word is in the
    lookups, the value is returned. Otherwise the previous function is used.

    default_func (callable): The default function to execute.
    *lookups (dict): Lookup dictionary mapping string to attribute value.
    RETURNS (callable): Lexical attribute getter.
    "
200,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,update_exc,"Update and validate tokenizer exceptions. Will overwrite exceptions.

    base_exceptions (dict): Base exceptions.
    *addition_dicts (dict): Exceptions to add to the base dict, in order.
    RETURNS (dict): Combined tokenizer exceptions.
    "
201,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,expand_exc,"Find string in tokenizer exceptions, duplicate entry and replace string.
    For example, to add additional versions with typographic apostrophes.

    excs (dict): Tokenizer exceptions.
    search (unicode): String to find and replace.
    replace (unicode): Replacement.
    RETURNS (dict): Combined tokenizer exceptions.
    "
202,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,minibatch,"Iterate over batches of items. `size` may be an iterator,
    so that batch-size can vary on each step.
    "
203,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,compounding,"Yield an infinite series of compounding values. Each time the
    generator is called, a value is produced by multiplying the previous
    value by the compound rate.

    EXAMPLE:
      >>> sizes = compounding(1., 10., 1.5)
      >>> assert next(sizes) == 1.
      >>> assert next(sizes) == 1 * 1.5
      >>> assert next(sizes) == 1.5 * 1.5
    "
204,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,decaying,Yield an infinite series of linearly decaying values.
205,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,itershuffle,"Shuffle an iterator. This works by holding `bufsize` items back
    and yielding them sometime later. Obviously, this is not unbiased C
    but should be good enough for batching. Larger bufsize means less bias.
    From https://gist.github.com/andres-erbsen/1307752

    iterable (iterable): Iterator to shuffle.
    bufsize (int): Items to hold back.
    YIELDS (iterable): The shuffled iterator.
    "
206,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,read_json,"Open and load JSON from file.

    location (Path): Path to JSON file.
    RETURNS (dict): Loaded JSON content.
    "
207,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,get_raw_input,"Get user input from the command line via raw_input / input.

    description (unicode): Text to display before prompt.
    default (unicode or False/None): Default value to display with prompt.
    RETURNS (unicode): User input.
    "
208,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,print_table,"Print data in table format.

    data (dict or list of tuples): Label/value pairs.
    title (unicode or None): Title, will be printed above.
    "
209,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,print_markdown,"Print data in GitHub-flavoured Markdown format for issues etc.

    data (dict or list of tuples): Label/value pairs.
    title (unicode or None): Title, will be rendered as headline 2.
    "
210,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,prints,"Print formatted message (manual ANSI escape sequences to avoid
    dependency)

    *texts (unicode): Texts to print. Each argument is rendered as paragraph.
    **kwargs: 'title' becomes coloured headline. exits=True performs sys exit.
    "
211,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,_wrap,"Wrap text at given width using textwrap module.

    text (unicode): Text to wrap. If it's a Path, it's converted to string.
    wrap_max (int): Maximum line length (indent is deducted).
    indent (int): Number of spaces for indentation.
    RETURNS (unicode): Wrapped text.
    "
212,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,minify_html,"Perform a template-specific, rudimentary HTML minification for displaCy.
    Disclaimer: NOT a general-purpose solution, only removes indentation and
    newlines.

    html (unicode): Markup to minify.
    RETURNS (unicode): ""Minified"" HTML.
    "
213,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\util.py,escape_html,"Replace <, >, &, "" with their HTML encoded representation. Intended to
    prevent HTML errors in rendered displaCy markup.

    text (unicode): The original text.
    RETURNS (unicode): Equivalent text to be safely used within HTML.
    "
214,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\_ml.py,concatenate_lists,"Compose two or more models `f`, `g`, etc, such that their outputs are
    concatenated, i.e. `concatenate(f, g)(x)` computes `hstack(f(x), g(x))`
    "
215,spaCy,C:\Users\njdx\Desktop\研一\科研\代码注释\dataset\spaCy\spacy\_ml.py,init_weights,"This is like the 'layer sequential unit variance', but instead
        of taking the actual inputs, we randomly generate whitened data.

        Why's this all so complicated? We have a huge number of inputs,
        and the maxout unit makes guessing the dynamics tricky. Instead
        we set the maxout weights to values that empirically result in
        whitened outputs given whitened inputs.
        "
